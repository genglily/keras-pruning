{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers import Conv2D,MaxPooling2D,ZeroPadding2D\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    " \n",
    "weight_decay = 0.0005\n",
    "nb_epoch=200\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 52)          59956     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 103)         48307     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               53248     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,224,881\n",
      "Trainable params: 1,220,517\n",
      "Non-trainable params: 4,364\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('0.8ent_b_cifar10vgg16_5_1.h5')\n",
    "model.load_weights('0.8ent_b_cifar10vgg16_5_1_weights.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [11.317330198587818, 40.18912508750436, 35.133658431381]\n",
      "1 [11.368109942889792, 15.98829962167586, 14.694114534828621]\n",
      "2 [23.273253092782742, 17.510516583008037, 3.803675710884699]\n",
      "3 [21.952739258767686, 33.63080155478253, 19.092064965748783]\n",
      "4 [6.100081602316809, 16.160523725558853, 14.702702742218024]\n",
      "5 [3.6487042413281507, 8.170473828367138, 11.896150746205537]\n",
      "6 [15.452362165146438, 31.669089617808154, 19.548816233842953]\n",
      "7 [3.3438065069539946, 4.972126922842586, 11.81860206063575]\n",
      "8 [43.47361966135982, 26.618113545039105, 10.355292665185308]\n",
      "9 [19.003342685791953, 15.276291867832363, 15.350305528116323]\n",
      "10 [13.643831206153008, 12.989342952156996, 12.765431219378767]\n",
      "11 [6.024644030743565, 15.029895499380736, 5.346327355328902]\n",
      "12 [36.47754378151908, 21.186782104573012, 25.17772241511781]\n",
      "13 [17.756974716999476, 23.84407443204569, 7.271462734783472]\n",
      "14 [17.468563563353428, 14.155726341010451, 16.343239945864486]\n",
      "15 [12.511409444853452, 8.756648834533696, 1.8983128196684138]\n",
      "16 [20.182270565555164, 18.305413368932232, 19.877383443082618]\n",
      "17 [12.45410200280693, 14.537922686912458, 1.9006582073708933]\n",
      "18 [20.794078762160275, 26.110432556029007, 21.202567107943743]\n",
      "19 [10.318762987060508, 37.918299785837995, 16.589177250525875]\n",
      "20 [7.079866611872913, 7.63573088439581, 16.670208419544974]\n",
      "21 [22.45819218319314, 6.546647734782988, 38.66316028158187]\n",
      "22 [9.950592581197363, 21.108632874449924, 7.455335892939896]\n",
      "23 [24.1918253419591, 28.005052779048693, 18.556597609330076]\n",
      "24 [0.7253868682221731, 16.475251827138763, 18.470909599125523]\n",
      "25 [3.3189584641439316, 23.8980352711444, 20.406465482523377]\n",
      "26 [19.015764698277103, 19.070334803698294, 5.4812043852522905]\n",
      "27 [16.848438806400214, 21.05661333721188, 21.01837139123269]\n",
      "28 [21.587593492792536, 32.608412110132534, 13.57719184645204]\n",
      "29 [17.397843993555192, 3.9114218691951854, 3.707656566429036]\n",
      "30 [4.648575378424748, 15.625476097482926, 14.645860271329601]\n",
      "31 [8.5081619737377, 10.951629558351941, 12.614452899517877]\n",
      "32 [13.162635793827716, 12.175859359867609, 10.55642941075707]\n",
      "33 [8.08819799108624, 1.9684334152235012, 6.0024464507218696]\n",
      "34 [13.22083877852302, 13.346631491656323, 5.4866120701907075]\n",
      "35 [23.13191882580956, 28.44673813220496, 28.607075293056194]\n",
      "36 [156.54169023378404, 108.61583914280325, 118.47170515306598]\n",
      "37 [9.835267423631247, 10.193010676907285, 15.321698069226803]\n",
      "38 [9.174122341701631, 3.4969721367292927, 14.793402364020233]\n",
      "39 [44.471229082008215, 32.98090486329638, 10.539980656788863]\n",
      "40 [41.41106993062449, 17.518567466805607, 57.50687798537667]\n",
      "41 [4.532618717126444, 14.801403283271918, 21.364809529486372]\n",
      "42 [16.305976520612194, 9.862017040248794, 10.425309789066128]\n",
      "43 [38.72768616099497, 21.663740439760364, 25.02633988397587]\n",
      "44 [18.07826197250029, 23.34625065290002, 5.170487687436266]\n",
      "45 [10.530072879376036, 29.01435417058682, 6.137865800697488]\n",
      "46 [16.66529938175674, 27.782081857591574, 17.941044952256316]\n",
      "47 [32.07857581860973, 43.39994951939614, 20.72544244126012]\n",
      "48 [1.720840015660105, 1.9253243606566124, 15.008689572760767]\n",
      "49 [15.797113173412708, 19.68028955572225, 15.815548930788895]\n",
      "50 [5.551798266260925, 9.528816988230513, 34.42509621999421]\n",
      "51 [33.22644849480059, 19.301555309975235, 26.844274122741332]\n",
      "52 [13.540005782561426, 20.34630525307167, 8.645104735287724]\n",
      "53 [7.19482307550572, 2.6496062724002365, 10.178133748618498]\n",
      "54 [9.415787838687155, 9.049227025020768, 22.4638357505119]\n",
      "55 [11.063359803661077, 13.937405504667577, 16.643062445276502]\n",
      "56 [9.075378488366617, 22.884861659973083, 2.5029422136068487]\n",
      "57 [4.197030055064125, 13.88296465771551, 4.62129269040288]\n",
      "58 [16.758080213137845, 21.761862939737963, 22.930337010921093]\n",
      "59 [16.88387641573034, 7.568351121467273, 16.536018765431955]\n",
      "60 [8.103656965242738, 17.648623492108133, 10.335157461567851]\n",
      "61 [8.019607105072899, 12.195590849979952, 15.515487101730557]\n",
      "62 [22.83859397218138, 4.645531318096671, 14.048527842059244]\n",
      "63 [22.869378821940398, 46.67259768988727, 12.707065529458978]\n",
      "64 [6.760459778290186, 19.030349747735816, 5.86459713985609]\n",
      "65 [6.991718178454372, 42.18010636708291, 1.579967065781777]\n",
      "66 [21.831325157160656, 27.593395952348914, 27.65403300736749]\n",
      "67 [17.064040620997815, 11.183992651228543, 3.683389150331795]\n",
      "68 [5.04655828751798, 8.91302441395716, 16.547340094754322]\n",
      "69 [9.071023891785481, 16.13562599015158, 13.965969722144116]\n",
      "70 [10.018507627433733, 14.680343377777412, 12.654061220014645]\n",
      "71 [7.4654206233195, 26.942603386798034, 20.57174437711905]\n",
      "72 [10.768807160972463, 32.141659109404664, 16.828622653017213]\n",
      "73 [23.851717981205955, 14.342828725179833, 8.45467995604681]\n",
      "74 [14.890391889350619, 11.552422126684215, 15.629031370806722]\n",
      "75 [23.643177623309544, 13.555759389748383, 24.84162404302937]\n",
      "76 [15.011265104865782, 8.063231523728405, 7.68412936369918]\n",
      "77 [7.523869740714823, 17.31778714116648, 3.091533097794706]\n",
      "78 [10.674693482440668, 10.827731159748787, 6.126917869653708]\n",
      "79 [8.918373904087511, 22.32699365332356, 10.188036079577994]\n",
      "80 [26.3273049065524, 85.13739846525522, 46.00853218892869]\n",
      "81 [16.242978082730964, 4.26552031112591, 13.64309044806446]\n",
      "82 [35.69767987488412, 17.534869196142584, 15.882462737383335]\n",
      "83 [10.367979362964695, 22.22761832949186, 4.046762177651047]\n",
      "84 [81.00924347305171, 39.73964781883011, 53.22394913137239]\n",
      "85 [33.43987607267907, 13.38881725452161, 0.9276840035667494]\n",
      "86 [16.487694327457895, 16.18048630846493, 16.141473195776456]\n",
      "87 [66.23409394107624, 12.456332417094377, 44.08085710941856]\n",
      "88 [12.217611484217992, 9.747486621958066, 14.825235887294411]\n",
      "89 [13.32458678100456, 19.880413337858453, 11.76404614639311]\n",
      "90 [54.98649608253218, 52.99806379884162, 48.34294481846036]\n",
      "91 [19.296733358380724, 10.640610480613717, 19.192814901634915]\n",
      "92 [9.292386601587179, 12.383394072440598, 19.328718924128296]\n",
      "93 [12.486533120482102, 3.2317416025582193, 22.6127328231238]\n",
      "94 [13.032489950892213, 11.49044362314848, 19.52850845627045]\n",
      "95 [13.171169130863202, 30.25765106292058, 19.918119380222006]\n",
      "96 [35.7244090672732, 18.111489147045177, 25.763726584947786]\n",
      "97 [16.441303755861483, 19.849838404153836, 17.747343133151723]\n",
      "98 [11.310713885178808, 5.780589009207062, 9.320838625591914]\n",
      "99 [5.066792963779473, 14.295307894843683, 12.963249795871283]\n",
      "100 [6.225055967025721, 13.052037498026277, 13.672065149347338]\n",
      "101 [11.649518695188402, 8.284823413226276, 16.196136159235476]\n",
      "102 [45.13389926557464, 30.72611810249843, 33.784805999962146]\n",
      "103 [6.381418250964106, 13.573164162046853, 8.455616173176724]\n",
      "104 [3.44124908607989, 16.6197084882857, 10.001075066952152]\n",
      "105 [19.349684043887297, 22.21313942250262, 31.29751621299852]\n",
      "106 [3.1703007742494314, 18.866360310177864, 7.516020274722174]\n",
      "107 [10.746931941221822, 20.853354167322124, 20.39445903877081]\n",
      "108 [27.40604636313492, 39.35463764534197, 23.955566122602793]\n",
      "109 [16.52636244116205, 9.389278753326046, 8.60130233440491]\n",
      "110 [9.750598497850488, 16.21004624456061, 20.32957332532992]\n",
      "111 [1.9680508780066768, 33.40132584424429, 12.691404972563609]\n",
      "112 [38.721809124730854, 15.629441873515121, 14.916713089400075]\n",
      "113 [9.810374230140274, 31.617994414585837, 46.680390619704575]\n",
      "114 [16.57113269896469, 21.727650532506615, 5.323695917690035]\n",
      "115 [3.939691206357757, 21.440145746347756, 8.364524894944102]\n",
      "116 [17.3877478290505, 19.89519585458626, 8.30637202219894]\n",
      "117 [17.75796008604642, 15.804605978785213, 14.895858069268165]\n",
      "118 [19.11051721660439, 70.70288185710092, 19.721973911328107]\n",
      "119 [22.54034875012682, 46.78575125872733, 53.25500806800352]\n",
      "120 [19.747252949271477, 10.140366043992916, 6.15920720987142]\n",
      "121 [11.576156974050457, 22.37497781507362, 20.670697410068428]\n",
      "122 [19.09655664297526, 6.616802506129441, 8.329819583793725]\n",
      "123 [20.989131294727855, 41.27434353228402, 13.831656312210455]\n",
      "124 [8.529667550346831, 34.6268348620823, 1.3831810756370577]\n",
      "125 [28.43287570789328, 72.42524119170074, 62.928872843985936]\n",
      "126 [16.00354212802237, 16.839848818564754, 15.169707452823674]\n",
      "127 [28.539442141827575, 43.99185746884951, 20.034859413927514]\n",
      "[[11.317330198587818, 40.18912508750436, 35.133658431381], [11.368109942889792, 15.98829962167586, 14.694114534828621], [23.273253092782742, 17.510516583008037, 3.803675710884699], [21.952739258767686, 33.63080155478253, 19.092064965748783], [6.100081602316809, 16.160523725558853, 14.702702742218024], [3.6487042413281507, 8.170473828367138, 11.896150746205537], [15.452362165146438, 31.669089617808154, 19.548816233842953], [3.3438065069539946, 4.972126922842586, 11.81860206063575], [43.47361966135982, 26.618113545039105, 10.355292665185308], [19.003342685791953, 15.276291867832363, 15.350305528116323], [13.643831206153008, 12.989342952156996, 12.765431219378767], [6.024644030743565, 15.029895499380736, 5.346327355328902], [36.47754378151908, 21.186782104573012, 25.17772241511781], [17.756974716999476, 23.84407443204569, 7.271462734783472], [17.468563563353428, 14.155726341010451, 16.343239945864486], [12.511409444853452, 8.756648834533696, 1.8983128196684138], [20.182270565555164, 18.305413368932232, 19.877383443082618], [12.45410200280693, 14.537922686912458, 1.9006582073708933], [20.794078762160275, 26.110432556029007, 21.202567107943743], [10.318762987060508, 37.918299785837995, 16.589177250525875], [7.079866611872913, 7.63573088439581, 16.670208419544974], [22.45819218319314, 6.546647734782988, 38.66316028158187], [9.950592581197363, 21.108632874449924, 7.455335892939896], [24.1918253419591, 28.005052779048693, 18.556597609330076], [0.7253868682221731, 16.475251827138763, 18.470909599125523], [3.3189584641439316, 23.8980352711444, 20.406465482523377], [19.015764698277103, 19.070334803698294, 5.4812043852522905], [16.848438806400214, 21.05661333721188, 21.01837139123269], [21.587593492792536, 32.608412110132534, 13.57719184645204], [17.397843993555192, 3.9114218691951854, 3.707656566429036], [4.648575378424748, 15.625476097482926, 14.645860271329601], [8.5081619737377, 10.951629558351941, 12.614452899517877], [13.162635793827716, 12.175859359867609, 10.55642941075707], [8.08819799108624, 1.9684334152235012, 6.0024464507218696], [13.22083877852302, 13.346631491656323, 5.4866120701907075], [23.13191882580956, 28.44673813220496, 28.607075293056194], [156.54169023378404, 108.61583914280325, 118.47170515306598], [9.835267423631247, 10.193010676907285, 15.321698069226803], [9.174122341701631, 3.4969721367292927, 14.793402364020233], [44.471229082008215, 32.98090486329638, 10.539980656788863], [41.41106993062449, 17.518567466805607, 57.50687798537667], [4.532618717126444, 14.801403283271918, 21.364809529486372], [16.305976520612194, 9.862017040248794, 10.425309789066128], [38.72768616099497, 21.663740439760364, 25.02633988397587], [18.07826197250029, 23.34625065290002, 5.170487687436266], [10.530072879376036, 29.01435417058682, 6.137865800697488], [16.66529938175674, 27.782081857591574, 17.941044952256316], [32.07857581860973, 43.39994951939614, 20.72544244126012], [1.720840015660105, 1.9253243606566124, 15.008689572760767], [15.797113173412708, 19.68028955572225, 15.815548930788895], [5.551798266260925, 9.528816988230513, 34.42509621999421], [33.22644849480059, 19.301555309975235, 26.844274122741332], [13.540005782561426, 20.34630525307167, 8.645104735287724], [7.19482307550572, 2.6496062724002365, 10.178133748618498], [9.415787838687155, 9.049227025020768, 22.4638357505119], [11.063359803661077, 13.937405504667577, 16.643062445276502], [9.075378488366617, 22.884861659973083, 2.5029422136068487], [4.197030055064125, 13.88296465771551, 4.62129269040288], [16.758080213137845, 21.761862939737963, 22.930337010921093], [16.88387641573034, 7.568351121467273, 16.536018765431955], [8.103656965242738, 17.648623492108133, 10.335157461567851], [8.019607105072899, 12.195590849979952, 15.515487101730557], [22.83859397218138, 4.645531318096671, 14.048527842059244], [22.869378821940398, 46.67259768988727, 12.707065529458978], [6.760459778290186, 19.030349747735816, 5.86459713985609], [6.991718178454372, 42.18010636708291, 1.579967065781777], [21.831325157160656, 27.593395952348914, 27.65403300736749], [17.064040620997815, 11.183992651228543, 3.683389150331795], [5.04655828751798, 8.91302441395716, 16.547340094754322], [9.071023891785481, 16.13562599015158, 13.965969722144116], [10.018507627433733, 14.680343377777412, 12.654061220014645], [7.4654206233195, 26.942603386798034, 20.57174437711905], [10.768807160972463, 32.141659109404664, 16.828622653017213], [23.851717981205955, 14.342828725179833, 8.45467995604681], [14.890391889350619, 11.552422126684215, 15.629031370806722], [23.643177623309544, 13.555759389748383, 24.84162404302937], [15.011265104865782, 8.063231523728405, 7.68412936369918], [7.523869740714823, 17.31778714116648, 3.091533097794706], [10.674693482440668, 10.827731159748787, 6.126917869653708], [8.918373904087511, 22.32699365332356, 10.188036079577994], [26.3273049065524, 85.13739846525522, 46.00853218892869], [16.242978082730964, 4.26552031112591, 13.64309044806446], [35.69767987488412, 17.534869196142584, 15.882462737383335], [10.367979362964695, 22.22761832949186, 4.046762177651047], [81.00924347305171, 39.73964781883011, 53.22394913137239], [33.43987607267907, 13.38881725452161, 0.9276840035667494], [16.487694327457895, 16.18048630846493, 16.141473195776456], [66.23409394107624, 12.456332417094377, 44.08085710941856], [12.217611484217992, 9.747486621958066, 14.825235887294411], [13.32458678100456, 19.880413337858453, 11.76404614639311], [54.98649608253218, 52.99806379884162, 48.34294481846036], [19.296733358380724, 10.640610480613717, 19.192814901634915], [9.292386601587179, 12.383394072440598, 19.328718924128296], [12.486533120482102, 3.2317416025582193, 22.6127328231238], [13.032489950892213, 11.49044362314848, 19.52850845627045], [13.171169130863202, 30.25765106292058, 19.918119380222006], [35.7244090672732, 18.111489147045177, 25.763726584947786], [16.441303755861483, 19.849838404153836, 17.747343133151723], [11.310713885178808, 5.780589009207062, 9.320838625591914], [5.066792963779473, 14.295307894843683, 12.963249795871283], [6.225055967025721, 13.052037498026277, 13.672065149347338], [11.649518695188402, 8.284823413226276, 16.196136159235476], [45.13389926557464, 30.72611810249843, 33.784805999962146], [6.381418250964106, 13.573164162046853, 8.455616173176724], [3.44124908607989, 16.6197084882857, 10.001075066952152], [19.349684043887297, 22.21313942250262, 31.29751621299852], [3.1703007742494314, 18.866360310177864, 7.516020274722174], [10.746931941221822, 20.853354167322124, 20.39445903877081], [27.40604636313492, 39.35463764534197, 23.955566122602793], [16.52636244116205, 9.389278753326046, 8.60130233440491], [9.750598497850488, 16.21004624456061, 20.32957332532992], [1.9680508780066768, 33.40132584424429, 12.691404972563609], [38.721809124730854, 15.629441873515121, 14.916713089400075], [9.810374230140274, 31.617994414585837, 46.680390619704575], [16.57113269896469, 21.727650532506615, 5.323695917690035], [3.939691206357757, 21.440145746347756, 8.364524894944102], [17.3877478290505, 19.89519585458626, 8.30637202219894], [17.75796008604642, 15.804605978785213, 14.895858069268165], [19.11051721660439, 70.70288185710092, 19.721973911328107], [22.54034875012682, 46.78575125872733, 53.25500806800352], [19.747252949271477, 10.140366043992916, 6.15920720987142], [11.576156974050457, 22.37497781507362, 20.670697410068428], [19.09655664297526, 6.616802506129441, 8.329819583793725], [20.989131294727855, 41.27434353228402, 13.831656312210455], [8.529667550346831, 34.6268348620823, 1.3831810756370577], [28.43287570789328, 72.42524119170074, 62.928872843985936], [16.00354212802237, 16.839848818564754, 15.169707452823674], [28.539442141827575, 43.99185746884951, 20.034859413927514]]\n"
     ]
    }
   ],
   "source": [
    "tensor =[]\n",
    "for i in range(0,128):\n",
    "   \n",
    "    u=model.get_layer('conv2d_4').get_weights()[0][:,:,:,i].squeeze()\n",
    "    v=np.mean(u, axis=1)\n",
    "    #print(v)\n",
    "    vT=v.T\n",
    "    D=np.cov(vT)\n",
    "    try:                 \n",
    "        invD=np.linalg.inv(D)\n",
    "        #t=np.mean(v,axis=1)\n",
    "        a=[]\n",
    "\n",
    "        for j in range(0,2): #表示filter的大小\n",
    "            for k in range(j+1,3):\n",
    "               #if (j!=k) and (j<k):\n",
    "                tp=v[j]-v[k]\n",
    "                d=np.sqrt(abs(np.dot(np.dot(tp,invD),tp.T)))\n",
    "                a.append(d)\n",
    "            \n",
    "        print(i,a)\n",
    "        tensor.append(a)\n",
    "    except:\n",
    "        print(\"不可逆\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [200. 100. 100.]\n",
      "{160.0, 120.0, 110.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{40.0, 20.0, 60.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{50.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{90.0, 50.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [100.   0. 100.]\n",
      "{80.0, 40.0, 50.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [100.   0.   0.]\n",
      "{40.0, 10.0, 70.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [100. 100.   0.]\n",
      "{50.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{50.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 50.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{20.0, 70.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{50.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [  0. 100. 100.]\n",
      "{70.0, 60.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "[0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.0, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.0, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.0, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.0, 1.584962500721156, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.0, 1.584962500721156]\n"
     ]
    }
   ],
   "source": [
    "entt=[]\n",
    "for i in range (0,128):\n",
    "    \n",
    "    data=tensor[i]\n",
    "    data0=np.array(data)\n",
    "    print('四舍五入，精确到个位\\n',np.round(data0,decimals=-2))\n",
    "    data1=np.round(data0,decimals=-1)\n",
    "\n",
    "    data1_value_list=set([data1[i] for i in range (data1.shape[0])])\n",
    "    print(data1_value_list)\n",
    "    ent=0.0\n",
    "    for data1_value in data1_value_list:\n",
    "        p=float(data1[data1==data1_value].shape[0])/data1.shape[0]\n",
    "        print(p)\n",
    "        logp=np.log2(p)\n",
    "        ent-=p*logp\n",
    "        print(ent)   \n",
    "    entt.append(ent)\n",
    "print(entt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156]\n",
      "[9, 10, 16, 27, 31, 32, 34, 49, 58, 70, 78, 86, 88, 90, 97, 98, 99, 100, 103, 126, 0, 1, 2, 3, 4, 5, 6, 7, 11, 13, 14, 15, 17, 18, 20, 22, 23, 24, 25, 26, 29, 33, 35, 37, 38, 42, 44, 45, 46, 48, 50, 51, 52, 53, 54, 55, 57, 59, 60, 61, 64, 66, 68, 69, 73, 74, 75, 76, 79, 82, 89, 91, 92, 94, 101, 102, 105, 107, 109, 110, 114, 116, 117, 118, 119, 120, 121, 122, 8]\n"
     ]
    }
   ],
   "source": [
    "import heapq #获取list中最小的\n",
    "\n",
    "f=int(len(entt)*0.7) #计算滤波器熵个数的80%\n",
    "m=entt\n",
    "max_number=heapq.nsmallest(f,m) #从m中找出最小的f个数，最大用nlargest\n",
    "max_index=[]\n",
    "for t in max_number:\n",
    "    index=m.index(t)\n",
    "    max_index.append(index)\n",
    "    m[index]=float('-inf')\n",
    "print(max_number)#输出最小的f个数\n",
    "print(max_index)#输出对应索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 89/128 channels from layer: conv2d_4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 39)        44967     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 39)        156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 52)          18304     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 103)         48307     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               53248     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,080,256\n",
      "Trainable params: 1,076,070\n",
      "Non-trainable params: 4,186\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerassurgeon.operations import delete_layer,insert_layer,delete_channels\n",
    "model_4 = delete_channels(model, model.layers[13],max_index)\n",
    "model_4.summary()\n",
    "model_4.save('8.7.6ent_b_cifar10vgg16_4.h5')\n",
    "model_4.save_weights('8.7.6ent_b_cifar10vgg16__weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "45000/45000 [==============================] - 28s 619us/step - loss: 0.8905 - acc: 0.7957 - val_loss: 0.7688 - val_acc: 0.8356\n",
      "Epoch 2/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7561 - acc: 0.8354 - val_loss: 0.7582 - val_acc: 0.8336\n",
      "Epoch 3/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7374 - acc: 0.8428 - val_loss: 0.7916 - val_acc: 0.8292\n",
      "Epoch 4/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.7286 - acc: 0.8486 - val_loss: 0.8383 - val_acc: 0.8130\n",
      "Epoch 5/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6984 - acc: 0.8575 - val_loss: 0.8522 - val_acc: 0.8164\n",
      "Epoch 6/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7054 - acc: 0.8566 - val_loss: 0.7795 - val_acc: 0.8348\n",
      "Epoch 7/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6934 - acc: 0.8609 - val_loss: 0.7204 - val_acc: 0.8506\n",
      "Epoch 8/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6873 - acc: 0.8637 - val_loss: 0.8298 - val_acc: 0.8236\n",
      "Epoch 9/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.6794 - acc: 0.8665 - val_loss: 0.7779 - val_acc: 0.8364\n",
      "Epoch 10/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6873 - acc: 0.8636 - val_loss: 0.7414 - val_acc: 0.8498\n",
      "Epoch 11/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6798 - acc: 0.8685 - val_loss: 0.7460 - val_acc: 0.8500\n",
      "Epoch 12/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.6801 - acc: 0.8680 - val_loss: 0.7470 - val_acc: 0.8506\n",
      "Epoch 13/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6800 - acc: 0.8685 - val_loss: 0.7200 - val_acc: 0.8556\n",
      "Epoch 14/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6679 - acc: 0.8706 - val_loss: 0.9160 - val_acc: 0.8062\n",
      "Epoch 15/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6738 - acc: 0.8690 - val_loss: 0.7339 - val_acc: 0.8548\n",
      "Epoch 16/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6722 - acc: 0.8724 - val_loss: 0.8006 - val_acc: 0.8392\n",
      "Epoch 17/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6726 - acc: 0.8722 - val_loss: 0.8088 - val_acc: 0.8380\n",
      "Epoch 18/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6712 - acc: 0.8713 - val_loss: 0.7893 - val_acc: 0.8336\n",
      "Epoch 19/200\n",
      "45000/45000 [==============================] - 21s 466us/step - loss: 0.6735 - acc: 0.8710 - val_loss: 0.7245 - val_acc: 0.8588\n",
      "Epoch 20/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6680 - acc: 0.8717 - val_loss: 0.7884 - val_acc: 0.8434\n",
      "Epoch 21/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6611 - acc: 0.8741 - val_loss: 0.8330 - val_acc: 0.8272\n",
      "Epoch 22/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6733 - acc: 0.8714 - val_loss: 0.7176 - val_acc: 0.8628\n",
      "Epoch 23/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6671 - acc: 0.8738 - val_loss: 0.8063 - val_acc: 0.8408\n",
      "Epoch 24/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6688 - acc: 0.8735 - val_loss: 0.8158 - val_acc: 0.8332\n",
      "Epoch 25/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6585 - acc: 0.8761 - val_loss: 0.7658 - val_acc: 0.8482\n",
      "Epoch 26/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6646 - acc: 0.8757 - val_loss: 0.7530 - val_acc: 0.8506\n",
      "Epoch 27/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6651 - acc: 0.8750 - val_loss: 0.7144 - val_acc: 0.8600\n",
      "Epoch 28/200\n",
      "45000/45000 [==============================] - 21s 467us/step - loss: 0.6576 - acc: 0.8794 - val_loss: 0.8023 - val_acc: 0.8430\n",
      "Epoch 29/200\n",
      "45000/45000 [==============================] - 21s 467us/step - loss: 0.6606 - acc: 0.8772 - val_loss: 0.8023 - val_acc: 0.8392\n",
      "Epoch 30/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6582 - acc: 0.8785 - val_loss: 0.7456 - val_acc: 0.8530\n",
      "Epoch 31/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6519 - acc: 0.8812 - val_loss: 0.7990 - val_acc: 0.8372\n",
      "Epoch 32/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6616 - acc: 0.8769 - val_loss: 0.7709 - val_acc: 0.8468 - E\n",
      "Epoch 33/200\n",
      "45000/45000 [==============================] - 21s 468us/step - loss: 0.6556 - acc: 0.8786 - val_loss: 0.7206 - val_acc: 0.8672\n",
      "Epoch 34/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6545 - acc: 0.8811 - val_loss: 0.7262 - val_acc: 0.8562\n",
      "Epoch 35/200\n",
      "45000/45000 [==============================] - 21s 468us/step - loss: 0.6589 - acc: 0.8779 - val_loss: 0.7119 - val_acc: 0.8628\n",
      "Epoch 36/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6495 - acc: 0.8818 - val_loss: 0.8157 - val_acc: 0.8402\n",
      "Epoch 37/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6557 - acc: 0.8811 - val_loss: 0.7172 - val_acc: 0.8638\n",
      "Epoch 38/200\n",
      "45000/45000 [==============================] - 21s 467us/step - loss: 0.6547 - acc: 0.8797 - val_loss: 0.7217 - val_acc: 0.8678\n",
      "Epoch 39/200\n",
      "45000/45000 [==============================] - 21s 468us/step - loss: 0.6568 - acc: 0.8792 - val_loss: 0.7894 - val_acc: 0.8404\n",
      "Epoch 40/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6491 - acc: 0.8815 - val_loss: 0.8407 - val_acc: 0.8336\n",
      "Epoch 41/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6497 - acc: 0.8821 - val_loss: 0.7825 - val_acc: 0.8472\n",
      "Epoch 42/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6639 - acc: 0.8774 - val_loss: 0.9756 - val_acc: 0.7868\n",
      "Epoch 43/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6579 - acc: 0.8807 - val_loss: 0.8155 - val_acc: 0.8350\n",
      "Epoch 44/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6533 - acc: 0.8803 - val_loss: 0.8868 - val_acc: 0.8160\n",
      "Epoch 45/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6558 - acc: 0.8796 - val_loss: 0.8158 - val_acc: 0.8444\n",
      "Epoch 46/200\n",
      "45000/45000 [==============================] - 21s 468us/step - loss: 0.6523 - acc: 0.8805 - val_loss: 0.9032 - val_acc: 0.8050\n",
      "Epoch 47/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6566 - acc: 0.8785 - val_loss: 0.7274 - val_acc: 0.8606\n",
      "Epoch 48/200\n",
      "45000/45000 [==============================] - 21s 468us/step - loss: 0.6535 - acc: 0.8805 - val_loss: 0.9491 - val_acc: 0.8004\n",
      "Epoch 49/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6613 - acc: 0.8779 - val_loss: 0.8545 - val_acc: 0.8316\n",
      "Epoch 50/200\n",
      "45000/45000 [==============================] - 21s 468us/step - loss: 0.6468 - acc: 0.8830 - val_loss: 0.7430 - val_acc: 0.8574\n",
      "Epoch 51/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6548 - acc: 0.8817 - val_loss: 0.7406 - val_acc: 0.8590\n",
      "Epoch 52/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6418 - acc: 0.8827 - val_loss: 0.8190 - val_acc: 0.8428\n",
      "Epoch 53/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6494 - acc: 0.8820 - val_loss: 0.7748 - val_acc: 0.8482\n",
      "Epoch 54/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6441 - acc: 0.8826 - val_loss: 0.7951 - val_acc: 0.8410\n",
      "Epoch 55/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6505 - acc: 0.8827 - val_loss: 0.8566 - val_acc: 0.8268\n",
      "Epoch 56/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6505 - acc: 0.8829 - val_loss: 0.8138 - val_acc: 0.8414\n",
      "Epoch 57/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6492 - acc: 0.8823 - val_loss: 0.7783 - val_acc: 0.8450\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6497 - acc: 0.8831 - val_loss: 0.7795 - val_acc: 0.8496\n",
      "Epoch 59/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6516 - acc: 0.8816 - val_loss: 0.7451 - val_acc: 0.8534\n",
      "Epoch 60/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6500 - acc: 0.8818 - val_loss: 0.7256 - val_acc: 0.8638\n",
      "Epoch 61/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6527 - acc: 0.8809 - val_loss: 0.7777 - val_acc: 0.8528\n",
      "Epoch 62/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6476 - acc: 0.8828 - val_loss: 0.7725 - val_acc: 0.8540\n",
      "Epoch 63/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6462 - acc: 0.8855 - val_loss: 0.6971 - val_acc: 0.8708\n",
      "Epoch 64/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6430 - acc: 0.8845 - val_loss: 0.7326 - val_acc: 0.8632\n",
      "Epoch 65/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6356 - acc: 0.8863 - val_loss: 0.8585 - val_acc: 0.8230\n",
      "Epoch 66/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6432 - acc: 0.8856 - val_loss: 0.7182 - val_acc: 0.8670\n",
      "Epoch 67/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6466 - acc: 0.8837 - val_loss: 0.8796 - val_acc: 0.8148\n",
      "Epoch 68/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6431 - acc: 0.8851 - val_loss: 0.7652 - val_acc: 0.8494\n",
      "Epoch 69/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6523 - acc: 0.8837 - val_loss: 0.7706 - val_acc: 0.8444\n",
      "Epoch 70/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6440 - acc: 0.8844 - val_loss: 0.7505 - val_acc: 0.8592\n",
      "Epoch 71/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6399 - acc: 0.8861 - val_loss: 0.7525 - val_acc: 0.8544\n",
      "Epoch 72/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6384 - acc: 0.8858 - val_loss: 0.7409 - val_acc: 0.8570\n",
      "Epoch 73/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6460 - acc: 0.8849 - val_loss: 0.7379 - val_acc: 0.8612\n",
      "Epoch 74/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6451 - acc: 0.8832 - val_loss: 0.8223 - val_acc: 0.8318\n",
      "Epoch 75/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6417 - acc: 0.8838 - val_loss: 0.7721 - val_acc: 0.8546\n",
      "Epoch 76/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6401 - acc: 0.8851 - val_loss: 0.7702 - val_acc: 0.8408\n",
      "Epoch 77/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6427 - acc: 0.8844 - val_loss: 0.7341 - val_acc: 0.8604\n",
      "Epoch 78/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6457 - acc: 0.8844 - val_loss: 0.7831 - val_acc: 0.8430\n",
      "Epoch 79/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6528 - acc: 0.8818 - val_loss: 0.7570 - val_acc: 0.8580\n",
      "Epoch 80/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6535 - acc: 0.8816 - val_loss: 0.7587 - val_acc: 0.8560\n",
      "Epoch 81/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6453 - acc: 0.8833 - val_loss: 0.8586 - val_acc: 0.8236\n",
      "Epoch 82/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6429 - acc: 0.8861 - val_loss: 0.7738 - val_acc: 0.8514\n",
      "Epoch 83/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6462 - acc: 0.8856 - val_loss: 0.7830 - val_acc: 0.8542\n",
      "Epoch 84/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6500 - acc: 0.8822 - val_loss: 0.8155 - val_acc: 0.8366\n",
      "Epoch 85/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6501 - acc: 0.8830 - val_loss: 0.7164 - val_acc: 0.8664\n",
      "Epoch 86/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6474 - acc: 0.8831 - val_loss: 0.7316 - val_acc: 0.8580\n",
      "Epoch 87/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6415 - acc: 0.8883 - val_loss: 0.7763 - val_acc: 0.8498\n",
      "Epoch 88/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6409 - acc: 0.8855 - val_loss: 0.8018 - val_acc: 0.8404\n",
      "Epoch 89/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6339 - acc: 0.8884 - val_loss: 0.7260 - val_acc: 0.8646\n",
      "Epoch 90/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6417 - acc: 0.8864 - val_loss: 0.7703 - val_acc: 0.8508\n",
      "Epoch 91/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6389 - acc: 0.8866 - val_loss: 0.8132 - val_acc: 0.8396\n",
      "Epoch 92/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6407 - acc: 0.8864 - val_loss: 0.7332 - val_acc: 0.8626\n",
      "Epoch 93/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6434 - acc: 0.8851 - val_loss: 0.7434 - val_acc: 0.8516\n",
      "Epoch 94/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6509 - acc: 0.8837 - val_loss: 0.7834 - val_acc: 0.8530\n",
      "Epoch 95/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6374 - acc: 0.8870 - val_loss: 0.7673 - val_acc: 0.8520\n",
      "Epoch 96/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6415 - acc: 0.8862 - val_loss: 0.7394 - val_acc: 0.8634\n",
      "Epoch 97/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6407 - acc: 0.8857 - val_loss: 0.7733 - val_acc: 0.8500\n",
      "Epoch 98/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6309 - acc: 0.8895 - val_loss: 0.7432 - val_acc: 0.8586\n",
      "Epoch 99/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6348 - acc: 0.8886 - val_loss: 0.8620 - val_acc: 0.8200\n",
      "Epoch 100/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6415 - acc: 0.8850 - val_loss: 0.7578 - val_acc: 0.8534\n",
      "Epoch 101/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6421 - acc: 0.8863 - val_loss: 0.7281 - val_acc: 0.8656\n",
      "Epoch 102/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6308 - acc: 0.8888 - val_loss: 0.7668 - val_acc: 0.8510\n",
      "Epoch 103/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6383 - acc: 0.8861 - val_loss: 0.7597 - val_acc: 0.8486\n",
      "Epoch 104/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6414 - acc: 0.8846 - val_loss: 0.7348 - val_acc: 0.8618\n",
      "Epoch 105/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6348 - acc: 0.8876 - val_loss: 0.8147 - val_acc: 0.8410\n",
      "Epoch 106/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6371 - acc: 0.8884 - val_loss: 0.7339 - val_acc: 0.8672\n",
      "Epoch 107/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6396 - acc: 0.8856 - val_loss: 0.7433 - val_acc: 0.8536\n",
      "Epoch 108/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6384 - acc: 0.8864 - val_loss: 0.8300 - val_acc: 0.8398\n",
      "Epoch 109/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6407 - acc: 0.8862 - val_loss: 0.8535 - val_acc: 0.8252\n",
      "Epoch 110/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6323 - acc: 0.8879 - val_loss: 0.7233 - val_acc: 0.8624\n",
      "Epoch 111/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6388 - acc: 0.8847 - val_loss: 0.8256 - val_acc: 0.8364\n",
      "Epoch 112/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6362 - acc: 0.8868 - val_loss: 0.7784 - val_acc: 0.8520\n",
      "Epoch 113/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6415 - acc: 0.8855 - val_loss: 0.7301 - val_acc: 0.8612\n",
      "Epoch 114/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6330 - acc: 0.8863 - val_loss: 0.7151 - val_acc: 0.8708\n",
      "Epoch 115/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6305 - acc: 0.8878 - val_loss: 0.7402 - val_acc: 0.8626\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6252 - acc: 0.8913 - val_loss: 0.7886 - val_acc: 0.8448\n",
      "Epoch 117/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6312 - acc: 0.8895 - val_loss: 0.8136 - val_acc: 0.8452\n",
      "Epoch 118/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6356 - acc: 0.8870 - val_loss: 0.7642 - val_acc: 0.8508\n",
      "Epoch 119/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6287 - acc: 0.8895 - val_loss: 0.7409 - val_acc: 0.8626\n",
      "Epoch 120/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6338 - acc: 0.8882 - val_loss: 0.7052 - val_acc: 0.8700\n",
      "Epoch 121/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6314 - acc: 0.8878 - val_loss: 0.7643 - val_acc: 0.8564\n",
      "Epoch 122/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6327 - acc: 0.8888 - val_loss: 0.7690 - val_acc: 0.8466\n",
      "Epoch 123/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6260 - acc: 0.8898 - val_loss: 0.7507 - val_acc: 0.8550\n",
      "Epoch 124/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6310 - acc: 0.8877 - val_loss: 0.8957 - val_acc: 0.8120\n",
      "Epoch 125/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6405 - acc: 0.8858 - val_loss: 0.7116 - val_acc: 0.8644\n",
      "Epoch 126/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6383 - acc: 0.8870 - val_loss: 0.7240 - val_acc: 0.8616\n",
      "Epoch 127/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6314 - acc: 0.8880 - val_loss: 0.7225 - val_acc: 0.8628\n",
      "Epoch 128/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6402 - acc: 0.8859 - val_loss: 0.9688 - val_acc: 0.7986\n",
      "Epoch 129/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6230 - acc: 0.8932 - val_loss: 0.7624 - val_acc: 0.8494\n",
      "Epoch 130/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6232 - acc: 0.8897 - val_loss: 0.7452 - val_acc: 0.8580\n",
      "Epoch 131/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6250 - acc: 0.8879 - val_loss: 0.7274 - val_acc: 0.8692\n",
      "Epoch 132/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6328 - acc: 0.8881 - val_loss: 0.7504 - val_acc: 0.8562\n",
      "Epoch 133/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6288 - acc: 0.8896 - val_loss: 0.7642 - val_acc: 0.8492\n",
      "Epoch 134/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6285 - acc: 0.8889 - val_loss: 0.9397 - val_acc: 0.8092\n",
      "Epoch 135/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6353 - acc: 0.8863 - val_loss: 0.7332 - val_acc: 0.8592\n",
      "Epoch 136/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6346 - acc: 0.8866 - val_loss: 0.7255 - val_acc: 0.8634\n",
      "Epoch 137/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6358 - acc: 0.8875 - val_loss: 0.7300 - val_acc: 0.8558\n",
      "Epoch 138/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6325 - acc: 0.8879 - val_loss: 0.7312 - val_acc: 0.8586\n",
      "Epoch 139/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6395 - acc: 0.8863 - val_loss: 0.7472 - val_acc: 0.8592\n",
      "Epoch 140/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6322 - acc: 0.8893 - val_loss: 0.8389 - val_acc: 0.8254\n",
      "Epoch 141/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6362 - acc: 0.8869 - val_loss: 0.7724 - val_acc: 0.8462\n",
      "Epoch 142/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6291 - acc: 0.8890 - val_loss: 0.7635 - val_acc: 0.8594.629\n",
      "Epoch 143/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6299 - acc: 0.8888 - val_loss: 0.8235 - val_acc: 0.8450\n",
      "Epoch 144/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6328 - acc: 0.8884 - val_loss: 0.7066 - val_acc: 0.8704\n",
      "Epoch 145/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6334 - acc: 0.8870 - val_loss: 0.7928 - val_acc: 0.8396\n",
      "Epoch 146/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6215 - acc: 0.8927 - val_loss: 0.7062 - val_acc: 0.8672\n",
      "Epoch 147/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6310 - acc: 0.8887 - val_loss: 0.7433 - val_acc: 0.8576\n",
      "Epoch 148/200\n",
      "45000/45000 [==============================] - 21s 468us/step - loss: 0.6248 - acc: 0.8900 - val_loss: 0.7466 - val_acc: 0.8626\n",
      "Epoch 149/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6347 - acc: 0.8875 - val_loss: 0.7221 - val_acc: 0.8640\n",
      "Epoch 150/200\n",
      "45000/45000 [==============================] - 21s 469us/step - loss: 0.6343 - acc: 0.8890 - val_loss: 0.7995 - val_acc: 0.8468\n",
      "Epoch 151/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6279 - acc: 0.8900 - val_loss: 0.7770 - val_acc: 0.8516\n",
      "Epoch 152/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6288 - acc: 0.8898 - val_loss: 0.6926 - val_acc: 0.8732acc\n",
      "Epoch 153/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6289 - acc: 0.8892 - val_loss: 0.8264 - val_acc: 0.8392\n",
      "Epoch 154/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6324 - acc: 0.8900 - val_loss: 0.7341 - val_acc: 0.8616\n",
      "Epoch 155/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6321 - acc: 0.8880 - val_loss: 0.8213 - val_acc: 0.8324\n",
      "Epoch 156/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6289 - acc: 0.8902 - val_loss: 0.6809 - val_acc: 0.8764\n",
      "Epoch 157/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6258 - acc: 0.8911 - val_loss: 0.7639 - val_acc: 0.8460\n",
      "Epoch 158/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6205 - acc: 0.8934 - val_loss: 0.7281 - val_acc: 0.8668\n",
      "Epoch 159/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6280 - acc: 0.8919 - val_loss: 0.8078 - val_acc: 0.8388\n",
      "Epoch 160/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6213 - acc: 0.8920 - val_loss: 0.7164 - val_acc: 0.8674\n",
      "Epoch 161/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6211 - acc: 0.8921 - val_loss: 0.7004 - val_acc: 0.8668\n",
      "Epoch 162/200\n",
      "45000/45000 [==============================] - 21s 476us/step - loss: 0.6261 - acc: 0.8890 - val_loss: 0.8315 - val_acc: 0.8306\n",
      "Epoch 163/200\n",
      "45000/45000 [==============================] - 21s 476us/step - loss: 0.6234 - acc: 0.8910 - val_loss: 0.7330 - val_acc: 0.8576\n",
      "Epoch 164/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6234 - acc: 0.8915 - val_loss: 0.7038 - val_acc: 0.8722\n",
      "Epoch 165/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6234 - acc: 0.8903 - val_loss: 0.8468 - val_acc: 0.8266\n",
      "Epoch 166/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6262 - acc: 0.8905 - val_loss: 0.9291 - val_acc: 0.8082\n",
      "Epoch 167/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6228 - acc: 0.8913 - val_loss: 0.7104 - val_acc: 0.8708\n",
      "Epoch 168/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6300 - acc: 0.8892 - val_loss: 0.7335 - val_acc: 0.8590\n",
      "Epoch 169/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6176 - acc: 0.8926 - val_loss: 0.7824 - val_acc: 0.8526\n",
      "Epoch 170/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6266 - acc: 0.8903 - val_loss: 0.7218 - val_acc: 0.8638\n",
      "Epoch 171/200\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 0.6232 - acc: 0.8922 - val_loss: 0.7682 - val_acc: 0.8514\n",
      "Epoch 172/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6227 - acc: 0.8914 - val_loss: 0.7933 - val_acc: 0.8480\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6201 - acc: 0.8919 - val_loss: 0.8304 - val_acc: 0.8422\n",
      "Epoch 174/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6140 - acc: 0.8932 - val_loss: 0.8494 - val_acc: 0.8342\n",
      "Epoch 175/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6133 - acc: 0.8922 - val_loss: 0.8052 - val_acc: 0.8408\n",
      "Epoch 176/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6224 - acc: 0.8910 - val_loss: 0.7503 - val_acc: 0.8550\n",
      "Epoch 177/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6321 - acc: 0.8905 - val_loss: 0.7056 - val_acc: 0.8726\n",
      "Epoch 178/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6199 - acc: 0.8924 - val_loss: 0.7678 - val_acc: 0.8512\n",
      "Epoch 179/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6219 - acc: 0.8913 - val_loss: 0.7302 - val_acc: 0.8576\n",
      "Epoch 180/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6185 - acc: 0.8944 - val_loss: 0.7809 - val_acc: 0.8464\n",
      "Epoch 181/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6181 - acc: 0.8931 - val_loss: 0.7869 - val_acc: 0.8420\n",
      "Epoch 182/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6226 - acc: 0.8908 - val_loss: 0.7952 - val_acc: 0.8466\n",
      "Epoch 183/200\n",
      "45000/45000 [==============================] - 21s 466us/step - loss: 0.6211 - acc: 0.8891 - val_loss: 0.7364 - val_acc: 0.8552\n",
      "Epoch 184/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6258 - acc: 0.8902 - val_loss: 0.8317 - val_acc: 0.8278\n",
      "Epoch 185/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6197 - acc: 0.8923 - val_loss: 0.6956 - val_acc: 0.8814\n",
      "Epoch 186/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6187 - acc: 0.8896 - val_loss: 0.6929 - val_acc: 0.8714\n",
      "Epoch 187/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6188 - acc: 0.8930 - val_loss: 0.8093 - val_acc: 0.8468\n",
      "Epoch 188/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6250 - acc: 0.8916 - val_loss: 0.7471 - val_acc: 0.8572\n",
      "Epoch 189/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6131 - acc: 0.8938 - val_loss: 0.8051 - val_acc: 0.8446\n",
      "Epoch 190/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6286 - acc: 0.8882 - val_loss: 0.7798 - val_acc: 0.8488\n",
      "Epoch 191/200\n",
      "45000/45000 [==============================] - 21s 471us/step - loss: 0.6178 - acc: 0.8920 - val_loss: 0.7393 - val_acc: 0.8604\n",
      "Epoch 192/200\n",
      "45000/45000 [==============================] - 21s 468us/step - loss: 0.6212 - acc: 0.8921 - val_loss: 0.7505 - val_acc: 0.8638\n",
      "Epoch 193/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6175 - acc: 0.8929 - val_loss: 0.7212 - val_acc: 0.8652\n",
      "Epoch 194/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6115 - acc: 0.8944 - val_loss: 0.7066 - val_acc: 0.8734\n",
      "Epoch 195/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6239 - acc: 0.8902 - val_loss: 0.8761 - val_acc: 0.8272\n",
      "Epoch 196/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6261 - acc: 0.8899 - val_loss: 0.7155 - val_acc: 0.8606\n",
      "Epoch 197/200\n",
      "45000/45000 [==============================] - 21s 467us/step - loss: 0.6207 - acc: 0.8920 - val_loss: 0.7768 - val_acc: 0.8436\n",
      "Epoch 198/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6182 - acc: 0.8919 - val_loss: 0.7161 - val_acc: 0.8688\n",
      "Epoch 199/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6157 - acc: 0.8930 - val_loss: 0.7019 - val_acc: 0.8758\n",
      "Epoch 200/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6202 - acc: 0.8906 - val_loss: 0.7477 - val_acc: 0.8548\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_4.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    " \n",
    "train_history=model_4.fit(x_train,y_train,epochs=200, batch_size=128,\n",
    "             validation_split=0.1, verbose=1)\n",
    "\n",
    "model_4.save('8.7.6ent_b_cifar10vgg16_4_1.h5')\n",
    "model_4.save_weights('8.7.6ent_b_cifar10vgg16_4_1_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7724098143577576, 0.8453, 0.9897, 0.8453]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import keras\n",
    "top1_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=1) #top-1精度\n",
    "\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "\n",
    "model_4.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',metrics=['accuracy','top_k_categorical_accuracy',top1_acc])\n",
    "\n",
    "model_4.evaluate(x_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试损失为：0.7724\n",
      "测试准确率为：0.8453\n"
     ]
    }
   ],
   "source": [
    "score=model_4.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"测试损失为：{:.4f}\".format(score[0]))\n",
    "print(\"测试准确率为：{:.4f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [11.308420910609762, 26.478920307909224, 26.34537458557846]\n",
      "1 [20.476583737708367, 1.2002084233163064, 17.873948853270484]\n",
      "2 [10.90440759699966, 46.57610614832921, 28.41881770907596]\n",
      "3 [5.96243087440583, 7.311813257036446, 16.195286472445016]\n",
      "4 [14.946310046934284, 11.08312555825844, 9.57033178210749]\n",
      "5 [28.55270010611272, 11.739058330793592, 23.135302990927965]\n",
      "6 [14.222955144786663, 35.72191217397358, 34.1287438431454]\n",
      "7 [8.988552548634049, 27.313881260842493, 8.449666230533898]\n",
      "8 [7.611988519607803, 11.359744835963072, 12.893112737871805]\n",
      "9 [13.70709408527973, 7.42596945807569, 11.430962950959383]\n",
      "10 [11.61127679170213, 8.71732999661579, 10.022331075636384]\n",
      "11 [11.564025407125245, 3.211935496179007, 8.494344989969754]\n",
      "12 [15.58527238616973, 34.915917952225364, 34.59493806999123]\n",
      "13 [74.80777215001027, 27.853226681248383, 15.445392274626432]\n",
      "14 [11.697110411843044, 19.650158737356815, 5.590659625147151]\n",
      "15 [19.708627971465674, 8.749008967271282, 8.15106594744266]\n",
      "16 [20.25527438009697, 29.20775409583061, 25.555719273729242]\n",
      "17 [48.65618351593081, 45.17532239417854, 92.80319736497287]\n",
      "18 [6.496911268697241, 20.841080799229317, 10.51232662307432]\n",
      "19 [23.89199785238729, 30.850232804469496, 29.145580616951285]\n",
      "20 [29.1203700957517, 6.470931196813488, 10.24293680599941]\n",
      "21 [5.246918500546134, 3.55496231525905, 5.777732892758709]\n",
      "22 [23.447378187691857, 11.171848024989288, 7.727697059638825]\n",
      "23 [37.213752109937175, 10.683676811048802, 22.02189315247332]\n",
      "24 [13.029189681051236, 21.820374911799842, 19.300893721863684]\n",
      "25 [11.777062843224401, 6.375452100393, 11.441053109597803]\n",
      "26 [17.413404624290028, 13.361225352500545, 8.213476019100879]\n",
      "27 [4.957180156229786, 10.277752823555927, 13.55110618045587]\n",
      "28 [6.02605362618682, 6.7394413842214105, 5.554631829821132]\n",
      "29 [11.103086899773345, 23.7825182703117, 8.856880846081525]\n",
      "30 [27.737074135549197, 21.891373179772092, 62.61243091776687]\n",
      "31 [11.523823990886882, 14.045527036019099, 15.953986268888707]\n",
      "32 [10.718711658331356, 15.719865217140647, 12.757929531011483]\n",
      "33 [103971552.44555695, 264134356.3870149, 275677797.3282135]\n",
      "34 [12.03137873791272, 16.575077075117374, 15.857422085491978]\n",
      "35 [1.8825034999941526, 1.654778600675824, 20.16781866182895]\n",
      "36 [7.137845690050375, 33.58832089578377, 38.861544031242346]\n",
      "37 [10.348452280026224, 16.318803643525985, 16.558216073565394]\n",
      "38 [7.36372107854165, 9.242710953219868, 12.869888314049295]\n",
      "39 [1.9435336700949917, 14.287770458475048, 2.454832806769515]\n",
      "40 [4.889927238437889, 17.13532043553688, 5.47585432450416]\n",
      "41 [12.177556759931667, 15.642525266895491, 15.79042097964318]\n",
      "42 [43.34096896072008, 67.91484336355845, 22.522782892141525]\n",
      "43 [11.571715095397984, 8.788361829167345, 13.655913831121621]\n",
      "44 [12.261667106254736, 23.67084441003158, 42.31259033554103]\n",
      "45 [21.620435067527094, 21.614677490274413, 18.936180622734966]\n",
      "46 [12.89690676131334, 15.128257432218883, 32.946237494564045]\n",
      "47 [22.425576715910417, 36.685229437522494, 20.46917728822562]\n",
      "48 [21.147552866307638, 15.965115904442571, 31.978701656245217]\n",
      "49 [17.926362411245524, 15.210735208021422, 1.9768923759360133]\n",
      "50 [80.89435224399529, 50.97406782688932, 137.86593927404283]\n",
      "51 [18.524091730747763, 7.415551939426353, 21.08278677411603]\n",
      "52 [7.604214275333002, 13.44251498300442, 11.21891440268424]\n",
      "53 [21.409231760673112, 18.36828874785722, 13.488648905357133]\n",
      "54 [19.384467857245436, 27.213773638209638, 19.89725494764923]\n",
      "55 [17.341779057317122, 17.65222184134264, 14.551152470096923]\n",
      "56 [22.892271970796536, 11.178103459363856, 132.58995722143305]\n",
      "57 [24.178942790603625, 30.34100288503902, 15.351446812317555]\n",
      "58 [23.450094353784934, 21.3395623808789, 39.37268999377857]\n",
      "59 [27.203843389378065, 10.63742738871553, 38.430923420326046]\n",
      "60 [44.54326665066622, 44.18800222065584, 3.153597179555188]\n",
      "61 [22.329150887073432, 18.834258021663423, 6.867749760755114]\n",
      "62 [4.561038684670043, 27.692555243759106, 11.494760325149562]\n",
      "63 [20.4322799409017, 30.413899603269535, 19.06544729492925]\n",
      "64 [12.935893175528612, 9.412863804649614, 14.361978338477305]\n",
      "65 [23.671753573971777, 26.34056428627822, 1.2270705027576014]\n",
      "66 [24.549624031124797, 46.548491133298874, 12.348521157963239]\n",
      "67 [15.318661131459029, 22.09344523218319, 15.31820108588967]\n",
      "68 [14.310794812034748, 11.120480144309603, 27.03232867784328]\n",
      "69 [9.719333839665413, 31.723878342540104, 26.778917440280825]\n",
      "70 [13.681784721231056, 22.71268867641048, 12.86473044286882]\n",
      "71 [24.377858410030296, 46.26313162297487, 23.560444587890892]\n",
      "72 [12.304520446612164, 11.988440986431732, 12.777348228274208]\n",
      "73 [10.693532891803537, 8.466968475679584, 9.334867982773703]\n",
      "74 [15.757114498961224, 6.170002327274026, 10.231975780883412]\n",
      "75 [20.351733257141127, 11.859347176299295, 18.446739054364247]\n",
      "76 [1.9393154375848913, 20.409120062849528, 15.166682100113503]\n",
      "77 [9.854232414192944, 19.10348964089168, 17.170494308710232]\n",
      "78 [20.94800351992875, 19.13353096425714, 14.470001001839119]\n",
      "79 [9.959892331809138, 14.35150581726609, 15.533847860210852]\n",
      "80 [11.664106324222763, 13.541429527358224, 51.266777679032145]\n",
      "81 [15.115008466732721, 30.128822068371676, 20.50092853211071]\n",
      "82 [44.744781189828004, 28.860387366134567, 31.620481386905677]\n",
      "83 [11.994838448931684, 23.675088877042818, 14.664988745091017]\n",
      "84 [10.300854112340664, 3.26258138796258, 7.72235606974807]\n",
      "85 [3.993335536410616, 10.575221780385776, 16.919779414414187]\n",
      "86 [10.043369708075005, 27.58518193975773, 15.69379438593766]\n",
      "87 [7.027456964383417, 3.7213197692747815, 31.255952863190497]\n",
      "88 [0.4855391755725021, 34.481804624449275, 17.604555216758044]\n",
      "89 [2.522485464865146, 10.73365641358446, 43.85138671230828]\n",
      "90 [8.560965630265256, 6.173074571913914, 8.756070347426244]\n",
      "91 [6.038797192587538, 11.953466153730428, 50.56655652497948]\n",
      "92 [6.171043059858875, 17.660062671962745, 8.179924991515588]\n",
      "93 [10.196364536303834, 8.8390077338057, 5.258280944361726]\n",
      "94 [9.617828471735121, 43.819527012175755, 77.50791388682765]\n",
      "95 [12.410768776565222, 11.459261989530392, 12.196147833886226]\n",
      "96 [1.1940286760448968, 7.7612330338004965, 18.628848094373474]\n",
      "97 [11.750333253720353, 9.303302190936495, 8.586863571082754]\n",
      "98 [5.392656512935902, 7.322078788939851, 10.921572002961348]\n",
      "99 [74.4165504115764, 16.168568738489796, 29.2316411308464]\n",
      "100 [27.62624142304169, 53.43700357842335, 22.50961801741191]\n",
      "101 [11.997970210704015, 6.07895213483518, 9.583099517181337]\n",
      "102 [21.580687666967762, 23.016801170860298, 26.69502827507275]\n",
      "103 [18.643622944699754, 10.545470103189597, 25.99698203250877]\n",
      "104 [11.731670205054815, 13.071488276813549, 20.13105439402246]\n",
      "105 [12.057849343285472, 1.4122012202150773, 14.891923557592492]\n",
      "106 [47.53769898414343, 22.367331954377665, 24.388780972592738]\n",
      "107 [4.815536927959437, 8.509521228712027, 6.994537981384372]\n",
      "108 [6.986263011429513, 4.462832928374311, 8.828212177217518]\n",
      "109 [205.62463787012186, 299.21571881623925, 141.8946772302243]\n",
      "110 [9.953843023489938, 9.617534219052764, 11.125002314535179]\n",
      "111 [9.537937276022847, 21.9646431243651, 7.614074493467646]\n",
      "112 [12.919480015356172, 12.870331445559383, 15.828740732129578]\n",
      "113 [16.54363007828065, 57.848929545819324, 75.7808006792414]\n",
      "114 [12.54285652540499, 19.149950668829185, 13.502981880910916]\n",
      "115 [7.678787380489554, 5.386710429029559, 7.0839655949270295]\n",
      "116 [10.623240560823357, 17.10526282654117, 5.79813262376431]\n",
      "117 [17.405846198995214, 13.342632774019759, 8.63909497778437]\n",
      "118 [20.105149661789977, 36.8342038277571, 58.15889957383902]\n",
      "119 [19.92011280119797, 20.168899056587488, 14.113287534271539]\n",
      "120 [24.053735182056364, 69.84207373875462, 19.756128446789738]\n",
      "121 [3.510561471248482, 28.780727833205848, 11.503948508219022]\n",
      "122 [7.043080883857796, 23.02058356194188, 5.211424934889664]\n",
      "123 [7.127980629388368, 13.89110002252577, 7.167994361436704]\n",
      "124 [2.566611178735505, 37.67864615472345, 4.826461875906434]\n",
      "125 [56.64614944771991, 14.609886837729267, 27.204191709547707]\n",
      "126 [6.313333989751427, 23.24507129890468, 13.613964616433915]\n",
      "127 [7.650898444953991, 1.275666888986978, 9.563302050096473]\n",
      "[[11.308420910609762, 26.478920307909224, 26.34537458557846], [20.476583737708367, 1.2002084233163064, 17.873948853270484], [10.90440759699966, 46.57610614832921, 28.41881770907596], [5.96243087440583, 7.311813257036446, 16.195286472445016], [14.946310046934284, 11.08312555825844, 9.57033178210749], [28.55270010611272, 11.739058330793592, 23.135302990927965], [14.222955144786663, 35.72191217397358, 34.1287438431454], [8.988552548634049, 27.313881260842493, 8.449666230533898], [7.611988519607803, 11.359744835963072, 12.893112737871805], [13.70709408527973, 7.42596945807569, 11.430962950959383], [11.61127679170213, 8.71732999661579, 10.022331075636384], [11.564025407125245, 3.211935496179007, 8.494344989969754], [15.58527238616973, 34.915917952225364, 34.59493806999123], [74.80777215001027, 27.853226681248383, 15.445392274626432], [11.697110411843044, 19.650158737356815, 5.590659625147151], [19.708627971465674, 8.749008967271282, 8.15106594744266], [20.25527438009697, 29.20775409583061, 25.555719273729242], [48.65618351593081, 45.17532239417854, 92.80319736497287], [6.496911268697241, 20.841080799229317, 10.51232662307432], [23.89199785238729, 30.850232804469496, 29.145580616951285], [29.1203700957517, 6.470931196813488, 10.24293680599941], [5.246918500546134, 3.55496231525905, 5.777732892758709], [23.447378187691857, 11.171848024989288, 7.727697059638825], [37.213752109937175, 10.683676811048802, 22.02189315247332], [13.029189681051236, 21.820374911799842, 19.300893721863684], [11.777062843224401, 6.375452100393, 11.441053109597803], [17.413404624290028, 13.361225352500545, 8.213476019100879], [4.957180156229786, 10.277752823555927, 13.55110618045587], [6.02605362618682, 6.7394413842214105, 5.554631829821132], [11.103086899773345, 23.7825182703117, 8.856880846081525], [27.737074135549197, 21.891373179772092, 62.61243091776687], [11.523823990886882, 14.045527036019099, 15.953986268888707], [10.718711658331356, 15.719865217140647, 12.757929531011483], [103971552.44555695, 264134356.3870149, 275677797.3282135], [12.03137873791272, 16.575077075117374, 15.857422085491978], [1.8825034999941526, 1.654778600675824, 20.16781866182895], [7.137845690050375, 33.58832089578377, 38.861544031242346], [10.348452280026224, 16.318803643525985, 16.558216073565394], [7.36372107854165, 9.242710953219868, 12.869888314049295], [1.9435336700949917, 14.287770458475048, 2.454832806769515], [4.889927238437889, 17.13532043553688, 5.47585432450416], [12.177556759931667, 15.642525266895491, 15.79042097964318], [43.34096896072008, 67.91484336355845, 22.522782892141525], [11.571715095397984, 8.788361829167345, 13.655913831121621], [12.261667106254736, 23.67084441003158, 42.31259033554103], [21.620435067527094, 21.614677490274413, 18.936180622734966], [12.89690676131334, 15.128257432218883, 32.946237494564045], [22.425576715910417, 36.685229437522494, 20.46917728822562], [21.147552866307638, 15.965115904442571, 31.978701656245217], [17.926362411245524, 15.210735208021422, 1.9768923759360133], [80.89435224399529, 50.97406782688932, 137.86593927404283], [18.524091730747763, 7.415551939426353, 21.08278677411603], [7.604214275333002, 13.44251498300442, 11.21891440268424], [21.409231760673112, 18.36828874785722, 13.488648905357133], [19.384467857245436, 27.213773638209638, 19.89725494764923], [17.341779057317122, 17.65222184134264, 14.551152470096923], [22.892271970796536, 11.178103459363856, 132.58995722143305], [24.178942790603625, 30.34100288503902, 15.351446812317555], [23.450094353784934, 21.3395623808789, 39.37268999377857], [27.203843389378065, 10.63742738871553, 38.430923420326046], [44.54326665066622, 44.18800222065584, 3.153597179555188], [22.329150887073432, 18.834258021663423, 6.867749760755114], [4.561038684670043, 27.692555243759106, 11.494760325149562], [20.4322799409017, 30.413899603269535, 19.06544729492925], [12.935893175528612, 9.412863804649614, 14.361978338477305], [23.671753573971777, 26.34056428627822, 1.2270705027576014], [24.549624031124797, 46.548491133298874, 12.348521157963239], [15.318661131459029, 22.09344523218319, 15.31820108588967], [14.310794812034748, 11.120480144309603, 27.03232867784328], [9.719333839665413, 31.723878342540104, 26.778917440280825], [13.681784721231056, 22.71268867641048, 12.86473044286882], [24.377858410030296, 46.26313162297487, 23.560444587890892], [12.304520446612164, 11.988440986431732, 12.777348228274208], [10.693532891803537, 8.466968475679584, 9.334867982773703], [15.757114498961224, 6.170002327274026, 10.231975780883412], [20.351733257141127, 11.859347176299295, 18.446739054364247], [1.9393154375848913, 20.409120062849528, 15.166682100113503], [9.854232414192944, 19.10348964089168, 17.170494308710232], [20.94800351992875, 19.13353096425714, 14.470001001839119], [9.959892331809138, 14.35150581726609, 15.533847860210852], [11.664106324222763, 13.541429527358224, 51.266777679032145], [15.115008466732721, 30.128822068371676, 20.50092853211071], [44.744781189828004, 28.860387366134567, 31.620481386905677], [11.994838448931684, 23.675088877042818, 14.664988745091017], [10.300854112340664, 3.26258138796258, 7.72235606974807], [3.993335536410616, 10.575221780385776, 16.919779414414187], [10.043369708075005, 27.58518193975773, 15.69379438593766], [7.027456964383417, 3.7213197692747815, 31.255952863190497], [0.4855391755725021, 34.481804624449275, 17.604555216758044], [2.522485464865146, 10.73365641358446, 43.85138671230828], [8.560965630265256, 6.173074571913914, 8.756070347426244], [6.038797192587538, 11.953466153730428, 50.56655652497948], [6.171043059858875, 17.660062671962745, 8.179924991515588], [10.196364536303834, 8.8390077338057, 5.258280944361726], [9.617828471735121, 43.819527012175755, 77.50791388682765], [12.410768776565222, 11.459261989530392, 12.196147833886226], [1.1940286760448968, 7.7612330338004965, 18.628848094373474], [11.750333253720353, 9.303302190936495, 8.586863571082754], [5.392656512935902, 7.322078788939851, 10.921572002961348], [74.4165504115764, 16.168568738489796, 29.2316411308464], [27.62624142304169, 53.43700357842335, 22.50961801741191], [11.997970210704015, 6.07895213483518, 9.583099517181337], [21.580687666967762, 23.016801170860298, 26.69502827507275], [18.643622944699754, 10.545470103189597, 25.99698203250877], [11.731670205054815, 13.071488276813549, 20.13105439402246], [12.057849343285472, 1.4122012202150773, 14.891923557592492], [47.53769898414343, 22.367331954377665, 24.388780972592738], [4.815536927959437, 8.509521228712027, 6.994537981384372], [6.986263011429513, 4.462832928374311, 8.828212177217518], [205.62463787012186, 299.21571881623925, 141.8946772302243], [9.953843023489938, 9.617534219052764, 11.125002314535179], [9.537937276022847, 21.9646431243651, 7.614074493467646], [12.919480015356172, 12.870331445559383, 15.828740732129578], [16.54363007828065, 57.848929545819324, 75.7808006792414], [12.54285652540499, 19.149950668829185, 13.502981880910916], [7.678787380489554, 5.386710429029559, 7.0839655949270295], [10.623240560823357, 17.10526282654117, 5.79813262376431], [17.405846198995214, 13.342632774019759, 8.63909497778437], [20.105149661789977, 36.8342038277571, 58.15889957383902], [19.92011280119797, 20.168899056587488, 14.113287534271539], [24.053735182056364, 69.84207373875462, 19.756128446789738], [3.510561471248482, 28.780727833205848, 11.503948508219022], [7.043080883857796, 23.02058356194188, 5.211424934889664], [7.127980629388368, 13.89110002252577, 7.167994361436704], [2.566611178735505, 37.67864615472345, 4.826461875906434], [56.64614944771991, 14.609886837729267, 27.204191709547707], [6.313333989751427, 23.24507129890468, 13.613964616433915], [7.650898444953991, 1.275666888986978, 9.563302050096473]]\n"
     ]
    }
   ],
   "source": [
    "tensor =[]\n",
    "for i in range(0,128):\n",
    "   \n",
    "    u=model_4.get_layer('conv2d_3').get_weights()[0][:,:,:,i].squeeze()\n",
    "    v=np.mean(u, axis=1)\n",
    "    #print(v)\n",
    "    vT=v.T\n",
    "    D=np.cov(vT)\n",
    "    try:                 \n",
    "        invD=np.linalg.inv(D)\n",
    "        #t=np.mean(v,axis=1)\n",
    "        a=[]\n",
    "\n",
    "        for j in range(0,2): #表示filter的大小\n",
    "            for k in range(j+1,3):\n",
    "               #if (j!=k) and (j<k):\n",
    "                tp=v[j]-v[k]\n",
    "                d=np.sqrt(abs(np.dot(np.dot(tp,invD),tp.T)))\n",
    "                a.append(d)\n",
    "            \n",
    "        print(i,a)\n",
    "        tensor.append(a)\n",
    "    except:\n",
    "        print(\"不可逆\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 50.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [100.   0.   0.]\n",
      "{20.0, 70.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{50.0, 90.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{20.0, 30.0, 60.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [1.039716e+08 2.641344e+08 2.756778e+08]\n",
      "{264134360.0, 275677800.0, 103971550.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{40.0, 20.0, 70.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [100. 100. 100.]\n",
      "{80.0, 50.0, 140.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{10.0, 20.0, 130.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 0.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{50.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{50.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{10.0, 50.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 40.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{10.0, 50.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{40.0, 10.0, 80.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [100.   0.   0.]\n",
      "{20.0, 70.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{50.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{50.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [200. 300. 100.]\n",
      "{140.0, 210.0, 300.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0. 100. 100.]\n",
      "{80.0, 20.0, 60.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{40.0, 20.0, 60.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{20.0, 70.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 40.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [100.   0.   0.]\n",
      "{10.0, 60.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "[0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.0, 1.584962500721156, 0.0, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 1.584962500721156, 1.584962500721156, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156, 0.0, 1.584962500721156, 0.0, 0.0, 1.584962500721156, 1.584962500721156, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896]\n"
     ]
    }
   ],
   "source": [
    "entt=[]\n",
    "for i in range (0,128):\n",
    "    \n",
    "    data=tensor[i]\n",
    "    data0=np.array(data)\n",
    "    print('四舍五入，精确到个位\\n',np.round(data0,decimals=-2))\n",
    "    data1=np.round(data0,decimals=-1)\n",
    "\n",
    "    data1_value_list=set([data1[i] for i in range (data1.shape[0])])\n",
    "    print(data1_value_list)\n",
    "    ent=0.0\n",
    "    for data1_value in data1_value_list:\n",
    "        p=float(data1[data1==data1_value].shape[0])/data1.shape[0]\n",
    "        print(p)\n",
    "        logp=np.log2(p)\n",
    "        ent-=p*logp\n",
    "        print(ent)   \n",
    "    entt.append(ent)\n",
    "print(entt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896]\n",
      "[4, 8, 9, 10, 25, 28, 38, 43, 45, 52, 64, 67, 72, 73, 90, 93, 95, 97, 98, 101, 110, 115, 123, 0, 1, 3, 7, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 29, 31, 32, 34, 35, 37, 39, 41, 47, 48, 49, 51, 53, 54, 55, 57, 58, 60, 61, 63, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 91, 92, 102, 104, 105, 106, 107, 108, 111, 112, 114, 116, 117]\n"
     ]
    }
   ],
   "source": [
    "import heapq #获取list中最小的\n",
    "\n",
    "f=int(len(entt)*0.7) #计算滤波器熵个数的80%\n",
    "m=entt\n",
    "max_number=heapq.nsmallest(f,m) #从m中找出最小的f个数，最大用nlargest\n",
    "max_index=[]\n",
    "for t in max_number:\n",
    "    index=m.index(t)\n",
    "    max_index.append(index)\n",
    "    m[index]=float('-inf')\n",
    "print(max_number)#输出最小的f个数\n",
    "print(max_index)#输出对应索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 89/128 channels from layer: conv2d_3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 39)        22503     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 39)        156       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 39)        13728     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 39)        156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 52)          18304     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 103)         48307     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               53248     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 997,308\n",
      "Trainable params: 993,300\n",
      "Non-trainable params: 4,008\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerassurgeon.operations import delete_layer,insert_layer,delete_channels\n",
    "model_3 = delete_channels(model_4, model_4.layers[9],max_index)\n",
    "model_3.summary()\n",
    "model_3.save('8.7.6ent_b_cifar10vgg16_3.h5')\n",
    "model_3.save_weights('8.7.6ent_b_cifar10vgg16_3_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "45000/45000 [==============================] - 26s 584us/step - loss: 0.8678 - acc: 0.7959 - val_loss: 0.7408 - val_acc: 0.8378\n",
      "Epoch 2/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.7629 - acc: 0.8304 - val_loss: 1.0166 - val_acc: 0.7654\n",
      "Epoch 3/200\n",
      "45000/45000 [==============================] - 20s 446us/step - loss: 0.7622 - acc: 0.8331 - val_loss: 0.8050 - val_acc: 0.8206\n",
      "Epoch 4/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.7438 - acc: 0.8400 - val_loss: 0.7657 - val_acc: 0.8338\n",
      "Epoch 5/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.7314 - acc: 0.8434 - val_loss: 0.7381 - val_acc: 0.8406\n",
      "Epoch 6/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.7227 - acc: 0.8460 - val_loss: 0.7309 - val_acc: 0.8436\n",
      "Epoch 7/200\n",
      "45000/45000 [==============================] - 20s 454us/step - loss: 0.7183 - acc: 0.8472 - val_loss: 0.7690 - val_acc: 0.8346\n",
      "Epoch 8/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.7180 - acc: 0.8491 - val_loss: 0.7710 - val_acc: 0.8310\n",
      "Epoch 9/200\n",
      "45000/45000 [==============================] - 20s 445us/step - loss: 0.7130 - acc: 0.8497 - val_loss: 0.7377 - val_acc: 0.8402\n",
      "Epoch 10/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.7100 - acc: 0.8520 - val_loss: 0.7192 - val_acc: 0.8498\n",
      "Epoch 11/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.7069 - acc: 0.8557 - val_loss: 0.7286 - val_acc: 0.8498\n",
      "Epoch 12/200\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 0.7165 - acc: 0.8514 - val_loss: 0.7133 - val_acc: 0.8486\n",
      "Epoch 13/200\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 0.7044 - acc: 0.8545 - val_loss: 0.7968 - val_acc: 0.8242\n",
      "Epoch 14/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.7092 - acc: 0.8528 - val_loss: 0.7354 - val_acc: 0.8470\n",
      "Epoch 15/200\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 0.7125 - acc: 0.8527 - val_loss: 0.7825 - val_acc: 0.8318\n",
      "Epoch 16/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.6999 - acc: 0.8560 - val_loss: 0.7022 - val_acc: 0.8576\n",
      "Epoch 17/200\n",
      "45000/45000 [==============================] - 20s 438us/step - loss: 0.6975 - acc: 0.8579 - val_loss: 0.7852 - val_acc: 0.8290\n",
      "Epoch 18/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.6998 - acc: 0.8573 - val_loss: 0.7868 - val_acc: 0.8328\n",
      "Epoch 19/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.7031 - acc: 0.8541 - val_loss: 0.7389 - val_acc: 0.8460\n",
      "Epoch 20/200\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 0.7007 - acc: 0.8564 - val_loss: 0.8267 - val_acc: 0.8204\n",
      "Epoch 21/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.7089 - acc: 0.8561 - val_loss: 0.7052 - val_acc: 0.8552\n",
      "Epoch 22/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.7000 - acc: 0.8568 - val_loss: 0.7573 - val_acc: 0.8472\n",
      "Epoch 23/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.7071 - acc: 0.8536 - val_loss: 0.7310 - val_acc: 0.8448\n",
      "Epoch 24/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.7032 - acc: 0.8558 - val_loss: 0.7293 - val_acc: 0.8508\n",
      "Epoch 25/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6944 - acc: 0.8598 - val_loss: 0.7641 - val_acc: 0.8446\n",
      "Epoch 26/200\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 0.7045 - acc: 0.8556 - val_loss: 0.7286 - val_acc: 0.8520\n",
      "Epoch 27/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.7050 - acc: 0.8556 - val_loss: 0.7124 - val_acc: 0.8568\n",
      "Epoch 28/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.6960 - acc: 0.8578 - val_loss: 0.7164 - val_acc: 0.8518\n",
      "Epoch 29/200\n",
      "45000/45000 [==============================] - 20s 446us/step - loss: 0.6968 - acc: 0.8582 - val_loss: 0.8807 - val_acc: 0.8130\n",
      "Epoch 30/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6964 - acc: 0.8573 - val_loss: 0.7134 - val_acc: 0.8538\n",
      "Epoch 31/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.7068 - acc: 0.8554 - val_loss: 0.9913 - val_acc: 0.7842\n",
      "Epoch 32/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6988 - acc: 0.8599 - val_loss: 0.7243 - val_acc: 0.8524\n",
      "Epoch 33/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.6881 - acc: 0.8606 - val_loss: 0.7265 - val_acc: 0.8506\n",
      "Epoch 34/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6940 - acc: 0.8609 - val_loss: 0.7836 - val_acc: 0.8314\n",
      "Epoch 35/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.7000 - acc: 0.8575 - val_loss: 0.7424 - val_acc: 0.8482\n",
      "Epoch 36/200\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 0.6863 - acc: 0.8624 - val_loss: 0.7548 - val_acc: 0.8446\n",
      "Epoch 37/200\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 0.6965 - acc: 0.8582 - val_loss: 0.7322 - val_acc: 0.8546\n",
      "Epoch 38/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.6938 - acc: 0.8610 - val_loss: 0.6969 - val_acc: 0.8610\n",
      "Epoch 39/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6941 - acc: 0.8586 - val_loss: 0.7096 - val_acc: 0.8606\n",
      "Epoch 40/200\n",
      "45000/45000 [==============================] - 20s 446us/step - loss: 0.6929 - acc: 0.8613 - val_loss: 0.7303 - val_acc: 0.8522\n",
      "Epoch 41/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.7002 - acc: 0.8572 - val_loss: 0.7573 - val_acc: 0.8470\n",
      "Epoch 42/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.7007 - acc: 0.8585 - val_loss: 0.8276 - val_acc: 0.8252\n",
      "Epoch 43/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.6909 - acc: 0.8596 - val_loss: 0.7430 - val_acc: 0.8444\n",
      "Epoch 44/200\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 0.6942 - acc: 0.8591 - val_loss: 0.7633 - val_acc: 0.8400\n",
      "Epoch 45/200\n",
      "45000/45000 [==============================] - 20s 445us/step - loss: 0.6904 - acc: 0.8592 - val_loss: 0.7400 - val_acc: 0.8530\n",
      "Epoch 46/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6998 - acc: 0.8578 - val_loss: 0.7299 - val_acc: 0.8530\n",
      "Epoch 47/200\n",
      "45000/45000 [==============================] - 20s 446us/step - loss: 0.6890 - acc: 0.8612 - val_loss: 0.7103 - val_acc: 0.8542\n",
      "Epoch 48/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.6945 - acc: 0.8592 - val_loss: 0.7557 - val_acc: 0.8448\n",
      "Epoch 49/200\n",
      "45000/45000 [==============================] - 20s 444us/step - loss: 0.6926 - acc: 0.8606 - val_loss: 0.7727 - val_acc: 0.8424\n",
      "Epoch 50/200\n",
      "45000/45000 [==============================] - 20s 446us/step - loss: 0.7000 - acc: 0.8580 - val_loss: 0.7253 - val_acc: 0.8508\n",
      "Epoch 51/200\n",
      "45000/45000 [==============================] - 19s 412us/step - loss: 0.6958 - acc: 0.8588 - val_loss: 0.9299 - val_acc: 0.7984\n",
      "Epoch 52/200\n",
      "45000/45000 [==============================] - 20s 441us/step - loss: 0.6971 - acc: 0.8596 - val_loss: 0.7437 - val_acc: 0.8424\n",
      "Epoch 53/200\n",
      "45000/45000 [==============================] - 20s 443us/step - loss: 0.6879 - acc: 0.8608 - val_loss: 0.7024 - val_acc: 0.8590\n",
      "Epoch 54/200\n",
      "45000/45000 [==============================] - 20s 455us/step - loss: 0.6883 - acc: 0.8637 - val_loss: 0.8222 - val_acc: 0.8154\n",
      "Epoch 55/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6869 - acc: 0.8626 - val_loss: 0.6969 - val_acc: 0.8576\n",
      "Epoch 56/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6912 - acc: 0.8618 - val_loss: 0.7480 - val_acc: 0.8380\n",
      "Epoch 57/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6872 - acc: 0.8609 - val_loss: 0.7581 - val_acc: 0.8466\n",
      "Epoch 58/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6860 - acc: 0.8621 - val_loss: 0.7408 - val_acc: 0.8476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6852 - acc: 0.8615 - val_loss: 0.7697 - val_acc: 0.8366\n",
      "Epoch 60/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6840 - acc: 0.8611 - val_loss: 0.7611 - val_acc: 0.8350\n",
      "Epoch 61/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6898 - acc: 0.8605 - val_loss: 0.7550 - val_acc: 0.8476\n",
      "Epoch 62/200\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 0.6913 - acc: 0.8612 - val_loss: 0.7397 - val_acc: 0.8500\n",
      "Epoch 63/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6939 - acc: 0.8602 - val_loss: 0.7511 - val_acc: 0.8378\n",
      "Epoch 64/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6958 - acc: 0.8589 - val_loss: 0.7139 - val_acc: 0.8544\n",
      "Epoch 65/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6925 - acc: 0.8621 - val_loss: 0.8317 - val_acc: 0.8246\n",
      "Epoch 66/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6797 - acc: 0.8652 - val_loss: 0.7750 - val_acc: 0.8422\n",
      "Epoch 67/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6863 - acc: 0.8638 - val_loss: 0.7160 - val_acc: 0.8530\n",
      "Epoch 68/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6922 - acc: 0.8602 - val_loss: 0.7612 - val_acc: 0.8386\n",
      "Epoch 69/200\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 0.6898 - acc: 0.8584 - val_loss: 0.7076 - val_acc: 0.8562\n",
      "Epoch 70/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6896 - acc: 0.8611 - val_loss: 0.7249 - val_acc: 0.8582\n",
      "Epoch 71/200\n",
      "45000/45000 [==============================] - 20s 454us/step - loss: 0.6862 - acc: 0.8625 - val_loss: 0.7486 - val_acc: 0.8436\n",
      "Epoch 72/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6840 - acc: 0.8626 - val_loss: 0.7268 - val_acc: 0.8526\n",
      "Epoch 73/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6827 - acc: 0.8630 - val_loss: 0.7227 - val_acc: 0.8512\n",
      "Epoch 74/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6804 - acc: 0.8638 - val_loss: 0.7433 - val_acc: 0.8410\n",
      "Epoch 75/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6771 - acc: 0.8659 - val_loss: 0.7992 - val_acc: 0.8348\n",
      "Epoch 76/200\n",
      "45000/45000 [==============================] - 20s 454us/step - loss: 0.6849 - acc: 0.8608 - val_loss: 0.7087 - val_acc: 0.8594\n",
      "Epoch 77/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6764 - acc: 0.8662 - val_loss: 0.7327 - val_acc: 0.8520\n",
      "Epoch 78/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6783 - acc: 0.8662 - val_loss: 0.7407 - val_acc: 0.8532\n",
      "Epoch 79/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6810 - acc: 0.8624 - val_loss: 0.7619 - val_acc: 0.8418\n",
      "Epoch 80/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6840 - acc: 0.8619 - val_loss: 0.8079 - val_acc: 0.8348\n",
      "Epoch 81/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6870 - acc: 0.8617 - val_loss: 0.7128 - val_acc: 0.8578\n",
      "Epoch 82/200\n",
      "45000/45000 [==============================] - 20s 454us/step - loss: 0.6863 - acc: 0.8617 - val_loss: 0.8066 - val_acc: 0.8314\n",
      "Epoch 83/200\n",
      "45000/45000 [==============================] - 20s 454us/step - loss: 0.6837 - acc: 0.8625 - val_loss: 0.7284 - val_acc: 0.8498\n",
      "Epoch 84/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6845 - acc: 0.8612 - val_loss: 0.7184 - val_acc: 0.8566\n",
      "Epoch 85/200\n",
      "45000/45000 [==============================] - 20s 454us/step - loss: 0.6785 - acc: 0.8648 - val_loss: 0.6923 - val_acc: 0.8618\n",
      "Epoch 86/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6779 - acc: 0.8651 - val_loss: 0.7461 - val_acc: 0.8442\n",
      "Epoch 87/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6866 - acc: 0.8616 - val_loss: 0.7255 - val_acc: 0.8480\n",
      "Epoch 88/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6699 - acc: 0.8670 - val_loss: 0.7429 - val_acc: 0.8488\n",
      "Epoch 89/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6830 - acc: 0.8634 - val_loss: 0.7575 - val_acc: 0.8450\n",
      "Epoch 90/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6829 - acc: 0.8617 - val_loss: 0.8195 - val_acc: 0.8170\n",
      "Epoch 91/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6829 - acc: 0.8616 - val_loss: 0.7629 - val_acc: 0.8432\n",
      "Epoch 92/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6852 - acc: 0.8628 - val_loss: 0.7254 - val_acc: 0.8542\n",
      "Epoch 93/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6832 - acc: 0.8629 - val_loss: 0.7129 - val_acc: 0.8566\n",
      "Epoch 94/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6875 - acc: 0.8618 - val_loss: 0.6922 - val_acc: 0.8598\n",
      "Epoch 95/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6851 - acc: 0.8632 - val_loss: 0.7241 - val_acc: 0.8512\n",
      "Epoch 96/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6779 - acc: 0.8651 - val_loss: 0.7100 - val_acc: 0.8570\n",
      "Epoch 97/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6862 - acc: 0.8632 - val_loss: 0.8519 - val_acc: 0.8224\n",
      "Epoch 98/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6888 - acc: 0.8629 - val_loss: 0.6965 - val_acc: 0.8616\n",
      "Epoch 99/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6789 - acc: 0.8632 - val_loss: 0.7182 - val_acc: 0.8540\n",
      "Epoch 100/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6817 - acc: 0.8640 - val_loss: 0.7481 - val_acc: 0.8492\n",
      "Epoch 101/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6823 - acc: 0.8634 - val_loss: 0.7275 - val_acc: 0.8560\n",
      "Epoch 102/200\n",
      "45000/45000 [==============================] - 20s 455us/step - loss: 0.6972 - acc: 0.8606 - val_loss: 0.7043 - val_acc: 0.8604\n",
      "Epoch 103/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6867 - acc: 0.8627 - val_loss: 0.8004 - val_acc: 0.8340\n",
      "Epoch 104/200\n",
      "45000/45000 [==============================] - 20s 454us/step - loss: 0.6874 - acc: 0.8623 - val_loss: 0.7040 - val_acc: 0.8646\n",
      "Epoch 105/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6919 - acc: 0.8623 - val_loss: 0.7327 - val_acc: 0.8480\n",
      "Epoch 106/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6843 - acc: 0.8628 - val_loss: 0.7939 - val_acc: 0.8282\n",
      "Epoch 107/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6819 - acc: 0.8637 - val_loss: 0.7381 - val_acc: 0.8456\n",
      "Epoch 108/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6745 - acc: 0.8664 - val_loss: 0.7054 - val_acc: 0.8614\n",
      "Epoch 109/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6784 - acc: 0.8655 - val_loss: 0.7630 - val_acc: 0.8444\n",
      "Epoch 110/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6813 - acc: 0.8642 - val_loss: 0.7145 - val_acc: 0.8552\n",
      "Epoch 111/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6742 - acc: 0.8690 - val_loss: 0.6977 - val_acc: 0.8626\n",
      "Epoch 112/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6829 - acc: 0.8635 - val_loss: 0.8102 - val_acc: 0.8330\n",
      "Epoch 113/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6736 - acc: 0.8674 - val_loss: 0.8317 - val_acc: 0.8166\n",
      "Epoch 114/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6669 - acc: 0.8682 - val_loss: 0.7206 - val_acc: 0.8484\n",
      "Epoch 115/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6742 - acc: 0.8657 - val_loss: 0.7965 - val_acc: 0.8320\n",
      "Epoch 116/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6762 - acc: 0.8666 - val_loss: 0.7392 - val_acc: 0.8502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6771 - acc: 0.8629 - val_loss: 1.0303 - val_acc: 0.7678\n",
      "Epoch 118/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6780 - acc: 0.8630 - val_loss: 0.7077 - val_acc: 0.8574\n",
      "Epoch 119/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6789 - acc: 0.8645 - val_loss: 0.6949 - val_acc: 0.8610\n",
      "Epoch 120/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6808 - acc: 0.8660 - val_loss: 0.7202 - val_acc: 0.8544\n",
      "Epoch 121/200\n",
      "45000/45000 [==============================] - 20s 442us/step - loss: 0.6714 - acc: 0.8660 - val_loss: 0.7025 - val_acc: 0.8562\n",
      "Epoch 122/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6767 - acc: 0.8653 - val_loss: 0.7418 - val_acc: 0.8488\n",
      "Epoch 123/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6743 - acc: 0.8669 - val_loss: 0.7762 - val_acc: 0.8352\n",
      "Epoch 124/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6765 - acc: 0.8679 - val_loss: 0.7127 - val_acc: 0.8570\n",
      "Epoch 125/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6729 - acc: 0.8656 - val_loss: 0.6731 - val_acc: 0.8740\n",
      "Epoch 126/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6838 - acc: 0.8639 - val_loss: 0.7216 - val_acc: 0.8556\n",
      "Epoch 127/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6695 - acc: 0.8682 - val_loss: 0.7756 - val_acc: 0.8400\n",
      "Epoch 128/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6758 - acc: 0.8654 - val_loss: 0.6792 - val_acc: 0.8696\n",
      "Epoch 129/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6773 - acc: 0.8639 - val_loss: 0.7315 - val_acc: 0.8476\n",
      "Epoch 130/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6740 - acc: 0.8670 - val_loss: 0.7723 - val_acc: 0.8390\n",
      "Epoch 131/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6797 - acc: 0.8654 - val_loss: 0.7656 - val_acc: 0.8418\n",
      "Epoch 132/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6912 - acc: 0.8610 - val_loss: 0.7212 - val_acc: 0.8552\n",
      "Epoch 133/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6772 - acc: 0.8662 - val_loss: 0.7044 - val_acc: 0.8592\n",
      "Epoch 134/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.6788 - acc: 0.8623 - val_loss: 0.7002 - val_acc: 0.8620\n",
      "Epoch 135/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6714 - acc: 0.8679 - val_loss: 0.7377 - val_acc: 0.8574\n",
      "Epoch 136/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.6744 - acc: 0.8643 - val_loss: 0.8041 - val_acc: 0.8302\n",
      "Epoch 137/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6718 - acc: 0.8669 - val_loss: 0.7696 - val_acc: 0.8452\n",
      "Epoch 138/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6738 - acc: 0.8668 - val_loss: 0.7475 - val_acc: 0.8402\n",
      "Epoch 139/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6771 - acc: 0.8640 - val_loss: 0.7366 - val_acc: 0.8480\n",
      "Epoch 140/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6737 - acc: 0.8666 - val_loss: 0.7004 - val_acc: 0.8642\n",
      "Epoch 141/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6794 - acc: 0.8634 - val_loss: 0.7201 - val_acc: 0.8500\n",
      "Epoch 142/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6755 - acc: 0.8655 - val_loss: 0.7285 - val_acc: 0.8580\n",
      "Epoch 143/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6665 - acc: 0.8709 - val_loss: 0.9133 - val_acc: 0.7994\n",
      "Epoch 144/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6709 - acc: 0.8657 - val_loss: 0.7797 - val_acc: 0.8364\n",
      "Epoch 145/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6838 - acc: 0.8634 - val_loss: 0.7867 - val_acc: 0.8358\n",
      "Epoch 146/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6788 - acc: 0.8656 - val_loss: 0.7209 - val_acc: 0.8576\n",
      "Epoch 147/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6684 - acc: 0.8687 - val_loss: 0.8190 - val_acc: 0.8238\n",
      "Epoch 148/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6734 - acc: 0.8669 - val_loss: 0.7284 - val_acc: 0.8530\n",
      "Epoch 149/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6762 - acc: 0.8653 - val_loss: 0.8594 - val_acc: 0.8154\n",
      "Epoch 150/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6827 - acc: 0.8642 - val_loss: 0.7319 - val_acc: 0.8544\n",
      "Epoch 151/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.6816 - acc: 0.8641 - val_loss: 0.7435 - val_acc: 0.8470\n",
      "Epoch 152/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6695 - acc: 0.8663 - val_loss: 0.7130 - val_acc: 0.8576\n",
      "Epoch 153/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6837 - acc: 0.8644 - val_loss: 0.8037 - val_acc: 0.8214\n",
      "Epoch 154/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6786 - acc: 0.8655 - val_loss: 0.7219 - val_acc: 0.8486\n",
      "Epoch 155/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6779 - acc: 0.8660 - val_loss: 0.8668 - val_acc: 0.8142\n",
      "Epoch 156/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6763 - acc: 0.8659 - val_loss: 0.7167 - val_acc: 0.8486\n",
      "Epoch 157/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6690 - acc: 0.8684 - val_loss: 0.6768 - val_acc: 0.8636\n",
      "Epoch 158/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6728 - acc: 0.8676 - val_loss: 0.8111 - val_acc: 0.8336\n",
      "Epoch 159/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6666 - acc: 0.8701 - val_loss: 0.7225 - val_acc: 0.8456\n",
      "Epoch 160/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6703 - acc: 0.8662 - val_loss: 0.8278 - val_acc: 0.8246\n",
      "Epoch 161/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6761 - acc: 0.8662 - val_loss: 0.7261 - val_acc: 0.8484\n",
      "Epoch 162/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6772 - acc: 0.8644 - val_loss: 0.7438 - val_acc: 0.8448\n",
      "Epoch 163/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6690 - acc: 0.8682 - val_loss: 0.7390 - val_acc: 0.8496\n",
      "Epoch 164/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6768 - acc: 0.8660 - val_loss: 0.6989 - val_acc: 0.8612\n",
      "Epoch 165/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6668 - acc: 0.8688 - val_loss: 0.7947 - val_acc: 0.8322\n",
      "Epoch 166/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6771 - acc: 0.8649 - val_loss: 0.7775 - val_acc: 0.8406\n",
      "Epoch 167/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6741 - acc: 0.8650 - val_loss: 0.7292 - val_acc: 0.8542\n",
      "Epoch 168/200\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 0.6772 - acc: 0.8639 - val_loss: 0.7069 - val_acc: 0.8568\n",
      "Epoch 169/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6750 - acc: 0.8669 - val_loss: 0.7210 - val_acc: 0.8532\n",
      "Epoch 170/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6731 - acc: 0.8657 - val_loss: 0.7496 - val_acc: 0.8436\n",
      "Epoch 171/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6732 - acc: 0.8665 - val_loss: 0.7489 - val_acc: 0.8472\n",
      "Epoch 172/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6727 - acc: 0.8652 - val_loss: 0.7249 - val_acc: 0.8514\n",
      "Epoch 173/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6737 - acc: 0.8667 - val_loss: 0.7148 - val_acc: 0.8560\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6608 - acc: 0.8695 - val_loss: 0.7629 - val_acc: 0.8422\n",
      "Epoch 175/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6780 - acc: 0.8670 - val_loss: 0.7074 - val_acc: 0.8576\n",
      "Epoch 176/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6690 - acc: 0.8670 - val_loss: 0.6947 - val_acc: 0.8596\n",
      "Epoch 177/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6770 - acc: 0.8639 - val_loss: 0.8638 - val_acc: 0.8066\n",
      "Epoch 178/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6709 - acc: 0.8665 - val_loss: 0.7349 - val_acc: 0.8450\n",
      "Epoch 179/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6647 - acc: 0.8691 - val_loss: 0.7733 - val_acc: 0.8412\n",
      "Epoch 180/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6700 - acc: 0.8684 - val_loss: 0.7795 - val_acc: 0.8326\n",
      "Epoch 181/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6735 - acc: 0.8666 - val_loss: 0.6856 - val_acc: 0.8670\n",
      "Epoch 182/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6687 - acc: 0.8667 - val_loss: 0.6981 - val_acc: 0.8576\n",
      "Epoch 183/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6716 - acc: 0.8665 - val_loss: 0.7983 - val_acc: 0.8400\n",
      "Epoch 184/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6726 - acc: 0.8656 - val_loss: 0.9002 - val_acc: 0.8038\n",
      "Epoch 185/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6667 - acc: 0.8688 - val_loss: 0.7751 - val_acc: 0.8410\n",
      "Epoch 186/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6739 - acc: 0.8663 - val_loss: 0.8857 - val_acc: 0.8124\n",
      "Epoch 187/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6683 - acc: 0.8674 - val_loss: 0.6709 - val_acc: 0.8696\n",
      "Epoch 188/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6671 - acc: 0.8676 - val_loss: 0.7516 - val_acc: 0.8424\n",
      "Epoch 189/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6658 - acc: 0.8698 - val_loss: 0.7647 - val_acc: 0.8412\n",
      "Epoch 190/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6629 - acc: 0.8681 - val_loss: 0.8632 - val_acc: 0.8170\n",
      "Epoch 191/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6655 - acc: 0.8692 - val_loss: 0.7405 - val_acc: 0.8422\n",
      "Epoch 192/200\n",
      "45000/45000 [==============================] - 20s 448us/step - loss: 0.6684 - acc: 0.8685 - val_loss: 0.7685 - val_acc: 0.8446\n",
      "Epoch 193/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6737 - acc: 0.8664 - val_loss: 0.7647 - val_acc: 0.8454\n",
      "Epoch 194/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6662 - acc: 0.8683 - val_loss: 0.7404 - val_acc: 0.8486\n",
      "Epoch 195/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6681 - acc: 0.8682 - val_loss: 0.6852 - val_acc: 0.8584\n",
      "Epoch 196/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6675 - acc: 0.8686 - val_loss: 0.7016 - val_acc: 0.8596\n",
      "Epoch 197/200\n",
      "45000/45000 [==============================] - 20s 455us/step - loss: 0.6611 - acc: 0.8698 - val_loss: 0.6855 - val_acc: 0.8658\n",
      "Epoch 198/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6637 - acc: 0.8692 - val_loss: 0.7393 - val_acc: 0.8524\n",
      "Epoch 199/200\n",
      "45000/45000 [==============================] - 20s 450us/step - loss: 0.6727 - acc: 0.8647 - val_loss: 0.7315 - val_acc: 0.8452\n",
      "Epoch 200/200\n",
      "45000/45000 [==============================] - 20s 449us/step - loss: 0.6670 - acc: 0.8689 - val_loss: 0.6712 - val_acc: 0.8700\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    " \n",
    "train_history=model_3.fit(x_train,y_train,epochs=200, batch_size=128,\n",
    "             validation_split=0.1, verbose=1)\n",
    "\n",
    "model_3.save('8.7.6ent_b_cifar10vgg16_3_1.h5')\n",
    "model_3.save_weights('8.7.6ent_b_cifar10vgg16_3_1_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6963983918190002, 0.8629, 0.9935, 0.8629]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import keras\n",
    "top1_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=1) #top-1精度\n",
    "\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "\n",
    "model_3.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',metrics=['accuracy','top_k_categorical_accuracy',top1_acc])\n",
    "\n",
    "model_3.evaluate(x_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试损失为：0.6964\n",
      "测试准确率为：0.8629\n"
     ]
    }
   ],
   "source": [
    "score=model_3.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"测试损失为：{:.4f}\".format(score[0]))\n",
    "print(\"测试准确率为：{:.4f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [9.573772026232913, 10.96541299623161, 12.776673446608358]\n",
      "1 [22.405420774813305, 5.969915545839044, 19.414438802038752]\n",
      "2 [8.708007018502192, 26.388167217715356, 1.8701771061713022]\n",
      "3 [11.543836470002812, 17.628113005035058, 12.161449448162267]\n",
      "4 [45.49184170617252, 7.205237287840316, 33.73041056333646]\n",
      "5 [6.949131433840055, 4.202001400200648, 11.651645824926833]\n",
      "6 [60.01505268740723, 55.95647428278658, 20.23519638927672]\n",
      "7 [23.658302167724347, 8.344651077134447, 7.403995460487223]\n",
      "8 [22.25741629283032, 33.18383290348912, 21.0102904402182]\n",
      "9 [17.399949006849962, 5.7699290805405195, 6.941863242987976]\n",
      "10 [8.543137983434653, 10.481809708738403, 14.811451452300018]\n",
      "11 [21.97196228287795, 16.487734079036912, 32.3550582209408]\n",
      "12 [4.047300780296161, 10.892429470493191, 5.736247866429198]\n",
      "13 [20.825539052793975, 43.333540915334346, 30.481851869280142]\n",
      "14 [16.10346476845839, 6.390828164375826, 16.063155580196675]\n",
      "15 [9.812689627497713, 4.642228745284021, 8.591012706035409]\n",
      "16 [8.865158937969069, 2.535308692239963, 8.48426681084264]\n",
      "17 [15.403544732022947, 7.531897552636268, 7.126352114960371]\n",
      "18 [20.078351505649504, 26.11246326566297, 9.40224896372494]\n",
      "19 [43.71683834496453, 24.526531100916785, 34.6342355688342]\n",
      "20 [33.61358880052502, 25.249095296231868, 5.764215023000997]\n",
      "21 [16.335575626986248, 16.952193639811345, 9.799922000808161]\n",
      "22 [10.612798618747776, 10.253746557536251, 14.9440674968641]\n",
      "23 [6.877930484778724, 17.151103263935124, 8.003214395240525]\n",
      "24 [18.111939953766825, 20.850350428684944, 8.651413650982155]\n",
      "25 [7.507642825429295, 7.498608269309879, 12.718820313955238]\n",
      "26 [5.55839987528641, 3.040059610083547, 9.848974361369653]\n",
      "27 [1.0157967055499961, 2.5299815072450538, 11.50956734448143]\n",
      "28 [13.727816703003956, 10.849560854414458, 18.353734537061094]\n",
      "29 [13.370576216065237, 11.130099381571233, 22.08806794952419]\n",
      "30 [36.62063046581955, 10.942069706656598, 41.40582149643768]\n",
      "31 [19.366862326258484, 15.12118517631971, 0.9500359678988547]\n",
      "32 [33.41981771806354, 54.67289980189273, 27.815751749317126]\n",
      "33 [27.38810479015629, 10.067004788848626, 8.632153626581848]\n",
      "34 [10.415271502841646, 11.50966126507764, 17.101656394620573]\n",
      "35 [22.117302099622936, 21.75529223763729, 26.285904035675618]\n",
      "36 [28.64905919632393, 25.852327226548958, 13.191188206111779]\n",
      "37 [9.469913373428076, 6.458351023454947, 10.38532058584649]\n",
      "38 [20.880833376096753, 21.135576343136883, 20.130691292230285]\n",
      "39 [9.78071295388176, 20.620706192192984, 20.215524737548336]\n",
      "40 [33.82805409765412, 9.922560396325416, 27.205815944105403]\n",
      "41 [17.586926683680538, 12.006270598156092, 13.666302200241416]\n",
      "42 [16.405432835164522, 13.470937900784353, 19.88916552658782]\n",
      "43 [29.766390965335834, 10.886344517249222, 27.524182732113967]\n",
      "44 [15.64900908090456, 14.182301556204006, 9.365412578140328]\n",
      "45 [18.378048287127612, 39.54269094048324, 4.962498985309641]\n",
      "46 [2.979978348570133, 9.829965779893984, 25.312486210572054]\n",
      "47 [7.248925359597694, 17.809431514459554, 11.19972404029314]\n",
      "48 [8.875218455890002, 6.992290406562052, 15.15335426589811]\n",
      "49 [1.695146543756489, 6.597505160059037, 12.754835969415266]\n",
      "50 [14.34397040303757, 12.136032826685405, 5.072187973209519]\n",
      "51 [9.436233789245854, 17.133057988708966, 8.014619269677604]\n",
      "52 [13.735406986562769, 16.16553714417868, 18.274256433057744]\n",
      "53 [7.9989477001950275, 14.88684613692094, 9.12976997282091]\n",
      "54 [23.139599001417704, 30.28722686361033, 39.28137420658938]\n",
      "55 [6.678039847274215, 32.08338656617162, 11.19931778533012]\n",
      "56 [11.07465126229083, 8.890560532052897, 14.463234165188982]\n",
      "57 [3.216119487618507, 3.6675907545555857, 10.18892541520262]\n",
      "58 [13.04468160683011, 13.038370990644342, 8.11888886943042]\n",
      "59 [12.26246647105946, 23.260094512425994, 7.715923513831079]\n",
      "60 [19.809330106035336, 27.36096577183505, 27.18097245352504]\n",
      "61 [6.821324259203665, 2.5078398801075426, 2.4893052226445826]\n",
      "62 [10.666889662696496, 42.08251302442098, 21.834037378719266]\n",
      "63 [14.080451933912064, 18.495921175029782, 10.63183987308113]\n",
      "[[9.573772026232913, 10.96541299623161, 12.776673446608358], [22.405420774813305, 5.969915545839044, 19.414438802038752], [8.708007018502192, 26.388167217715356, 1.8701771061713022], [11.543836470002812, 17.628113005035058, 12.161449448162267], [45.49184170617252, 7.205237287840316, 33.73041056333646], [6.949131433840055, 4.202001400200648, 11.651645824926833], [60.01505268740723, 55.95647428278658, 20.23519638927672], [23.658302167724347, 8.344651077134447, 7.403995460487223], [22.25741629283032, 33.18383290348912, 21.0102904402182], [17.399949006849962, 5.7699290805405195, 6.941863242987976], [8.543137983434653, 10.481809708738403, 14.811451452300018], [21.97196228287795, 16.487734079036912, 32.3550582209408], [4.047300780296161, 10.892429470493191, 5.736247866429198], [20.825539052793975, 43.333540915334346, 30.481851869280142], [16.10346476845839, 6.390828164375826, 16.063155580196675], [9.812689627497713, 4.642228745284021, 8.591012706035409], [8.865158937969069, 2.535308692239963, 8.48426681084264], [15.403544732022947, 7.531897552636268, 7.126352114960371], [20.078351505649504, 26.11246326566297, 9.40224896372494], [43.71683834496453, 24.526531100916785, 34.6342355688342], [33.61358880052502, 25.249095296231868, 5.764215023000997], [16.335575626986248, 16.952193639811345, 9.799922000808161], [10.612798618747776, 10.253746557536251, 14.9440674968641], [6.877930484778724, 17.151103263935124, 8.003214395240525], [18.111939953766825, 20.850350428684944, 8.651413650982155], [7.507642825429295, 7.498608269309879, 12.718820313955238], [5.55839987528641, 3.040059610083547, 9.848974361369653], [1.0157967055499961, 2.5299815072450538, 11.50956734448143], [13.727816703003956, 10.849560854414458, 18.353734537061094], [13.370576216065237, 11.130099381571233, 22.08806794952419], [36.62063046581955, 10.942069706656598, 41.40582149643768], [19.366862326258484, 15.12118517631971, 0.9500359678988547], [33.41981771806354, 54.67289980189273, 27.815751749317126], [27.38810479015629, 10.067004788848626, 8.632153626581848], [10.415271502841646, 11.50966126507764, 17.101656394620573], [22.117302099622936, 21.75529223763729, 26.285904035675618], [28.64905919632393, 25.852327226548958, 13.191188206111779], [9.469913373428076, 6.458351023454947, 10.38532058584649], [20.880833376096753, 21.135576343136883, 20.130691292230285], [9.78071295388176, 20.620706192192984, 20.215524737548336], [33.82805409765412, 9.922560396325416, 27.205815944105403], [17.586926683680538, 12.006270598156092, 13.666302200241416], [16.405432835164522, 13.470937900784353, 19.88916552658782], [29.766390965335834, 10.886344517249222, 27.524182732113967], [15.64900908090456, 14.182301556204006, 9.365412578140328], [18.378048287127612, 39.54269094048324, 4.962498985309641], [2.979978348570133, 9.829965779893984, 25.312486210572054], [7.248925359597694, 17.809431514459554, 11.19972404029314], [8.875218455890002, 6.992290406562052, 15.15335426589811], [1.695146543756489, 6.597505160059037, 12.754835969415266], [14.34397040303757, 12.136032826685405, 5.072187973209519], [9.436233789245854, 17.133057988708966, 8.014619269677604], [13.735406986562769, 16.16553714417868, 18.274256433057744], [7.9989477001950275, 14.88684613692094, 9.12976997282091], [23.139599001417704, 30.28722686361033, 39.28137420658938], [6.678039847274215, 32.08338656617162, 11.19931778533012], [11.07465126229083, 8.890560532052897, 14.463234165188982], [3.216119487618507, 3.6675907545555857, 10.18892541520262], [13.04468160683011, 13.038370990644342, 8.11888886943042], [12.26246647105946, 23.260094512425994, 7.715923513831079], [19.809330106035336, 27.36096577183505, 27.18097245352504], [6.821324259203665, 2.5078398801075426, 2.4893052226445826], [10.666889662696496, 42.08251302442098, 21.834037378719266], [14.080451933912064, 18.495921175029782, 10.63183987308113]]\n"
     ]
    }
   ],
   "source": [
    "tensor =[]\n",
    "for i in range(0,64):\n",
    "   \n",
    "    u=model_3.get_layer('conv2d_2').get_weights()[0][:,:,:,i].squeeze()\n",
    "    v=np.mean(u, axis=1)\n",
    "    #print(v)\n",
    "    vT=v.T\n",
    "    D=np.cov(vT)\n",
    "    try:                 \n",
    "        invD=np.linalg.inv(D)\n",
    "        #t=np.mean(v,axis=1)\n",
    "        a=[]\n",
    "\n",
    "        for j in range(0,2): #表示filter的大小\n",
    "            for k in range(j+1,3):\n",
    "               #if (j!=k) and (j<k):\n",
    "                tp=v[j]-v[k]\n",
    "                d=np.sqrt(abs(np.dot(np.dot(tp,invD),tp.T)))\n",
    "                a.append(d)\n",
    "            \n",
    "        print(i,a)\n",
    "        tensor.append(a)\n",
    "    except:\n",
    "        print(\"不可逆\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{50.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [100. 100.   0.]\n",
      "{60.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{50.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "[0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896]\n"
     ]
    }
   ],
   "source": [
    "entt=[]\n",
    "for i in range (0,64):\n",
    "    \n",
    "    data=tensor[i]\n",
    "    data0=np.array(data)\n",
    "    print('四舍五入，精确到个位\\n',np.round(data0,decimals=-2))\n",
    "    data1=np.round(data0,decimals=-1)\n",
    "\n",
    "    data1_value_list=set([data1[i] for i in range (data1.shape[0])])\n",
    "    print(data1_value_list)\n",
    "    ent=0.0\n",
    "    for data1_value in data1_value_list:\n",
    "        p=float(data1[data1==data1_value].shape[0])/data1.shape[0]\n",
    "        print(p)\n",
    "        logp=np.log2(p)\n",
    "        ent-=p*logp\n",
    "        print(ent)   \n",
    "    entt.append(ent)\n",
    "print(entt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896]\n",
      "[0, 10, 22, 25, 37, 38, 50, 53, 56, 58, 1, 3, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 20, 21, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n"
     ]
    }
   ],
   "source": [
    "import heapq #获取list中最小的\n",
    "\n",
    "f=int(len(entt)*0.6) #计算滤波器熵个数的80%\n",
    "m=entt\n",
    "max_number=heapq.nsmallest(f,m) #从m中找出最小的f个数，最大用nlargest\n",
    "max_index=[]\n",
    "for t in max_number:\n",
    "    index=m.index(t)\n",
    "    max_index.append(index)\n",
    "    m[index]=float('-inf')\n",
    "print(max_number)#输出最小的f个数\n",
    "print(max_index)#输出对应索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 38/64 channels from layer: conv2d_2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 26)        15002     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 26)        104       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 39)        9165      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 39)        156       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 39)        13728     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 39)        156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 52)          18304     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 103)         48307     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               53248     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 961,892\n",
      "Trainable params: 957,960\n",
      "Non-trainable params: 3,932\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerassurgeon.operations import delete_layer,insert_layer,delete_channels\n",
    "model_2 = delete_channels(model_3, model_3.layers[5],max_index)\n",
    "model_2.summary()\n",
    "model_2.save('8.7.6ent_b_cifar10vgg16_2.h5')\n",
    "model_2.save_weights('8.7.6ent_b_cifar10vgg16_2_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "45000/45000 [==============================] - 26s 577us/step - loss: 0.8387 - acc: 0.8063 - val_loss: 0.7749 - val_acc: 0.8252\n",
      "Epoch 2/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.7605 - acc: 0.8300 - val_loss: 0.7798 - val_acc: 0.8290\n",
      "Epoch 3/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.7487 - acc: 0.8359 - val_loss: 0.7232 - val_acc: 0.8416\n",
      "Epoch 4/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7544 - acc: 0.8338 - val_loss: 0.8030 - val_acc: 0.8200\n",
      "Epoch 5/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7462 - acc: 0.8375 - val_loss: 0.7388 - val_acc: 0.8358\n",
      "Epoch 6/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7435 - acc: 0.8385 - val_loss: 0.8500 - val_acc: 0.8040\n",
      "Epoch 7/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.7274 - acc: 0.8454 - val_loss: 0.7514 - val_acc: 0.8450\n",
      "Epoch 8/200\n",
      "45000/45000 [==============================] - 19s 422us/step - loss: 0.7291 - acc: 0.8433 - val_loss: 0.7518 - val_acc: 0.8364\n",
      "Epoch 9/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7284 - acc: 0.8449 - val_loss: 0.7869 - val_acc: 0.8212\n",
      "Epoch 10/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.7193 - acc: 0.8471 - val_loss: 0.8039 - val_acc: 0.8202\n",
      "Epoch 11/200\n",
      "45000/45000 [==============================] - 19s 421us/step - loss: 0.7237 - acc: 0.8481 - val_loss: 0.7202 - val_acc: 0.8448\n",
      "Epoch 12/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7288 - acc: 0.8442 - val_loss: 0.9160 - val_acc: 0.7964\n",
      "Epoch 13/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7264 - acc: 0.8457 - val_loss: 0.7831 - val_acc: 0.8312\n",
      "Epoch 14/200\n",
      "45000/45000 [==============================] - 19s 431us/step - loss: 0.7212 - acc: 0.8480 - val_loss: 0.8017 - val_acc: 0.8262\n",
      "Epoch 15/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7215 - acc: 0.8472 - val_loss: 0.8347 - val_acc: 0.8114\n",
      "Epoch 16/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7173 - acc: 0.8502 - val_loss: 0.7455 - val_acc: 0.8410\n",
      "Epoch 17/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7302 - acc: 0.8458 - val_loss: 0.7300 - val_acc: 0.8478\n",
      "Epoch 18/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.7137 - acc: 0.8500 - val_loss: 0.8848 - val_acc: 0.8028\n",
      "Epoch 19/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7184 - acc: 0.8465 - val_loss: 0.7870 - val_acc: 0.8266\n",
      "Epoch 20/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7248 - acc: 0.8453 - val_loss: 0.8138 - val_acc: 0.8178\n",
      "Epoch 21/200\n",
      "45000/45000 [==============================] - 19s 422us/step - loss: 0.7222 - acc: 0.8489 - val_loss: 0.8135 - val_acc: 0.8218\n",
      "Epoch 22/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7223 - acc: 0.8480 - val_loss: 0.7823 - val_acc: 0.8272\n",
      "Epoch 23/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7149 - acc: 0.8510 - val_loss: 0.7998 - val_acc: 0.8212\n",
      "Epoch 24/200\n",
      "45000/45000 [==============================] - 19s 421us/step - loss: 0.7227 - acc: 0.8482 - val_loss: 0.7340 - val_acc: 0.8432\n",
      "Epoch 25/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7210 - acc: 0.8485 - val_loss: 0.8625 - val_acc: 0.8128\n",
      "Epoch 26/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7226 - acc: 0.8486 - val_loss: 0.7457 - val_acc: 0.8370\n",
      "Epoch 27/200\n",
      "45000/45000 [==============================] - 19s 422us/step - loss: 0.7386 - acc: 0.8428 - val_loss: 0.7343 - val_acc: 0.8430\n",
      "Epoch 28/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7115 - acc: 0.8511 - val_loss: 0.7497 - val_acc: 0.8434\n",
      "Epoch 29/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7227 - acc: 0.8509 - val_loss: 0.8149 - val_acc: 0.8222\n",
      "Epoch 30/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7137 - acc: 0.8507 - val_loss: 0.8034 - val_acc: 0.8262\n",
      "Epoch 31/200\n",
      "45000/45000 [==============================] - 19s 421us/step - loss: 0.7128 - acc: 0.8516 - val_loss: 0.7315 - val_acc: 0.8446\n",
      "Epoch 32/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7112 - acc: 0.8528 - val_loss: 0.6919 - val_acc: 0.8564\n",
      "Epoch 33/200\n",
      "45000/45000 [==============================] - 19s 419us/step - loss: 0.7135 - acc: 0.8501 - val_loss: 0.7610 - val_acc: 0.8338\n",
      "Epoch 34/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7194 - acc: 0.8488 - val_loss: 0.7780 - val_acc: 0.8342\n",
      "Epoch 35/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7180 - acc: 0.8498 - val_loss: 0.7167 - val_acc: 0.8474\n",
      "Epoch 36/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7163 - acc: 0.8508 - val_loss: 0.7959 - val_acc: 0.8256\n",
      "Epoch 37/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7130 - acc: 0.8494 - val_loss: 0.7080 - val_acc: 0.8490\n",
      "Epoch 38/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7147 - acc: 0.8518 - val_loss: 0.7104 - val_acc: 0.8484\n",
      "Epoch 39/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7182 - acc: 0.8501 - val_loss: 0.7486 - val_acc: 0.8400\n",
      "Epoch 40/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7127 - acc: 0.8514 - val_loss: 0.7038 - val_acc: 0.8520\n",
      "Epoch 41/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7199 - acc: 0.8477 - val_loss: 0.9225 - val_acc: 0.7962\n",
      "Epoch 42/200\n",
      "45000/45000 [==============================] - 19s 420us/step - loss: 0.7161 - acc: 0.8509 - val_loss: 0.7932 - val_acc: 0.8312\n",
      "Epoch 43/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7212 - acc: 0.8479 - val_loss: 0.6949 - val_acc: 0.8566\n",
      "Epoch 44/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.7120 - acc: 0.8506 - val_loss: 0.7580 - val_acc: 0.8396\n",
      "Epoch 45/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.7137 - acc: 0.8512 - val_loss: 0.7931 - val_acc: 0.8342\n",
      "Epoch 46/200\n",
      "45000/45000 [==============================] - 19s 421us/step - loss: 0.7127 - acc: 0.8509 - val_loss: 0.7660 - val_acc: 0.8320\n",
      "Epoch 47/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.7109 - acc: 0.8515 - val_loss: 0.7241 - val_acc: 0.8510\n",
      "Epoch 48/200\n",
      "45000/45000 [==============================] - 19s 420us/step - loss: 0.7096 - acc: 0.8519 - val_loss: 0.7273 - val_acc: 0.8494\n",
      "Epoch 49/200\n",
      "45000/45000 [==============================] - 19s 422us/step - loss: 0.7108 - acc: 0.8528 - val_loss: 0.8022 - val_acc: 0.8238\n",
      "Epoch 50/200\n",
      "45000/45000 [==============================] - 19s 420us/step - loss: 0.7013 - acc: 0.8529 - val_loss: 0.7023 - val_acc: 0.8540\n",
      "Epoch 51/200\n",
      "45000/45000 [==============================] - 19s 421us/step - loss: 0.7125 - acc: 0.8518 - val_loss: 0.7345 - val_acc: 0.8482\n",
      "Epoch 52/200\n",
      "45000/45000 [==============================] - 19s 422us/step - loss: 0.7097 - acc: 0.8520 - val_loss: 1.0325 - val_acc: 0.7606\n",
      "Epoch 53/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7149 - acc: 0.8519 - val_loss: 0.8287 - val_acc: 0.8200\n",
      "Epoch 54/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7011 - acc: 0.8544 - val_loss: 0.7276 - val_acc: 0.8470\n",
      "Epoch 55/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.7111 - acc: 0.8520 - val_loss: 0.7575 - val_acc: 0.8396\n",
      "Epoch 56/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7076 - acc: 0.8535 - val_loss: 0.7203 - val_acc: 0.8486\n",
      "Epoch 57/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7082 - acc: 0.8508 - val_loss: 0.8018 - val_acc: 0.8256\n",
      "Epoch 58/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7057 - acc: 0.8527 - val_loss: 0.7104 - val_acc: 0.8552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7089 - acc: 0.8524 - val_loss: 0.7249 - val_acc: 0.8450\n",
      "Epoch 60/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.7105 - acc: 0.8516 - val_loss: 0.8077 - val_acc: 0.8204\n",
      "Epoch 61/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7039 - acc: 0.8542 - val_loss: 0.8019 - val_acc: 0.8238\n",
      "Epoch 62/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7046 - acc: 0.8525 - val_loss: 0.7181 - val_acc: 0.8472\n",
      "Epoch 63/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7127 - acc: 0.8521 - val_loss: 0.7480 - val_acc: 0.8402\n",
      "Epoch 64/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7081 - acc: 0.8525 - val_loss: 0.7196 - val_acc: 0.8494\n",
      "Epoch 65/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7109 - acc: 0.8530 - val_loss: 0.6939 - val_acc: 0.8558\n",
      "Epoch 66/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.7071 - acc: 0.8531 - val_loss: 0.6995 - val_acc: 0.8512\n",
      "Epoch 67/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7009 - acc: 0.8538 - val_loss: 0.7433 - val_acc: 0.8422\n",
      "Epoch 68/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7000 - acc: 0.8564 - val_loss: 0.7286 - val_acc: 0.8464\n",
      "Epoch 69/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.7085 - acc: 0.8529 - val_loss: 0.7188 - val_acc: 0.8526\n",
      "Epoch 70/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7072 - acc: 0.8541 - val_loss: 0.8783 - val_acc: 0.8128\n",
      "Epoch 71/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7057 - acc: 0.8537 - val_loss: 0.7266 - val_acc: 0.8450\n",
      "Epoch 72/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7097 - acc: 0.8546 - val_loss: 0.8825 - val_acc: 0.8020\n",
      "Epoch 73/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.7053 - acc: 0.8536 - val_loss: 0.7386 - val_acc: 0.8484\n",
      "Epoch 74/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7066 - acc: 0.8534 - val_loss: 0.7200 - val_acc: 0.8518\n",
      "Epoch 75/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.7074 - acc: 0.8525 - val_loss: 0.8223 - val_acc: 0.8190\n",
      "Epoch 76/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7022 - acc: 0.8550 - val_loss: 0.7471 - val_acc: 0.8438\n",
      "Epoch 77/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7012 - acc: 0.8556 - val_loss: 0.7373 - val_acc: 0.8468\n",
      "Epoch 78/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6970 - acc: 0.8559 - val_loss: 0.7294 - val_acc: 0.8476\n",
      "Epoch 79/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6989 - acc: 0.8555 - val_loss: 0.7318 - val_acc: 0.8454\n",
      "Epoch 80/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.7084 - acc: 0.8512 - val_loss: 0.7841 - val_acc: 0.8290\n",
      "Epoch 81/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6999 - acc: 0.8563 - val_loss: 0.8598 - val_acc: 0.8100\n",
      "Epoch 82/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7010 - acc: 0.8534 - val_loss: 0.7064 - val_acc: 0.8586\n",
      "Epoch 83/200\n",
      "45000/45000 [==============================] - 19s 422us/step - loss: 0.7119 - acc: 0.8520 - val_loss: 0.8048 - val_acc: 0.8236\n",
      "Epoch 84/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7058 - acc: 0.8533 - val_loss: 0.7449 - val_acc: 0.8402\n",
      "Epoch 85/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7041 - acc: 0.8537 - val_loss: 0.8039 - val_acc: 0.8268\n",
      "Epoch 86/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6976 - acc: 0.8564 - val_loss: 0.8290 - val_acc: 0.8222\n",
      "Epoch 87/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7013 - acc: 0.8549 - val_loss: 0.7814 - val_acc: 0.8396\n",
      "Epoch 88/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7126 - acc: 0.8513 - val_loss: 0.7245 - val_acc: 0.8492\n",
      "Epoch 89/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6968 - acc: 0.8580 - val_loss: 0.8029 - val_acc: 0.8232\n",
      "Epoch 90/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7002 - acc: 0.8555 - val_loss: 0.7229 - val_acc: 0.8502\n",
      "Epoch 91/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7050 - acc: 0.8541 - val_loss: 0.7802 - val_acc: 0.8336\n",
      "Epoch 92/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6961 - acc: 0.8573 - val_loss: 0.7483 - val_acc: 0.8412\n",
      "Epoch 93/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7013 - acc: 0.8541 - val_loss: 0.7253 - val_acc: 0.8484\n",
      "Epoch 94/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7023 - acc: 0.8562 - val_loss: 0.7028 - val_acc: 0.8584\n",
      "Epoch 95/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7021 - acc: 0.8544 - val_loss: 0.7811 - val_acc: 0.8348\n",
      "Epoch 96/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6994 - acc: 0.8549 - val_loss: 0.7389 - val_acc: 0.8490\n",
      "Epoch 97/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6952 - acc: 0.8566 - val_loss: 0.7253 - val_acc: 0.8520\n",
      "Epoch 98/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6968 - acc: 0.8567 - val_loss: 0.7653 - val_acc: 0.8368\n",
      "Epoch 99/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.7007 - acc: 0.8539 - val_loss: 0.7712 - val_acc: 0.8334\n",
      "Epoch 100/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.7001 - acc: 0.8545 - val_loss: 0.6937 - val_acc: 0.8574\n",
      "Epoch 101/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.6980 - acc: 0.8546 - val_loss: 0.7841 - val_acc: 0.8346\n",
      "Epoch 102/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7046 - acc: 0.8535 - val_loss: 0.7996 - val_acc: 0.8288\n",
      "Epoch 103/200\n",
      "45000/45000 [==============================] - 19s 422us/step - loss: 0.6887 - acc: 0.8589 - val_loss: 0.7228 - val_acc: 0.8522\n",
      "Epoch 104/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6958 - acc: 0.8559 - val_loss: 0.7642 - val_acc: 0.8336\n",
      "Epoch 105/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7017 - acc: 0.8548 - val_loss: 0.7931 - val_acc: 0.8226\n",
      "Epoch 106/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6929 - acc: 0.8563 - val_loss: 0.7020 - val_acc: 0.8594\n",
      "Epoch 107/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7008 - acc: 0.8549 - val_loss: 0.6892 - val_acc: 0.8612\n",
      "Epoch 108/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7018 - acc: 0.8551 - val_loss: 0.7377 - val_acc: 0.8490\n",
      "Epoch 109/200\n",
      "45000/45000 [==============================] - 19s 422us/step - loss: 0.6954 - acc: 0.8570 - val_loss: 0.7441 - val_acc: 0.8418\n",
      "Epoch 110/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6955 - acc: 0.8572 - val_loss: 0.7676 - val_acc: 0.8340\n",
      "Epoch 111/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6929 - acc: 0.8580 - val_loss: 0.7391 - val_acc: 0.8450\n",
      "Epoch 112/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.6987 - acc: 0.8559 - val_loss: 0.8096 - val_acc: 0.8216\n",
      "Epoch 113/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7020 - acc: 0.8542 - val_loss: 0.7735 - val_acc: 0.8282\n",
      "Epoch 114/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.6973 - acc: 0.8537 - val_loss: 0.7515 - val_acc: 0.8412\n",
      "Epoch 115/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7031 - acc: 0.8570 - val_loss: 0.7544 - val_acc: 0.8380\n",
      "Epoch 116/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.6995 - acc: 0.8558 - val_loss: 0.7764 - val_acc: 0.8336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.7049 - acc: 0.8548 - val_loss: 0.8436 - val_acc: 0.8216\n",
      "Epoch 118/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6889 - acc: 0.8582 - val_loss: 0.7211 - val_acc: 0.8452\n",
      "Epoch 119/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6968 - acc: 0.8574 - val_loss: 0.7663 - val_acc: 0.8434\n",
      "Epoch 120/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6990 - acc: 0.8571 - val_loss: 0.7379 - val_acc: 0.8402\n",
      "Epoch 121/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.6915 - acc: 0.8582 - val_loss: 0.7116 - val_acc: 0.8502\n",
      "Epoch 122/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.6968 - acc: 0.8566 - val_loss: 0.7059 - val_acc: 0.8552\n",
      "Epoch 123/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6885 - acc: 0.8597 - val_loss: 0.7093 - val_acc: 0.8574\n",
      "Epoch 124/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6973 - acc: 0.8562 - val_loss: 0.8011 - val_acc: 0.8354\n",
      "Epoch 125/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.7027 - acc: 0.8537 - val_loss: 0.7326 - val_acc: 0.8438\n",
      "Epoch 126/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6954 - acc: 0.8580 - val_loss: 0.7461 - val_acc: 0.8490\n",
      "Epoch 127/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6953 - acc: 0.8574 - val_loss: 0.7358 - val_acc: 0.8456\n",
      "Epoch 128/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.6925 - acc: 0.8605 - val_loss: 0.7885 - val_acc: 0.8280\n",
      "Epoch 129/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6959 - acc: 0.8579 - val_loss: 0.7337 - val_acc: 0.8480\n",
      "Epoch 130/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6940 - acc: 0.8574 - val_loss: 0.6954 - val_acc: 0.8584\n",
      "Epoch 131/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6966 - acc: 0.8570 - val_loss: 0.7688 - val_acc: 0.8344\n",
      "Epoch 132/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6893 - acc: 0.8582 - val_loss: 0.7502 - val_acc: 0.8442\n",
      "Epoch 133/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6973 - acc: 0.8557 - val_loss: 0.7766 - val_acc: 0.8284\n",
      "Epoch 134/200\n",
      "45000/45000 [==============================] - 19s 419us/step - loss: 0.6850 - acc: 0.8600 - val_loss: 0.7106 - val_acc: 0.8510\n",
      "Epoch 135/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6917 - acc: 0.8572 - val_loss: 0.7523 - val_acc: 0.8418\n",
      "Epoch 136/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6897 - acc: 0.8602 - val_loss: 0.7569 - val_acc: 0.8354\n",
      "Epoch 137/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6920 - acc: 0.8584 - val_loss: 0.7942 - val_acc: 0.8332\n",
      "Epoch 138/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6936 - acc: 0.8580 - val_loss: 0.7212 - val_acc: 0.8534\n",
      "Epoch 139/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.6907 - acc: 0.8589 - val_loss: 0.7950 - val_acc: 0.8248\n",
      "Epoch 140/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6991 - acc: 0.8555 - val_loss: 0.6984 - val_acc: 0.8572\n",
      "Epoch 141/200\n",
      "45000/45000 [==============================] - 19s 420us/step - loss: 0.6873 - acc: 0.8588 - val_loss: 0.7014 - val_acc: 0.8544\n",
      "Epoch 142/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6835 - acc: 0.8603 - val_loss: 0.7160 - val_acc: 0.8514\n",
      "Epoch 143/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7011 - acc: 0.8546 - val_loss: 0.7346 - val_acc: 0.8438\n",
      "Epoch 144/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6987 - acc: 0.8566 - val_loss: 0.9910 - val_acc: 0.7700\n",
      "Epoch 145/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6977 - acc: 0.8563 - val_loss: 0.7374 - val_acc: 0.8438\n",
      "Epoch 146/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6892 - acc: 0.8596 - val_loss: 0.7593 - val_acc: 0.8344\n",
      "Epoch 147/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6967 - acc: 0.8567 - val_loss: 0.8070 - val_acc: 0.8226\n",
      "Epoch 148/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.7014 - acc: 0.8552 - val_loss: 0.7723 - val_acc: 0.8368\n",
      "Epoch 149/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6926 - acc: 0.8570 - val_loss: 0.8062 - val_acc: 0.8276\n",
      "Epoch 150/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6881 - acc: 0.8605 - val_loss: 0.7520 - val_acc: 0.8428\n",
      "Epoch 151/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6887 - acc: 0.8583 - val_loss: 0.7132 - val_acc: 0.8522\n",
      "Epoch 152/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6960 - acc: 0.8568 - val_loss: 0.7296 - val_acc: 0.8450\n",
      "Epoch 153/200\n",
      "45000/45000 [==============================] - 19s 422us/step - loss: 0.6952 - acc: 0.8561 - val_loss: 0.7694 - val_acc: 0.8356\n",
      "Epoch 154/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6964 - acc: 0.8563 - val_loss: 0.7100 - val_acc: 0.8504\n",
      "Epoch 155/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6985 - acc: 0.8552 - val_loss: 0.7906 - val_acc: 0.8296\n",
      "Epoch 156/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.6975 - acc: 0.8572 - val_loss: 0.7238 - val_acc: 0.8480\n",
      "Epoch 157/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6919 - acc: 0.8576 - val_loss: 0.7351 - val_acc: 0.8446\n",
      "Epoch 158/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6885 - acc: 0.8607 - val_loss: 0.7036 - val_acc: 0.8514\n",
      "Epoch 159/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6862 - acc: 0.8599 - val_loss: 0.7356 - val_acc: 0.8490\n",
      "Epoch 160/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6933 - acc: 0.8568 - val_loss: 0.7355 - val_acc: 0.8474\n",
      "Epoch 161/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6942 - acc: 0.8576 - val_loss: 0.8927 - val_acc: 0.7972\n",
      "Epoch 162/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6876 - acc: 0.8607 - val_loss: 0.7072 - val_acc: 0.8494\n",
      "Epoch 163/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6918 - acc: 0.8573 - val_loss: 0.7297 - val_acc: 0.8446\n",
      "Epoch 164/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6938 - acc: 0.8588 - val_loss: 0.7363 - val_acc: 0.8418\n",
      "Epoch 165/200\n",
      "45000/45000 [==============================] - 19s 422us/step - loss: 0.6930 - acc: 0.8570 - val_loss: 0.7335 - val_acc: 0.8474\n",
      "Epoch 166/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6851 - acc: 0.8583 - val_loss: 0.7627 - val_acc: 0.8364\n",
      "Epoch 167/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6895 - acc: 0.8582 - val_loss: 0.7134 - val_acc: 0.8538\n",
      "Epoch 168/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6911 - acc: 0.8581 - val_loss: 0.7228 - val_acc: 0.8444\n",
      "Epoch 169/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6996 - acc: 0.8556 - val_loss: 0.7184 - val_acc: 0.8536\n",
      "Epoch 170/200\n",
      "45000/45000 [==============================] - 19s 422us/step - loss: 0.6882 - acc: 0.8600 - val_loss: 0.7214 - val_acc: 0.8530\n",
      "Epoch 171/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.6950 - acc: 0.8575 - val_loss: 0.7976 - val_acc: 0.8268\n",
      "Epoch 172/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6939 - acc: 0.8562 - val_loss: 0.8800 - val_acc: 0.8032\n",
      "Epoch 173/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6909 - acc: 0.8568 - val_loss: 0.7253 - val_acc: 0.8554\n",
      "Epoch 174/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.6945 - acc: 0.8558 - val_loss: 0.7209 - val_acc: 0.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6844 - acc: 0.8603 - val_loss: 0.7582 - val_acc: 0.8364\n",
      "Epoch 176/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6863 - acc: 0.8597 - val_loss: 0.7466 - val_acc: 0.8408\n",
      "Epoch 177/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6905 - acc: 0.8591 - val_loss: 0.7437 - val_acc: 0.8458\n",
      "Epoch 178/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6925 - acc: 0.8586 - val_loss: 0.8099 - val_acc: 0.8210\n",
      "Epoch 179/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6889 - acc: 0.8599 - val_loss: 0.7603 - val_acc: 0.8386\n",
      "Epoch 180/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6947 - acc: 0.8573 - val_loss: 0.8104 - val_acc: 0.8236\n",
      "Epoch 181/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6871 - acc: 0.8594 - val_loss: 0.8230 - val_acc: 0.8216\n",
      "Epoch 182/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.6867 - acc: 0.8588 - val_loss: 0.7264 - val_acc: 0.8526\n",
      "Epoch 183/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6863 - acc: 0.8585 - val_loss: 0.7305 - val_acc: 0.8464\n",
      "Epoch 184/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6867 - acc: 0.8573 - val_loss: 0.7409 - val_acc: 0.8428\n",
      "Epoch 185/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6920 - acc: 0.8573 - val_loss: 0.7385 - val_acc: 0.8440\n",
      "Epoch 186/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.6928 - acc: 0.8568 - val_loss: 0.7068 - val_acc: 0.8548\n",
      "Epoch 187/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.7008 - acc: 0.8557 - val_loss: 0.7386 - val_acc: 0.8446\n",
      "Epoch 188/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.6999 - acc: 0.8552 - val_loss: 0.7682 - val_acc: 0.8402\n",
      "Epoch 189/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.6910 - acc: 0.8577 - val_loss: 0.8213 - val_acc: 0.8222\n",
      "Epoch 190/200\n",
      "45000/45000 [==============================] - 19s 423us/step - loss: 0.6912 - acc: 0.8568 - val_loss: 0.9264 - val_acc: 0.7836\n",
      "Epoch 191/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6923 - acc: 0.8583 - val_loss: 0.7645 - val_acc: 0.8366\n",
      "Epoch 192/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.6875 - acc: 0.8581 - val_loss: 0.7984 - val_acc: 0.8314\n",
      "Epoch 193/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6829 - acc: 0.8606 - val_loss: 0.7462 - val_acc: 0.8422\n",
      "Epoch 194/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.6830 - acc: 0.8608 - val_loss: 0.6942 - val_acc: 0.8602\n",
      "Epoch 195/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6847 - acc: 0.8602 - val_loss: 0.7234 - val_acc: 0.8486\n",
      "Epoch 196/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.6866 - acc: 0.8610 - val_loss: 0.7501 - val_acc: 0.8394\n",
      "Epoch 197/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6997 - acc: 0.8571 - val_loss: 0.7815 - val_acc: 0.8348\n",
      "Epoch 198/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6857 - acc: 0.8587 - val_loss: 0.7964 - val_acc: 0.8224\n",
      "Epoch 199/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6902 - acc: 0.8591 - val_loss: 0.6924 - val_acc: 0.8612\n",
      "Epoch 200/200\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 0.6912 - acc: 0.8581 - val_loss: 0.8108 - val_acc: 0.8208\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    " \n",
    "train_history=model_2.fit(x_train,y_train,epochs=200, batch_size=128,\n",
    "             validation_split=0.1, verbose=1)\n",
    "\n",
    "model_2.save('8.7.6ent_b_cifar10vgg16_2_1.h5')\n",
    "model_2.save_weights('8.7.6ent_b_cifar10vgg16_2_1_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8574253187179566, 0.8093, 0.9881, 0.8093]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import keras\n",
    "top1_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=1) #top-1精度\n",
    "\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',metrics=['accuracy','top_k_categorical_accuracy',top1_acc])\n",
    "\n",
    "model_2.evaluate(x_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试损失为：0.8574\n",
      "测试准确率为：0.8093\n"
     ]
    }
   ],
   "source": [
    "score=model_2.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"测试损失为：{:.4f}\".format(score[0]))\n",
    "print(\"测试准确率为：{:.4f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [16.644323118959143, 1.5526349161416408, 1.9765302934397464]\n",
      "1 [2.9154759474226504, 1.3228756555322954, 0.5590169943749475]\n",
      "2 [3.146635656905295, 2.837096019037473, 4.692058983146991]\n",
      "3 [2.5369909678440226, 10.207552639958413, 8.831261420326921]\n",
      "4 [20.421903976501596, 17.228361406923703, 1.6817761124195998]\n",
      "5 [2.327026933899001, 1.4675199349396213, 1.0847759483980834]\n",
      "6 [1.9657425815319673, 2.8740712303467855, 2.042306123862451]\n",
      "7 [2.227754615077702, 2.4980461114639176, 1.653594569415369]\n",
      "8 [10.110114210598656, 10.218423295704296, 2.001953065453505]\n",
      "9 [12.611061947657188, 2.0000011813576455, 1.2348743471297698]\n",
      "10 [2.099106952968333, 1.9921721813136535, 2.692582403567252]\n",
      "11 [5.586622317557722, 2.5101206283957294, 3.007197738343821]\n",
      "12 [30.66874045788391, 21.8770303056471, 9.106411905099579]\n",
      "13 [13.856441699170102, 0.0, 11.215700207848984]\n",
      "14 [30.454622986357094, 8.326921014553404, 50.83351847297014]\n",
      "15 [1.557491974297139, 1.25, 2.0223362109204297]\n",
      "16 [1.8192224472284855, 1.5077922602268523, 1.7316278919704429]\n",
      "17 [1.6495861553172175, 1.5350200796522526, 2.6819443541111156]\n",
      "18 [3.1735294330602954, 2.375, 1.8856468551931986]\n",
      "19 [2.9154759474226504, 2.1578345627040085, 1.9803724397193574]\n",
      "20 [3.3594397493871697, 1.9450340313649777, 4.561575534648007]\n",
      "21 [1.9999999999220108, 1.4150803375810637, 6.362827490506664]\n",
      "22 [4.270337404750052, 5.130455067957466, 3.5273072390434366]\n",
      "23 [4.046603514059662, 4.782781617427248, 0.5]\n",
      "24 [0.32666654252679245, 0.8738871995672942, 9.118307853840468]\n",
      "25 [2.4660314911407926, 2.420697867520286, 1.7016153961818692]\n",
      "26 [0.9314268854661791, 3.7772410239735943, 30.140110120236745]\n",
      "27 [1.3228415920952514, 5.034373149243991, 5.656854249492381]\n",
      "28 [7.044073334015369, 2.760489930130586, 1.68326230503934]\n",
      "29 [4.583265861823783, 4.1500672696877645, 5.806300045820201]\n",
      "30 [2.2707377655731187, 2.4109126902482387, 1.7919088983539313]\n",
      "31 [16.142515561074315, 2.0000008998175005, 2.452218769994729]\n",
      "32 [1.5811388300841898, 0.0, 0.0]\n",
      "33 [18.89649969213688, 0.7744081744707395, 5.939403610248896]\n",
      "34 [1.7216017736527922, 4.69086544340604, 4.752046069119368]\n",
      "35 [1.6298006013006623, 2.462214450449026, 1.0897247358851685]\n",
      "36 [5.058260260823498, 10.604498190403437, 3.777590943762197]\n",
      "37 [1.3896578736832648, 5.188305390124747, 1.6459814731307283]\n",
      "38 [3.901963682253009, 1.556029492961077, 3.06181583742168]\n",
      "39 [1.983716968256561, 10.80569258805486, 10.916147932560243]\n",
      "40 [1.9611011722993357, 1.9999998045905902, 2.1989485047340582]\n",
      "41 [9.425748063814352, 1.59375, 1.7365554986812255]\n",
      "42 [3.114273616718372, 4.948410534220614, 2.786651536298322]\n",
      "43 [14.223267342782613, 13.18081732555764, 2.542818985028651]\n",
      "44 [2.073083513021305, 3.616026327400239, 6.279253219998547]\n",
      "45 [5.680238070032084, 5.852388657232216, 2.154209875319704]\n",
      "46 [2.8840212218549537, 3.1473384197746563, 3.048422239608539]\n",
      "47 [2.171369095755026, 2.339370909026613, 1.9588042430268524]\n",
      "48 [2.618540604807373, 2.0657059577162737, 1.9326811100206331]\n",
      "49 [7.0710678118654755, 4.367851302414037, 1.7633419974582356]\n",
      "50 [9.668933291630525, 2.921719513800256, 2.007985055685238]\n",
      "51 [2.3487266347310682, 7.273300681789015, 1.3575363703618597]\n",
      "52 [4.7035388384258, 3.1410351385794417, 2.8186475959327866]\n",
      "53 [3.595177810480283, 11.07796109055399, 4.995930255186221]\n",
      "54 [2.2707377655731187, 2.6339134382131846, 4.3535186918169995]\n",
      "55 [1.9973393126742423, 12.80244443843013, 12.928217446464869]\n",
      "56 [3.36804839632687, 2.053959590644373, 2.75]\n",
      "57 [2.0138270913066045, 2.2912926860023486, 1.7283214912664873]\n",
      "58 [2.6164294611168097, 1.9105995459540965, 1.7853571071357126]\n",
      "59 [6.799011350736416, 3.0375790248214227, 1.937090558389336]\n",
      "60 [1.7166680145509143, 1.1819239253140608, 8.400000163345108]\n",
      "61 [8.430854129758698, 8.588159886178197, 2.292354637974552]\n",
      "62 [6.144729824458758, 16.580735614963686, 10.676468604774891]\n",
      "63 [4.992363265499854, 4.751404554607682, 2.1102322591004574]\n",
      "[[16.644323118959143, 1.5526349161416408, 1.9765302934397464], [2.9154759474226504, 1.3228756555322954, 0.5590169943749475], [3.146635656905295, 2.837096019037473, 4.692058983146991], [2.5369909678440226, 10.207552639958413, 8.831261420326921], [20.421903976501596, 17.228361406923703, 1.6817761124195998], [2.327026933899001, 1.4675199349396213, 1.0847759483980834], [1.9657425815319673, 2.8740712303467855, 2.042306123862451], [2.227754615077702, 2.4980461114639176, 1.653594569415369], [10.110114210598656, 10.218423295704296, 2.001953065453505], [12.611061947657188, 2.0000011813576455, 1.2348743471297698], [2.099106952968333, 1.9921721813136535, 2.692582403567252], [5.586622317557722, 2.5101206283957294, 3.007197738343821], [30.66874045788391, 21.8770303056471, 9.106411905099579], [13.856441699170102, 0.0, 11.215700207848984], [30.454622986357094, 8.326921014553404, 50.83351847297014], [1.557491974297139, 1.25, 2.0223362109204297], [1.8192224472284855, 1.5077922602268523, 1.7316278919704429], [1.6495861553172175, 1.5350200796522526, 2.6819443541111156], [3.1735294330602954, 2.375, 1.8856468551931986], [2.9154759474226504, 2.1578345627040085, 1.9803724397193574], [3.3594397493871697, 1.9450340313649777, 4.561575534648007], [1.9999999999220108, 1.4150803375810637, 6.362827490506664], [4.270337404750052, 5.130455067957466, 3.5273072390434366], [4.046603514059662, 4.782781617427248, 0.5], [0.32666654252679245, 0.8738871995672942, 9.118307853840468], [2.4660314911407926, 2.420697867520286, 1.7016153961818692], [0.9314268854661791, 3.7772410239735943, 30.140110120236745], [1.3228415920952514, 5.034373149243991, 5.656854249492381], [7.044073334015369, 2.760489930130586, 1.68326230503934], [4.583265861823783, 4.1500672696877645, 5.806300045820201], [2.2707377655731187, 2.4109126902482387, 1.7919088983539313], [16.142515561074315, 2.0000008998175005, 2.452218769994729], [1.5811388300841898, 0.0, 0.0], [18.89649969213688, 0.7744081744707395, 5.939403610248896], [1.7216017736527922, 4.69086544340604, 4.752046069119368], [1.6298006013006623, 2.462214450449026, 1.0897247358851685], [5.058260260823498, 10.604498190403437, 3.777590943762197], [1.3896578736832648, 5.188305390124747, 1.6459814731307283], [3.901963682253009, 1.556029492961077, 3.06181583742168], [1.983716968256561, 10.80569258805486, 10.916147932560243], [1.9611011722993357, 1.9999998045905902, 2.1989485047340582], [9.425748063814352, 1.59375, 1.7365554986812255], [3.114273616718372, 4.948410534220614, 2.786651536298322], [14.223267342782613, 13.18081732555764, 2.542818985028651], [2.073083513021305, 3.616026327400239, 6.279253219998547], [5.680238070032084, 5.852388657232216, 2.154209875319704], [2.8840212218549537, 3.1473384197746563, 3.048422239608539], [2.171369095755026, 2.339370909026613, 1.9588042430268524], [2.618540604807373, 2.0657059577162737, 1.9326811100206331], [7.0710678118654755, 4.367851302414037, 1.7633419974582356], [9.668933291630525, 2.921719513800256, 2.007985055685238], [2.3487266347310682, 7.273300681789015, 1.3575363703618597], [4.7035388384258, 3.1410351385794417, 2.8186475959327866], [3.595177810480283, 11.07796109055399, 4.995930255186221], [2.2707377655731187, 2.6339134382131846, 4.3535186918169995], [1.9973393126742423, 12.80244443843013, 12.928217446464869], [3.36804839632687, 2.053959590644373, 2.75], [2.0138270913066045, 2.2912926860023486, 1.7283214912664873], [2.6164294611168097, 1.9105995459540965, 1.7853571071357126], [6.799011350736416, 3.0375790248214227, 1.937090558389336], [1.7166680145509143, 1.1819239253140608, 8.400000163345108], [8.430854129758698, 8.588159886178197, 2.292354637974552], [6.144729824458758, 16.580735614963686, 10.676468604774891], [4.992363265499854, 4.751404554607682, 2.1102322591004574]]\n"
     ]
    }
   ],
   "source": [
    "tensor =[]\n",
    "for i in range(0,64):\n",
    "   \n",
    "    u=model_2.get_layer('conv2d_1').get_weights()[0][:,:,:,i].squeeze()\n",
    "    v=np.mean(u, axis=1)\n",
    "    #print(v)\n",
    "    vT=v.T\n",
    "    D=np.cov(vT)\n",
    "    try:                 \n",
    "        invD=np.linalg.inv(D)\n",
    "        #t=np.mean(v,axis=1)\n",
    "        a=[]\n",
    "\n",
    "        for j in range(0,2): #表示filter的大小\n",
    "            for k in range(j+1,3):\n",
    "               #if (j!=k) and (j<k):\n",
    "                tp=v[j]-v[k]\n",
    "                d=np.sqrt(abs(np.dot(np.dot(tp,invD),tp.T)))\n",
    "                a.append(d)\n",
    "            \n",
    "        print(i,a)\n",
    "        tensor.append(a)\n",
    "    except:\n",
    "        print(\"不可逆\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{10.0, 50.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "[0.9182958340544896, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 1.584962500721156, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0]\n"
     ]
    }
   ],
   "source": [
    "entt=[]\n",
    "for i in range (0,64):\n",
    "    \n",
    "    data=tensor[i]\n",
    "    data0=np.array(data)\n",
    "    print('四舍五入，精确到个位\\n',np.round(data0,decimals=-2))\n",
    "    data1=np.round(data0,decimals=-1)\n",
    "\n",
    "    data1_value_list=set([data1[i] for i in range (data1.shape[0])])\n",
    "    print(data1_value_list)\n",
    "    ent=0.0\n",
    "    for data1_value in data1_value_list:\n",
    "        p=float(data1[data1==data1_value].shape[0])/data1.shape[0]\n",
    "        print(p)\n",
    "        logp=np.log2(p)\n",
    "        ent-=p*logp\n",
    "        print(ent)   \n",
    "    entt.append(ent)\n",
    "print(entt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896]\n",
      "[1, 2, 5, 6, 7, 10, 15, 16, 17, 18, 19, 20, 23, 25, 30, 32, 34, 35, 38, 40, 42, 46, 47, 48, 52, 54, 56, 57, 58, 63, 0, 3, 4, 8, 9, 11, 13, 21]\n"
     ]
    }
   ],
   "source": [
    "import heapq #获取list中最小的\n",
    "\n",
    "f=int(len(entt)*0.6) #计算滤波器熵个数的80%\n",
    "m=entt\n",
    "max_number=heapq.nsmallest(f,m) #从m中找出最小的f个数，最大用nlargest\n",
    "max_index=[]\n",
    "for t in max_number:\n",
    "    index=m.index(t)\n",
    "    max_index.append(index)\n",
    "    m[index]=float('-inf')\n",
    "print(max_number)#输出最小的f个数\n",
    "print(max_index)#输出对应索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 38/64 channels from layer: conv2d_1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 26)        728       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 26)        104       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 26)        6110      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 26)        104       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 39)        9165      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 39)        156       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 39)        13728     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 39)        156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 52)          18304     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 103)         48307     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               53248     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 951,784\n",
      "Trainable params: 947,928\n",
      "Non-trainable params: 3,856\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerassurgeon.operations import delete_layer,insert_layer,delete_channels\n",
    "model_1 = delete_channels(model_2, model_2.layers[1],max_index)\n",
    "model_1.summary()\n",
    "model_1.save('8.7.6ent_b_cifar10vgg16_1.h5')\n",
    "model_1.save_weights('8.7.6ent_b_cifar10vgg16_1_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "45000/45000 [==============================] - 25s 558us/step - loss: 0.7411 - acc: 0.8365 - val_loss: 0.7183 - val_acc: 0.8462\n",
      "Epoch 2/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7376 - acc: 0.8389 - val_loss: 0.9433 - val_acc: 0.7782\n",
      "Epoch 3/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7291 - acc: 0.8435 - val_loss: 0.7213 - val_acc: 0.8400\n",
      "Epoch 4/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7270 - acc: 0.8457 - val_loss: 1.2385 - val_acc: 0.6936\n",
      "Epoch 5/200\n",
      "45000/45000 [==============================] - 18s 401us/step - loss: 0.7296 - acc: 0.8423 - val_loss: 0.8813 - val_acc: 0.8004\n",
      "Epoch 6/200\n",
      "45000/45000 [==============================] - 18s 400us/step - loss: 0.7192 - acc: 0.8485 - val_loss: 0.7198 - val_acc: 0.8508\n",
      "Epoch 7/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7171 - acc: 0.8492 - val_loss: 0.6834 - val_acc: 0.8620\n",
      "Epoch 8/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7241 - acc: 0.8468 - val_loss: 1.0244 - val_acc: 0.7492\n",
      "Epoch 9/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7254 - acc: 0.8476 - val_loss: 1.1404 - val_acc: 0.7330\n",
      "Epoch 10/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7204 - acc: 0.8478 - val_loss: 0.7272 - val_acc: 0.8456\n",
      "Epoch 11/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7128 - acc: 0.8505 - val_loss: 0.6901 - val_acc: 0.8588\n",
      "Epoch 12/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7119 - acc: 0.8512 - val_loss: 0.7179 - val_acc: 0.8498\n",
      "Epoch 13/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7189 - acc: 0.8502 - val_loss: 1.0948 - val_acc: 0.7378\n",
      "Epoch 14/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7164 - acc: 0.8504 - val_loss: 0.7612 - val_acc: 0.8364\n",
      "Epoch 15/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.7164 - acc: 0.8496 - val_loss: 0.8043 - val_acc: 0.8276\n",
      "Epoch 16/200\n",
      "45000/45000 [==============================] - 18s 400us/step - loss: 0.7239 - acc: 0.8477 - val_loss: 0.7969 - val_acc: 0.8262\n",
      "Epoch 17/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7359 - acc: 0.8441 - val_loss: 0.9304 - val_acc: 0.7880\n",
      "Epoch 18/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7282 - acc: 0.8461 - val_loss: 0.8347 - val_acc: 0.8174\n",
      "Epoch 19/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7214 - acc: 0.8489 - val_loss: 0.8303 - val_acc: 0.8198\n",
      "Epoch 20/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7148 - acc: 0.8524 - val_loss: 0.8303 - val_acc: 0.8174\n",
      "Epoch 21/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7180 - acc: 0.8495 - val_loss: 0.8530 - val_acc: 0.8134\n",
      "Epoch 22/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7135 - acc: 0.8521 - val_loss: 0.7322 - val_acc: 0.8514\n",
      "Epoch 23/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7132 - acc: 0.8508 - val_loss: 0.7832 - val_acc: 0.8338\n",
      "Epoch 24/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.7191 - acc: 0.8489 - val_loss: 0.9039 - val_acc: 0.8032\n",
      "Epoch 25/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7118 - acc: 0.8504 - val_loss: 0.7708 - val_acc: 0.8330\n",
      "Epoch 26/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7192 - acc: 0.8494 - val_loss: 0.7363 - val_acc: 0.8416\n",
      "Epoch 27/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7159 - acc: 0.8502 - val_loss: 0.7216 - val_acc: 0.8544\n",
      "Epoch 28/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7166 - acc: 0.8522 - val_loss: 0.7541 - val_acc: 0.8474\n",
      "Epoch 29/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7177 - acc: 0.8495 - val_loss: 0.7685 - val_acc: 0.8358\n",
      "Epoch 30/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7244 - acc: 0.8482 - val_loss: 0.8044 - val_acc: 0.8198\n",
      "Epoch 31/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7173 - acc: 0.8485 - val_loss: 0.7092 - val_acc: 0.8534\n",
      "Epoch 32/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7154 - acc: 0.8507 - val_loss: 0.8257 - val_acc: 0.8182\n",
      "Epoch 33/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.7128 - acc: 0.8509 - val_loss: 0.7485 - val_acc: 0.8416\n",
      "Epoch 34/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7098 - acc: 0.8519 - val_loss: 0.7267 - val_acc: 0.8468\n",
      "Epoch 35/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.7071 - acc: 0.8513 - val_loss: 0.7226 - val_acc: 0.8482\n",
      "Epoch 36/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7187 - acc: 0.8480 - val_loss: 0.7302 - val_acc: 0.8472\n",
      "Epoch 37/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.7074 - acc: 0.8532 - val_loss: 0.7701 - val_acc: 0.8302\n",
      "Epoch 38/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7088 - acc: 0.8527 - val_loss: 0.7152 - val_acc: 0.8498\n",
      "Epoch 39/200\n",
      "45000/45000 [==============================] - 18s 391us/step - loss: 0.7084 - acc: 0.8536 - val_loss: 0.7607 - val_acc: 0.8416\n",
      "Epoch 40/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7106 - acc: 0.8522 - val_loss: 0.7799 - val_acc: 0.8280\n",
      "Epoch 41/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7052 - acc: 0.8518 - val_loss: 0.7529 - val_acc: 0.8418\n",
      "Epoch 42/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7063 - acc: 0.8537 - val_loss: 0.7658 - val_acc: 0.8406\n",
      "Epoch 43/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7162 - acc: 0.8494 - val_loss: 0.7894 - val_acc: 0.8328\n",
      "Epoch 44/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7117 - acc: 0.8521 - val_loss: 0.7511 - val_acc: 0.8406\n",
      "Epoch 45/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7113 - acc: 0.8517 - val_loss: 0.7012 - val_acc: 0.8574\n",
      "Epoch 46/200\n",
      "45000/45000 [==============================] - 18s 393us/step - loss: 0.7161 - acc: 0.8495 - val_loss: 0.7833 - val_acc: 0.8336\n",
      "Epoch 47/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7044 - acc: 0.8542 - val_loss: 0.7867 - val_acc: 0.8328\n",
      "Epoch 48/200\n",
      "45000/45000 [==============================] - 18s 392us/step - loss: 0.7184 - acc: 0.8501 - val_loss: 0.7521 - val_acc: 0.8416\n",
      "Epoch 49/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.7094 - acc: 0.8533 - val_loss: 0.7063 - val_acc: 0.8536\n",
      "Epoch 50/200\n",
      "45000/45000 [==============================] - 18s 392us/step - loss: 0.7162 - acc: 0.8508 - val_loss: 0.9861 - val_acc: 0.7688\n",
      "Epoch 51/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7115 - acc: 0.8516 - val_loss: 0.7409 - val_acc: 0.8476\n",
      "Epoch 52/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7107 - acc: 0.8522 - val_loss: 0.7345 - val_acc: 0.8486\n",
      "Epoch 53/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7077 - acc: 0.8527 - val_loss: 0.7044 - val_acc: 0.8536\n",
      "Epoch 54/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7127 - acc: 0.8512 - val_loss: 0.7322 - val_acc: 0.8476\n",
      "Epoch 55/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7049 - acc: 0.8534 - val_loss: 0.8967 - val_acc: 0.8012\n",
      "Epoch 56/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7055 - acc: 0.8525 - val_loss: 0.7678 - val_acc: 0.8332\n",
      "Epoch 57/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7113 - acc: 0.8521 - val_loss: 0.8353 - val_acc: 0.8214\n",
      "Epoch 58/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7040 - acc: 0.8530 - val_loss: 0.7331 - val_acc: 0.8482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7124 - acc: 0.8522 - val_loss: 0.7259 - val_acc: 0.8464\n",
      "Epoch 60/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7062 - acc: 0.8549 - val_loss: 0.7888 - val_acc: 0.8302\n",
      "Epoch 61/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7102 - acc: 0.8530 - val_loss: 0.8206 - val_acc: 0.8216\n",
      "Epoch 62/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7035 - acc: 0.8548 - val_loss: 0.7515 - val_acc: 0.8362\n",
      "Epoch 63/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7031 - acc: 0.8533 - val_loss: 0.7654 - val_acc: 0.8386\n",
      "Epoch 64/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7114 - acc: 0.8514 - val_loss: 0.8122 - val_acc: 0.8238\n",
      "Epoch 65/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7052 - acc: 0.8509 - val_loss: 0.7368 - val_acc: 0.8426\n",
      "Epoch 66/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7080 - acc: 0.8526 - val_loss: 0.7276 - val_acc: 0.8468\n",
      "Epoch 67/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7160 - acc: 0.8492 - val_loss: 0.8912 - val_acc: 0.8044\n",
      "Epoch 68/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7108 - acc: 0.8527 - val_loss: 0.7494 - val_acc: 0.8396\n",
      "Epoch 69/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7019 - acc: 0.8532 - val_loss: 0.8323 - val_acc: 0.8058\n",
      "Epoch 70/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7062 - acc: 0.8531 - val_loss: 1.0078 - val_acc: 0.7768\n",
      "Epoch 71/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7073 - acc: 0.8528 - val_loss: 0.7167 - val_acc: 0.8542\n",
      "Epoch 72/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7059 - acc: 0.8540 - val_loss: 0.8421 - val_acc: 0.8158\n",
      "Epoch 73/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7071 - acc: 0.8541 - val_loss: 0.6977 - val_acc: 0.8560\n",
      "Epoch 74/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7045 - acc: 0.8546 - val_loss: 0.7017 - val_acc: 0.8526\n",
      "Epoch 75/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7068 - acc: 0.8521 - val_loss: 0.6967 - val_acc: 0.8560\n",
      "Epoch 76/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7046 - acc: 0.8521 - val_loss: 0.7122 - val_acc: 0.8508\n",
      "Epoch 77/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7029 - acc: 0.8530 - val_loss: 0.7955 - val_acc: 0.8280\n",
      "Epoch 78/200\n",
      "45000/45000 [==============================] - 18s 400us/step - loss: 0.7063 - acc: 0.8534 - val_loss: 0.7200 - val_acc: 0.8482\n",
      "Epoch 79/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.7054 - acc: 0.8525 - val_loss: 0.7407 - val_acc: 0.8432\n",
      "Epoch 80/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7044 - acc: 0.8540 - val_loss: 0.7517 - val_acc: 0.8420\n",
      "Epoch 81/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7055 - acc: 0.8504 - val_loss: 0.7533 - val_acc: 0.8398\n",
      "Epoch 82/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7070 - acc: 0.8528 - val_loss: 0.7512 - val_acc: 0.8384\n",
      "Epoch 83/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7011 - acc: 0.8534 - val_loss: 0.7443 - val_acc: 0.8448\n",
      "Epoch 84/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.6979 - acc: 0.8562 - val_loss: 0.7422 - val_acc: 0.8428\n",
      "Epoch 85/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.7034 - acc: 0.8544 - val_loss: 0.7904 - val_acc: 0.8334\n",
      "Epoch 86/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7076 - acc: 0.8518 - val_loss: 0.7804 - val_acc: 0.8326\n",
      "Epoch 87/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7028 - acc: 0.8534 - val_loss: 0.7251 - val_acc: 0.8506\n",
      "Epoch 88/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7041 - acc: 0.8526 - val_loss: 0.7600 - val_acc: 0.8378\n",
      "Epoch 89/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7123 - acc: 0.8495 - val_loss: 0.8689 - val_acc: 0.8126\n",
      "Epoch 90/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.7083 - acc: 0.8512 - val_loss: 0.8735 - val_acc: 0.8106\n",
      "Epoch 91/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7015 - acc: 0.8535 - val_loss: 0.7371 - val_acc: 0.8440\n",
      "Epoch 92/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7031 - acc: 0.8538 - val_loss: 0.8381 - val_acc: 0.8174\n",
      "Epoch 93/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.6992 - acc: 0.8544 - val_loss: 0.7566 - val_acc: 0.8456\n",
      "Epoch 94/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7081 - acc: 0.8526 - val_loss: 0.7549 - val_acc: 0.8434\n",
      "Epoch 95/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7077 - acc: 0.8515 - val_loss: 0.8293 - val_acc: 0.8194\n",
      "Epoch 96/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.7053 - acc: 0.8540 - val_loss: 0.7438 - val_acc: 0.8398\n",
      "Epoch 97/200\n",
      "45000/45000 [==============================] - 18s 400us/step - loss: 0.7001 - acc: 0.8550 - val_loss: 0.7303 - val_acc: 0.8472\n",
      "Epoch 98/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7020 - acc: 0.8546 - val_loss: 0.7298 - val_acc: 0.8516\n",
      "Epoch 99/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7073 - acc: 0.8512 - val_loss: 0.7913 - val_acc: 0.8348\n",
      "Epoch 100/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6958 - acc: 0.8564 - val_loss: 0.7867 - val_acc: 0.8298\n",
      "Epoch 101/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7028 - acc: 0.8539 - val_loss: 0.8524 - val_acc: 0.8096\n",
      "Epoch 102/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6986 - acc: 0.8537 - val_loss: 0.8894 - val_acc: 0.8064\n",
      "Epoch 103/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6981 - acc: 0.8554 - val_loss: 0.6846 - val_acc: 0.8534\n",
      "Epoch 104/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7021 - acc: 0.8540 - val_loss: 0.7231 - val_acc: 0.8436\n",
      "Epoch 105/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6976 - acc: 0.8528 - val_loss: 0.7297 - val_acc: 0.8476\n",
      "Epoch 106/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7061 - acc: 0.8530 - val_loss: 0.8645 - val_acc: 0.8068\n",
      "Epoch 107/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7046 - acc: 0.8531 - val_loss: 0.7811 - val_acc: 0.8354\n",
      "Epoch 108/200\n",
      "45000/45000 [==============================] - 18s 393us/step - loss: 0.6961 - acc: 0.8535 - val_loss: 0.7684 - val_acc: 0.8344\n",
      "Epoch 109/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7096 - acc: 0.8521 - val_loss: 0.7356 - val_acc: 0.8454\n",
      "Epoch 110/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7055 - acc: 0.8535 - val_loss: 0.7656 - val_acc: 0.8376\n",
      "Epoch 111/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7026 - acc: 0.8537 - val_loss: 0.7338 - val_acc: 0.8486\n",
      "Epoch 112/200\n",
      "45000/45000 [==============================] - 18s 393us/step - loss: 0.7027 - acc: 0.8534 - val_loss: 0.7751 - val_acc: 0.8338\n",
      "Epoch 113/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.7030 - acc: 0.8528 - val_loss: 0.7482 - val_acc: 0.8394\n",
      "Epoch 114/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7062 - acc: 0.8524 - val_loss: 0.7495 - val_acc: 0.8372\n",
      "Epoch 115/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6943 - acc: 0.8558 - val_loss: 0.7113 - val_acc: 0.8580\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7035 - acc: 0.8517 - val_loss: 0.7322 - val_acc: 0.8456\n",
      "Epoch 117/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.6964 - acc: 0.8551 - val_loss: 0.7834 - val_acc: 0.8322\n",
      "Epoch 118/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6893 - acc: 0.8582 - val_loss: 0.7476 - val_acc: 0.8372\n",
      "Epoch 119/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7025 - acc: 0.8539 - val_loss: 0.8115 - val_acc: 0.8260\n",
      "Epoch 120/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7009 - acc: 0.8542 - val_loss: 0.7297 - val_acc: 0.8512\n",
      "Epoch 121/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.6975 - acc: 0.8568 - val_loss: 0.7683 - val_acc: 0.8330\n",
      "Epoch 122/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6956 - acc: 0.8561 - val_loss: 0.7538 - val_acc: 0.8376\n",
      "Epoch 123/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7026 - acc: 0.8548 - val_loss: 0.7491 - val_acc: 0.8408\n",
      "Epoch 124/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7039 - acc: 0.8537 - val_loss: 0.7328 - val_acc: 0.8412\n",
      "Epoch 125/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.7037 - acc: 0.8534 - val_loss: 0.7954 - val_acc: 0.8234\n",
      "Epoch 126/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6998 - acc: 0.8531 - val_loss: 0.7127 - val_acc: 0.8552\n",
      "Epoch 127/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.6999 - acc: 0.8547 - val_loss: 0.7736 - val_acc: 0.8326\n",
      "Epoch 128/200\n",
      "45000/45000 [==============================] - 18s 401us/step - loss: 0.6971 - acc: 0.8560 - val_loss: 0.6934 - val_acc: 0.8582\n",
      "Epoch 129/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.6968 - acc: 0.8558 - val_loss: 0.7685 - val_acc: 0.8322\n",
      "Epoch 130/200\n",
      "45000/45000 [==============================] - 18s 400us/step - loss: 0.7030 - acc: 0.8527 - val_loss: 0.8490 - val_acc: 0.8164\n",
      "Epoch 131/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6977 - acc: 0.8550 - val_loss: 0.6858 - val_acc: 0.8576\n",
      "Epoch 132/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.6926 - acc: 0.8564 - val_loss: 0.7087 - val_acc: 0.8502\n",
      "Epoch 133/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.6937 - acc: 0.8569 - val_loss: 0.7432 - val_acc: 0.8428\n",
      "Epoch 134/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6898 - acc: 0.8579 - val_loss: 0.8393 - val_acc: 0.8114\n",
      "Epoch 135/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.6962 - acc: 0.8550 - val_loss: 0.8069 - val_acc: 0.8182\n",
      "Epoch 136/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6988 - acc: 0.8560 - val_loss: 0.8031 - val_acc: 0.8308\n",
      "Epoch 137/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.6970 - acc: 0.8555 - val_loss: 0.7453 - val_acc: 0.8414\n",
      "Epoch 138/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6966 - acc: 0.8555 - val_loss: 0.7555 - val_acc: 0.8398\n",
      "Epoch 139/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6912 - acc: 0.8574 - val_loss: 0.7817 - val_acc: 0.8360\n",
      "Epoch 140/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6982 - acc: 0.8539 - val_loss: 0.7343 - val_acc: 0.8444\n",
      "Epoch 141/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6988 - acc: 0.8529 - val_loss: 1.0579 - val_acc: 0.7482\n",
      "Epoch 142/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6979 - acc: 0.8545 - val_loss: 0.7748 - val_acc: 0.8314\n",
      "Epoch 143/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6982 - acc: 0.8568 - val_loss: 0.7354 - val_acc: 0.8434\n",
      "Epoch 144/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6958 - acc: 0.8563 - val_loss: 0.7175 - val_acc: 0.8498\n",
      "Epoch 145/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6990 - acc: 0.8540 - val_loss: 0.7322 - val_acc: 0.8474\n",
      "Epoch 146/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7015 - acc: 0.8532 - val_loss: 0.7465 - val_acc: 0.8438\n",
      "Epoch 147/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6999 - acc: 0.8537 - val_loss: 0.7067 - val_acc: 0.8492\n",
      "Epoch 148/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.6982 - acc: 0.8546 - val_loss: 0.7524 - val_acc: 0.8386\n",
      "Epoch 149/200\n",
      "45000/45000 [==============================] - 18s 391us/step - loss: 0.6986 - acc: 0.8554 - val_loss: 0.8763 - val_acc: 0.8024\n",
      "Epoch 150/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.6983 - acc: 0.8552 - val_loss: 0.7136 - val_acc: 0.8518\n",
      "Epoch 151/200\n",
      "45000/45000 [==============================] - 18s 393us/step - loss: 0.7039 - acc: 0.8535 - val_loss: 0.7045 - val_acc: 0.8596\n",
      "Epoch 152/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6944 - acc: 0.8560 - val_loss: 0.7448 - val_acc: 0.8402\n",
      "Epoch 153/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.6923 - acc: 0.8558 - val_loss: 0.8060 - val_acc: 0.8224\n",
      "Epoch 154/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.6897 - acc: 0.8581 - val_loss: 0.8090 - val_acc: 0.8310\n",
      "Epoch 155/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6977 - acc: 0.8547 - val_loss: 0.7057 - val_acc: 0.8544\n",
      "Epoch 156/200\n",
      "45000/45000 [==============================] - 17s 389us/step - loss: 0.7022 - acc: 0.8546 - val_loss: 0.7716 - val_acc: 0.8340\n",
      "Epoch 157/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7000 - acc: 0.8551 - val_loss: 0.7086 - val_acc: 0.8524\n",
      "Epoch 158/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6982 - acc: 0.8548 - val_loss: 0.8265 - val_acc: 0.8170\n",
      "Epoch 159/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.6961 - acc: 0.8568 - val_loss: 0.7234 - val_acc: 0.8514\n",
      "Epoch 160/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.6946 - acc: 0.8556 - val_loss: 0.7944 - val_acc: 0.8346\n",
      "Epoch 161/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.6925 - acc: 0.8547 - val_loss: 0.8662 - val_acc: 0.8048\n",
      "Epoch 162/200\n",
      "45000/45000 [==============================] - 18s 393us/step - loss: 0.6940 - acc: 0.8569 - val_loss: 0.7498 - val_acc: 0.8414\n",
      "Epoch 163/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.7032 - acc: 0.8546 - val_loss: 0.7565 - val_acc: 0.8394\n",
      "Epoch 164/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6926 - acc: 0.8557 - val_loss: 0.9420 - val_acc: 0.7720\n",
      "Epoch 165/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.6949 - acc: 0.8559 - val_loss: 0.7703 - val_acc: 0.8326\n",
      "Epoch 166/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.6923 - acc: 0.8567 - val_loss: 0.7338 - val_acc: 0.8466\n",
      "Epoch 167/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6940 - acc: 0.8571 - val_loss: 0.6923 - val_acc: 0.8580\n",
      "Epoch 168/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6993 - acc: 0.8540 - val_loss: 0.7613 - val_acc: 0.8316\n",
      "Epoch 169/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6970 - acc: 0.8540 - val_loss: 0.7229 - val_acc: 0.8486\n",
      "Epoch 170/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6962 - acc: 0.8550 - val_loss: 0.7292 - val_acc: 0.8476\n",
      "Epoch 171/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6904 - acc: 0.8583 - val_loss: 0.7437 - val_acc: 0.8446\n",
      "Epoch 172/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.7005 - acc: 0.8545 - val_loss: 1.0157 - val_acc: 0.7654\n",
      "Epoch 173/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6925 - acc: 0.8559 - val_loss: 0.8152 - val_acc: 0.8218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.6968 - acc: 0.8552 - val_loss: 0.7722 - val_acc: 0.8318\n",
      "Epoch 175/200\n",
      "45000/45000 [==============================] - 18s 400us/step - loss: 0.6933 - acc: 0.8548 - val_loss: 0.7563 - val_acc: 0.8386\n",
      "Epoch 176/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.6965 - acc: 0.8542 - val_loss: 0.7529 - val_acc: 0.8358\n",
      "Epoch 177/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.6971 - acc: 0.8546 - val_loss: 0.7164 - val_acc: 0.8496\n",
      "Epoch 178/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6962 - acc: 0.8540 - val_loss: 0.6946 - val_acc: 0.8584\n",
      "Epoch 179/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.7007 - acc: 0.8549 - val_loss: 0.7209 - val_acc: 0.8482\n",
      "Epoch 180/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.6925 - acc: 0.8569 - val_loss: 0.6994 - val_acc: 0.8528\n",
      "Epoch 181/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6955 - acc: 0.8548 - val_loss: 0.7262 - val_acc: 0.8466\n",
      "Epoch 182/200\n",
      "45000/45000 [==============================] - 18s 393us/step - loss: 0.6956 - acc: 0.8563 - val_loss: 0.7180 - val_acc: 0.8476\n",
      "Epoch 183/200\n",
      "45000/45000 [==============================] - 17s 385us/step - loss: 0.6967 - acc: 0.8538 - val_loss: 0.6999 - val_acc: 0.8560\n",
      "Epoch 184/200\n",
      "45000/45000 [==============================] - 16s 353us/step - loss: 0.6924 - acc: 0.8576 - val_loss: 0.6819 - val_acc: 0.8636\n",
      "Epoch 185/200\n",
      "45000/45000 [==============================] - 16s 358us/step - loss: 0.6841 - acc: 0.8596 - val_loss: 0.7711 - val_acc: 0.8358\n",
      "Epoch 186/200\n",
      "45000/45000 [==============================] - 16s 347us/step - loss: 0.6902 - acc: 0.8574 - val_loss: 0.7126 - val_acc: 0.8506\n",
      "Epoch 187/200\n",
      "45000/45000 [==============================] - 16s 345us/step - loss: 0.6917 - acc: 0.8570 - val_loss: 0.7521 - val_acc: 0.8424\n",
      "Epoch 188/200\n",
      "45000/45000 [==============================] - 17s 373us/step - loss: 0.6893 - acc: 0.8568 - val_loss: 0.7160 - val_acc: 0.8468\n",
      "Epoch 189/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6889 - acc: 0.8565 - val_loss: 0.7704 - val_acc: 0.8300\n",
      "Epoch 190/200\n",
      "45000/45000 [==============================] - 18s 395us/step - loss: 0.6909 - acc: 0.8549 - val_loss: 0.6981 - val_acc: 0.8534\n",
      "Epoch 191/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6959 - acc: 0.8548 - val_loss: 0.7438 - val_acc: 0.8402\n",
      "Epoch 192/200\n",
      "45000/45000 [==============================] - 18s 398us/step - loss: 0.6926 - acc: 0.8557 - val_loss: 0.9432 - val_acc: 0.7874\n",
      "Epoch 193/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6947 - acc: 0.8551 - val_loss: 0.6826 - val_acc: 0.8578\n",
      "Epoch 194/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6897 - acc: 0.8552 - val_loss: 0.7161 - val_acc: 0.8464\n",
      "Epoch 195/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6916 - acc: 0.8563 - val_loss: 0.6978 - val_acc: 0.8526\n",
      "Epoch 196/200\n",
      "45000/45000 [==============================] - 18s 396us/step - loss: 0.6912 - acc: 0.8575 - val_loss: 0.7278 - val_acc: 0.8430\n",
      "Epoch 197/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6910 - acc: 0.8562 - val_loss: 0.7107 - val_acc: 0.8480\n",
      "Epoch 198/200\n",
      "45000/45000 [==============================] - 18s 394us/step - loss: 0.6970 - acc: 0.8548 - val_loss: 0.7665 - val_acc: 0.8352\n",
      "Epoch 199/200\n",
      "45000/45000 [==============================] - 18s 399us/step - loss: 0.6957 - acc: 0.8566 - val_loss: 0.7870 - val_acc: 0.8302\n",
      "Epoch 200/200\n",
      "45000/45000 [==============================] - 18s 397us/step - loss: 0.6887 - acc: 0.8594 - val_loss: 0.7579 - val_acc: 0.8370\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    " \n",
    "train_history=model_1.fit(x_train,y_train,epochs=200, batch_size=128,\n",
    "             validation_split=0.1, verbose=1)\n",
    "\n",
    "model_1.save('8.7.6ent_b_cifar10vgg16_1_1.h5')\n",
    "model_1.save_weights('8.7.6ent_b_cifar10vgg16_1_1_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.767930428981781, 0.8359, 0.9914, 0.8359]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import keras\n",
    "top1_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=1) #top-1精度\n",
    "\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',metrics=['accuracy','top_k_categorical_accuracy',top1_acc])\n",
    "\n",
    "model_1.evaluate(x_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试损失为：0.7679\n",
      "测试准确率为：0.8359\n"
     ]
    }
   ],
   "source": [
    "score=model_1.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"测试损失为：{:.4f}\".format(score[0]))\n",
    "print(\"测试准确率为：{:.4f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
