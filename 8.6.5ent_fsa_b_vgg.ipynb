{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers import Conv2D,MaxPooling2D,ZeroPadding2D\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    " \n",
    "weight_decay = 0.0005\n",
    "nb_epoch=200\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 52)          59956     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 103)         48307     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               53248     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,224,881\n",
      "Trainable params: 1,220,517\n",
      "Non-trainable params: 4,364\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('0.8ent_b_cifar10vgg16_5_1.h5')\n",
    "model.load_weights('0.8ent_b_cifar10vgg16_5_1_weights.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [11.317330198587818, 40.18912508750436, 35.133658431381]\n",
      "1 [11.368109942889792, 15.98829962167586, 14.694114534828621]\n",
      "2 [23.273253092782742, 17.510516583008037, 3.803675710884699]\n",
      "3 [21.952739258767686, 33.63080155478253, 19.092064965748783]\n",
      "4 [6.100081602316809, 16.160523725558853, 14.702702742218024]\n",
      "5 [3.6487042413281507, 8.170473828367138, 11.896150746205537]\n",
      "6 [15.452362165146438, 31.669089617808154, 19.548816233842953]\n",
      "7 [3.3438065069539946, 4.972126922842586, 11.81860206063575]\n",
      "8 [43.47361966135982, 26.618113545039105, 10.355292665185308]\n",
      "9 [19.003342685791953, 15.276291867832363, 15.350305528116323]\n",
      "10 [13.643831206153008, 12.989342952156996, 12.765431219378767]\n",
      "11 [6.024644030743565, 15.029895499380736, 5.346327355328902]\n",
      "12 [36.47754378151908, 21.186782104573012, 25.17772241511781]\n",
      "13 [17.756974716999476, 23.84407443204569, 7.271462734783472]\n",
      "14 [17.468563563353428, 14.155726341010451, 16.343239945864486]\n",
      "15 [12.511409444853452, 8.756648834533696, 1.8983128196684138]\n",
      "16 [20.182270565555164, 18.305413368932232, 19.877383443082618]\n",
      "17 [12.45410200280693, 14.537922686912458, 1.9006582073708933]\n",
      "18 [20.794078762160275, 26.110432556029007, 21.202567107943743]\n",
      "19 [10.318762987060508, 37.918299785837995, 16.589177250525875]\n",
      "20 [7.079866611872913, 7.63573088439581, 16.670208419544974]\n",
      "21 [22.45819218319314, 6.546647734782988, 38.66316028158187]\n",
      "22 [9.950592581197363, 21.108632874449924, 7.455335892939896]\n",
      "23 [24.1918253419591, 28.005052779048693, 18.556597609330076]\n",
      "24 [0.7253868682221731, 16.475251827138763, 18.470909599125523]\n",
      "25 [3.3189584641439316, 23.8980352711444, 20.406465482523377]\n",
      "26 [19.015764698277103, 19.070334803698294, 5.4812043852522905]\n",
      "27 [16.848438806400214, 21.05661333721188, 21.01837139123269]\n",
      "28 [21.587593492792536, 32.608412110132534, 13.57719184645204]\n",
      "29 [17.397843993555192, 3.9114218691951854, 3.707656566429036]\n",
      "30 [4.648575378424748, 15.625476097482926, 14.645860271329601]\n",
      "31 [8.5081619737377, 10.951629558351941, 12.614452899517877]\n",
      "32 [13.162635793827716, 12.175859359867609, 10.55642941075707]\n",
      "33 [8.08819799108624, 1.9684334152235012, 6.0024464507218696]\n",
      "34 [13.22083877852302, 13.346631491656323, 5.4866120701907075]\n",
      "35 [23.13191882580956, 28.44673813220496, 28.607075293056194]\n",
      "36 [156.54169023378404, 108.61583914280325, 118.47170515306598]\n",
      "37 [9.835267423631247, 10.193010676907285, 15.321698069226803]\n",
      "38 [9.174122341701631, 3.4969721367292927, 14.793402364020233]\n",
      "39 [44.471229082008215, 32.98090486329638, 10.539980656788863]\n",
      "40 [41.41106993062449, 17.518567466805607, 57.50687798537667]\n",
      "41 [4.532618717126444, 14.801403283271918, 21.364809529486372]\n",
      "42 [16.305976520612194, 9.862017040248794, 10.425309789066128]\n",
      "43 [38.72768616099497, 21.663740439760364, 25.02633988397587]\n",
      "44 [18.07826197250029, 23.34625065290002, 5.170487687436266]\n",
      "45 [10.530072879376036, 29.01435417058682, 6.137865800697488]\n",
      "46 [16.66529938175674, 27.782081857591574, 17.941044952256316]\n",
      "47 [32.07857581860973, 43.39994951939614, 20.72544244126012]\n",
      "48 [1.720840015660105, 1.9253243606566124, 15.008689572760767]\n",
      "49 [15.797113173412708, 19.68028955572225, 15.815548930788895]\n",
      "50 [5.551798266260925, 9.528816988230513, 34.42509621999421]\n",
      "51 [33.22644849480059, 19.301555309975235, 26.844274122741332]\n",
      "52 [13.540005782561426, 20.34630525307167, 8.645104735287724]\n",
      "53 [7.19482307550572, 2.6496062724002365, 10.178133748618498]\n",
      "54 [9.415787838687155, 9.049227025020768, 22.4638357505119]\n",
      "55 [11.063359803661077, 13.937405504667577, 16.643062445276502]\n",
      "56 [9.075378488366617, 22.884861659973083, 2.5029422136068487]\n",
      "57 [4.197030055064125, 13.88296465771551, 4.62129269040288]\n",
      "58 [16.758080213137845, 21.761862939737963, 22.930337010921093]\n",
      "59 [16.88387641573034, 7.568351121467273, 16.536018765431955]\n",
      "60 [8.103656965242738, 17.648623492108133, 10.335157461567851]\n",
      "61 [8.019607105072899, 12.195590849979952, 15.515487101730557]\n",
      "62 [22.83859397218138, 4.645531318096671, 14.048527842059244]\n",
      "63 [22.869378821940398, 46.67259768988727, 12.707065529458978]\n",
      "64 [6.760459778290186, 19.030349747735816, 5.86459713985609]\n",
      "65 [6.991718178454372, 42.18010636708291, 1.579967065781777]\n",
      "66 [21.831325157160656, 27.593395952348914, 27.65403300736749]\n",
      "67 [17.064040620997815, 11.183992651228543, 3.683389150331795]\n",
      "68 [5.04655828751798, 8.91302441395716, 16.547340094754322]\n",
      "69 [9.071023891785481, 16.13562599015158, 13.965969722144116]\n",
      "70 [10.018507627433733, 14.680343377777412, 12.654061220014645]\n",
      "71 [7.4654206233195, 26.942603386798034, 20.57174437711905]\n",
      "72 [10.768807160972463, 32.141659109404664, 16.828622653017213]\n",
      "73 [23.851717981205955, 14.342828725179833, 8.45467995604681]\n",
      "74 [14.890391889350619, 11.552422126684215, 15.629031370806722]\n",
      "75 [23.643177623309544, 13.555759389748383, 24.84162404302937]\n",
      "76 [15.011265104865782, 8.063231523728405, 7.68412936369918]\n",
      "77 [7.523869740714823, 17.31778714116648, 3.091533097794706]\n",
      "78 [10.674693482440668, 10.827731159748787, 6.126917869653708]\n",
      "79 [8.918373904087511, 22.32699365332356, 10.188036079577994]\n",
      "80 [26.3273049065524, 85.13739846525522, 46.00853218892869]\n",
      "81 [16.242978082730964, 4.26552031112591, 13.64309044806446]\n",
      "82 [35.69767987488412, 17.534869196142584, 15.882462737383335]\n",
      "83 [10.367979362964695, 22.22761832949186, 4.046762177651047]\n",
      "84 [81.00924347305171, 39.73964781883011, 53.22394913137239]\n",
      "85 [33.43987607267907, 13.38881725452161, 0.9276840035667494]\n",
      "86 [16.487694327457895, 16.18048630846493, 16.141473195776456]\n",
      "87 [66.23409394107624, 12.456332417094377, 44.08085710941856]\n",
      "88 [12.217611484217992, 9.747486621958066, 14.825235887294411]\n",
      "89 [13.32458678100456, 19.880413337858453, 11.76404614639311]\n",
      "90 [54.98649608253218, 52.99806379884162, 48.34294481846036]\n",
      "91 [19.296733358380724, 10.640610480613717, 19.192814901634915]\n",
      "92 [9.292386601587179, 12.383394072440598, 19.328718924128296]\n",
      "93 [12.486533120482102, 3.2317416025582193, 22.6127328231238]\n",
      "94 [13.032489950892213, 11.49044362314848, 19.52850845627045]\n",
      "95 [13.171169130863202, 30.25765106292058, 19.918119380222006]\n",
      "96 [35.7244090672732, 18.111489147045177, 25.763726584947786]\n",
      "97 [16.441303755861483, 19.849838404153836, 17.747343133151723]\n",
      "98 [11.310713885178808, 5.780589009207062, 9.320838625591914]\n",
      "99 [5.066792963779473, 14.295307894843683, 12.963249795871283]\n",
      "100 [6.225055967025721, 13.052037498026277, 13.672065149347338]\n",
      "101 [11.649518695188402, 8.284823413226276, 16.196136159235476]\n",
      "102 [45.13389926557464, 30.72611810249843, 33.784805999962146]\n",
      "103 [6.381418250964106, 13.573164162046853, 8.455616173176724]\n",
      "104 [3.44124908607989, 16.6197084882857, 10.001075066952152]\n",
      "105 [19.349684043887297, 22.21313942250262, 31.29751621299852]\n",
      "106 [3.1703007742494314, 18.866360310177864, 7.516020274722174]\n",
      "107 [10.746931941221822, 20.853354167322124, 20.39445903877081]\n",
      "108 [27.40604636313492, 39.35463764534197, 23.955566122602793]\n",
      "109 [16.52636244116205, 9.389278753326046, 8.60130233440491]\n",
      "110 [9.750598497850488, 16.21004624456061, 20.32957332532992]\n",
      "111 [1.9680508780066768, 33.40132584424429, 12.691404972563609]\n",
      "112 [38.721809124730854, 15.629441873515121, 14.916713089400075]\n",
      "113 [9.810374230140274, 31.617994414585837, 46.680390619704575]\n",
      "114 [16.57113269896469, 21.727650532506615, 5.323695917690035]\n",
      "115 [3.939691206357757, 21.440145746347756, 8.364524894944102]\n",
      "116 [17.3877478290505, 19.89519585458626, 8.30637202219894]\n",
      "117 [17.75796008604642, 15.804605978785213, 14.895858069268165]\n",
      "118 [19.11051721660439, 70.70288185710092, 19.721973911328107]\n",
      "119 [22.54034875012682, 46.78575125872733, 53.25500806800352]\n",
      "120 [19.747252949271477, 10.140366043992916, 6.15920720987142]\n",
      "121 [11.576156974050457, 22.37497781507362, 20.670697410068428]\n",
      "122 [19.09655664297526, 6.616802506129441, 8.329819583793725]\n",
      "123 [20.989131294727855, 41.27434353228402, 13.831656312210455]\n",
      "124 [8.529667550346831, 34.6268348620823, 1.3831810756370577]\n",
      "125 [28.43287570789328, 72.42524119170074, 62.928872843985936]\n",
      "126 [16.00354212802237, 16.839848818564754, 15.169707452823674]\n",
      "127 [28.539442141827575, 43.99185746884951, 20.034859413927514]\n",
      "[[11.317330198587818, 40.18912508750436, 35.133658431381], [11.368109942889792, 15.98829962167586, 14.694114534828621], [23.273253092782742, 17.510516583008037, 3.803675710884699], [21.952739258767686, 33.63080155478253, 19.092064965748783], [6.100081602316809, 16.160523725558853, 14.702702742218024], [3.6487042413281507, 8.170473828367138, 11.896150746205537], [15.452362165146438, 31.669089617808154, 19.548816233842953], [3.3438065069539946, 4.972126922842586, 11.81860206063575], [43.47361966135982, 26.618113545039105, 10.355292665185308], [19.003342685791953, 15.276291867832363, 15.350305528116323], [13.643831206153008, 12.989342952156996, 12.765431219378767], [6.024644030743565, 15.029895499380736, 5.346327355328902], [36.47754378151908, 21.186782104573012, 25.17772241511781], [17.756974716999476, 23.84407443204569, 7.271462734783472], [17.468563563353428, 14.155726341010451, 16.343239945864486], [12.511409444853452, 8.756648834533696, 1.8983128196684138], [20.182270565555164, 18.305413368932232, 19.877383443082618], [12.45410200280693, 14.537922686912458, 1.9006582073708933], [20.794078762160275, 26.110432556029007, 21.202567107943743], [10.318762987060508, 37.918299785837995, 16.589177250525875], [7.079866611872913, 7.63573088439581, 16.670208419544974], [22.45819218319314, 6.546647734782988, 38.66316028158187], [9.950592581197363, 21.108632874449924, 7.455335892939896], [24.1918253419591, 28.005052779048693, 18.556597609330076], [0.7253868682221731, 16.475251827138763, 18.470909599125523], [3.3189584641439316, 23.8980352711444, 20.406465482523377], [19.015764698277103, 19.070334803698294, 5.4812043852522905], [16.848438806400214, 21.05661333721188, 21.01837139123269], [21.587593492792536, 32.608412110132534, 13.57719184645204], [17.397843993555192, 3.9114218691951854, 3.707656566429036], [4.648575378424748, 15.625476097482926, 14.645860271329601], [8.5081619737377, 10.951629558351941, 12.614452899517877], [13.162635793827716, 12.175859359867609, 10.55642941075707], [8.08819799108624, 1.9684334152235012, 6.0024464507218696], [13.22083877852302, 13.346631491656323, 5.4866120701907075], [23.13191882580956, 28.44673813220496, 28.607075293056194], [156.54169023378404, 108.61583914280325, 118.47170515306598], [9.835267423631247, 10.193010676907285, 15.321698069226803], [9.174122341701631, 3.4969721367292927, 14.793402364020233], [44.471229082008215, 32.98090486329638, 10.539980656788863], [41.41106993062449, 17.518567466805607, 57.50687798537667], [4.532618717126444, 14.801403283271918, 21.364809529486372], [16.305976520612194, 9.862017040248794, 10.425309789066128], [38.72768616099497, 21.663740439760364, 25.02633988397587], [18.07826197250029, 23.34625065290002, 5.170487687436266], [10.530072879376036, 29.01435417058682, 6.137865800697488], [16.66529938175674, 27.782081857591574, 17.941044952256316], [32.07857581860973, 43.39994951939614, 20.72544244126012], [1.720840015660105, 1.9253243606566124, 15.008689572760767], [15.797113173412708, 19.68028955572225, 15.815548930788895], [5.551798266260925, 9.528816988230513, 34.42509621999421], [33.22644849480059, 19.301555309975235, 26.844274122741332], [13.540005782561426, 20.34630525307167, 8.645104735287724], [7.19482307550572, 2.6496062724002365, 10.178133748618498], [9.415787838687155, 9.049227025020768, 22.4638357505119], [11.063359803661077, 13.937405504667577, 16.643062445276502], [9.075378488366617, 22.884861659973083, 2.5029422136068487], [4.197030055064125, 13.88296465771551, 4.62129269040288], [16.758080213137845, 21.761862939737963, 22.930337010921093], [16.88387641573034, 7.568351121467273, 16.536018765431955], [8.103656965242738, 17.648623492108133, 10.335157461567851], [8.019607105072899, 12.195590849979952, 15.515487101730557], [22.83859397218138, 4.645531318096671, 14.048527842059244], [22.869378821940398, 46.67259768988727, 12.707065529458978], [6.760459778290186, 19.030349747735816, 5.86459713985609], [6.991718178454372, 42.18010636708291, 1.579967065781777], [21.831325157160656, 27.593395952348914, 27.65403300736749], [17.064040620997815, 11.183992651228543, 3.683389150331795], [5.04655828751798, 8.91302441395716, 16.547340094754322], [9.071023891785481, 16.13562599015158, 13.965969722144116], [10.018507627433733, 14.680343377777412, 12.654061220014645], [7.4654206233195, 26.942603386798034, 20.57174437711905], [10.768807160972463, 32.141659109404664, 16.828622653017213], [23.851717981205955, 14.342828725179833, 8.45467995604681], [14.890391889350619, 11.552422126684215, 15.629031370806722], [23.643177623309544, 13.555759389748383, 24.84162404302937], [15.011265104865782, 8.063231523728405, 7.68412936369918], [7.523869740714823, 17.31778714116648, 3.091533097794706], [10.674693482440668, 10.827731159748787, 6.126917869653708], [8.918373904087511, 22.32699365332356, 10.188036079577994], [26.3273049065524, 85.13739846525522, 46.00853218892869], [16.242978082730964, 4.26552031112591, 13.64309044806446], [35.69767987488412, 17.534869196142584, 15.882462737383335], [10.367979362964695, 22.22761832949186, 4.046762177651047], [81.00924347305171, 39.73964781883011, 53.22394913137239], [33.43987607267907, 13.38881725452161, 0.9276840035667494], [16.487694327457895, 16.18048630846493, 16.141473195776456], [66.23409394107624, 12.456332417094377, 44.08085710941856], [12.217611484217992, 9.747486621958066, 14.825235887294411], [13.32458678100456, 19.880413337858453, 11.76404614639311], [54.98649608253218, 52.99806379884162, 48.34294481846036], [19.296733358380724, 10.640610480613717, 19.192814901634915], [9.292386601587179, 12.383394072440598, 19.328718924128296], [12.486533120482102, 3.2317416025582193, 22.6127328231238], [13.032489950892213, 11.49044362314848, 19.52850845627045], [13.171169130863202, 30.25765106292058, 19.918119380222006], [35.7244090672732, 18.111489147045177, 25.763726584947786], [16.441303755861483, 19.849838404153836, 17.747343133151723], [11.310713885178808, 5.780589009207062, 9.320838625591914], [5.066792963779473, 14.295307894843683, 12.963249795871283], [6.225055967025721, 13.052037498026277, 13.672065149347338], [11.649518695188402, 8.284823413226276, 16.196136159235476], [45.13389926557464, 30.72611810249843, 33.784805999962146], [6.381418250964106, 13.573164162046853, 8.455616173176724], [3.44124908607989, 16.6197084882857, 10.001075066952152], [19.349684043887297, 22.21313942250262, 31.29751621299852], [3.1703007742494314, 18.866360310177864, 7.516020274722174], [10.746931941221822, 20.853354167322124, 20.39445903877081], [27.40604636313492, 39.35463764534197, 23.955566122602793], [16.52636244116205, 9.389278753326046, 8.60130233440491], [9.750598497850488, 16.21004624456061, 20.32957332532992], [1.9680508780066768, 33.40132584424429, 12.691404972563609], [38.721809124730854, 15.629441873515121, 14.916713089400075], [9.810374230140274, 31.617994414585837, 46.680390619704575], [16.57113269896469, 21.727650532506615, 5.323695917690035], [3.939691206357757, 21.440145746347756, 8.364524894944102], [17.3877478290505, 19.89519585458626, 8.30637202219894], [17.75796008604642, 15.804605978785213, 14.895858069268165], [19.11051721660439, 70.70288185710092, 19.721973911328107], [22.54034875012682, 46.78575125872733, 53.25500806800352], [19.747252949271477, 10.140366043992916, 6.15920720987142], [11.576156974050457, 22.37497781507362, 20.670697410068428], [19.09655664297526, 6.616802506129441, 8.329819583793725], [20.989131294727855, 41.27434353228402, 13.831656312210455], [8.529667550346831, 34.6268348620823, 1.3831810756370577], [28.43287570789328, 72.42524119170074, 62.928872843985936], [16.00354212802237, 16.839848818564754, 15.169707452823674], [28.539442141827575, 43.99185746884951, 20.034859413927514]]\n"
     ]
    }
   ],
   "source": [
    "tensor =[]\n",
    "for i in range(0,128):\n",
    "   \n",
    "    u=model.get_layer('conv2d_4').get_weights()[0][:,:,:,i].squeeze()\n",
    "    v=np.mean(u, axis=1)\n",
    "    #print(v)\n",
    "    vT=v.T\n",
    "    D=np.cov(vT)\n",
    "    try:                 \n",
    "        invD=np.linalg.inv(D)\n",
    "        #t=np.mean(v,axis=1)\n",
    "        a=[]\n",
    "\n",
    "        for j in range(0,2): #表示filter的大小\n",
    "            for k in range(j+1,3):\n",
    "               #if (j!=k) and (j<k):\n",
    "                tp=v[j]-v[k]\n",
    "                d=np.sqrt(abs(np.dot(np.dot(tp,invD),tp.T)))\n",
    "                a.append(d)\n",
    "            \n",
    "        print(i,a)\n",
    "        tensor.append(a)\n",
    "    except:\n",
    "        print(\"不可逆\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [200. 100. 100.]\n",
      "{160.0, 120.0, 110.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{40.0, 20.0, 60.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{50.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{90.0, 50.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [100.   0. 100.]\n",
      "{80.0, 40.0, 50.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [100.   0.   0.]\n",
      "{40.0, 10.0, 70.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [100. 100.   0.]\n",
      "{50.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{50.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 50.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{20.0, 70.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{50.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [  0. 100. 100.]\n",
      "{70.0, 60.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "[0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.0, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.0, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.0, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.0, 1.584962500721156, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.0, 1.584962500721156]\n"
     ]
    }
   ],
   "source": [
    "entt=[]\n",
    "for i in range (0,128):\n",
    "    \n",
    "    data=tensor[i]\n",
    "    data0=np.array(data)\n",
    "    print('四舍五入，精确到个位\\n',np.round(data0,decimals=-2))\n",
    "    data1=np.round(data0,decimals=-1)\n",
    "\n",
    "    data1_value_list=set([data1[i] for i in range (data1.shape[0])])\n",
    "    print(data1_value_list)\n",
    "    ent=0.0\n",
    "    for data1_value in data1_value_list:\n",
    "        p=float(data1[data1==data1_value].shape[0])/data1.shape[0]\n",
    "        print(p)\n",
    "        logp=np.log2(p)\n",
    "        ent-=p*logp\n",
    "        print(ent)   \n",
    "    entt.append(ent)\n",
    "print(entt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896]\n",
      "[9, 10, 16, 27, 31, 32, 34, 49, 58, 70, 78, 86, 88, 90, 97, 98, 99, 100, 103, 126, 0, 1, 2, 3, 4, 5, 6, 7, 11, 13, 14, 15, 17, 18, 20, 22, 23, 24, 25, 26, 29, 33, 35, 37, 38, 42, 44, 45, 46, 48, 50, 51, 52, 53, 54, 55, 57, 59, 60, 61, 64, 66, 68, 69, 73, 74, 75, 76, 79, 82, 89, 91, 92, 94, 101, 102]\n"
     ]
    }
   ],
   "source": [
    "import heapq #获取list中最小的\n",
    "\n",
    "f=int(len(entt)*0.6) #计算滤波器熵个数的80%\n",
    "m=entt\n",
    "max_number=heapq.nsmallest(f,m) #从m中找出最小的f个数，最大用nlargest\n",
    "max_index=[]\n",
    "for t in max_number:\n",
    "    index=m.index(t)\n",
    "    max_index.append(index)\n",
    "    m[index]=float('-inf')\n",
    "print(max_number)#输出最小的f个数\n",
    "print(max_index)#输出对应索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 76/128 channels from layer: conv2d_4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 52)        59956     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 52)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 103)         48307     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               53248     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,101,381\n",
      "Trainable params: 1,097,169\n",
      "Non-trainable params: 4,212\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerassurgeon.operations import delete_layer,insert_layer,delete_channels\n",
    "model_4 = delete_channels(model, model.layers[13],max_index)\n",
    "model_4.summary()\n",
    "model_4.save('8.7.6ent_b_cifar10vgg16_4.h5')\n",
    "model_4.save_weights('8.7.6ent_b_cifar10vgg16__weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "45000/45000 [==============================] - 30s 659us/step - loss: 0.8101 - acc: 0.8225 - val_loss: 0.8679 - val_acc: 0.8022\n",
      "Epoch 2/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.7165 - acc: 0.8538 - val_loss: 0.7579 - val_acc: 0.8420\n",
      "Epoch 3/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.7070 - acc: 0.8585 - val_loss: 0.7092 - val_acc: 0.8584cc: 0.858\n",
      "Epoch 4/200\n",
      "45000/45000 [==============================] - 22s 479us/step - loss: 0.6880 - acc: 0.8653 - val_loss: 0.7699 - val_acc: 0.8412\n",
      "Epoch 5/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6777 - acc: 0.8693 - val_loss: 0.7166 - val_acc: 0.8622\n",
      "Epoch 6/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6809 - acc: 0.8718 - val_loss: 0.7268 - val_acc: 0.8540\n",
      "Epoch 7/200\n",
      "45000/45000 [==============================] - 21s 476us/step - loss: 0.6795 - acc: 0.8710 - val_loss: 0.8566 - val_acc: 0.8248\n",
      "Epoch 8/200\n",
      "45000/45000 [==============================] - 21s 476us/step - loss: 0.6748 - acc: 0.8714 - val_loss: 0.7621 - val_acc: 0.8516\n",
      "Epoch 9/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6725 - acc: 0.8720 - val_loss: 0.7366 - val_acc: 0.8594\n",
      "Epoch 10/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6727 - acc: 0.8737 - val_loss: 0.7371 - val_acc: 0.8638\n",
      "Epoch 11/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6634 - acc: 0.8768 - val_loss: 0.7742 - val_acc: 0.8500\n",
      "Epoch 12/200\n",
      "45000/45000 [==============================] - 22s 479us/step - loss: 0.6609 - acc: 0.8792 - val_loss: 0.7610 - val_acc: 0.8556\n",
      "Epoch 13/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6634 - acc: 0.8779 - val_loss: 0.7781 - val_acc: 0.8520\n",
      "Epoch 14/200\n",
      "45000/45000 [==============================] - 22s 479us/step - loss: 0.6566 - acc: 0.8782 - val_loss: 0.7401 - val_acc: 0.8580\n",
      "Epoch 15/200\n",
      "45000/45000 [==============================] - 21s 476us/step - loss: 0.6482 - acc: 0.8826 - val_loss: 0.7607 - val_acc: 0.8534\n",
      "Epoch 16/200\n",
      "45000/45000 [==============================] - 21s 476us/step - loss: 0.6610 - acc: 0.8778 - val_loss: 0.7556 - val_acc: 0.8482\n",
      "Epoch 17/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6576 - acc: 0.8805 - val_loss: 0.8006 - val_acc: 0.8348\n",
      "Epoch 18/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6514 - acc: 0.8807 - val_loss: 0.7492 - val_acc: 0.8552\n",
      "Epoch 19/200\n",
      "45000/45000 [==============================] - 22s 478us/step - loss: 0.6604 - acc: 0.8788 - val_loss: 1.2816 - val_acc: 0.7244\n",
      "Epoch 20/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6520 - acc: 0.8823 - val_loss: 0.6909 - val_acc: 0.8728  - ETA: 1\n",
      "Epoch 21/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6439 - acc: 0.8854 - val_loss: 0.8500 - val_acc: 0.8214\n",
      "Epoch 22/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6527 - acc: 0.8810 - val_loss: 0.7041 - val_acc: 0.8706\n",
      "Epoch 23/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6416 - acc: 0.8859 - val_loss: 0.7277 - val_acc: 0.8612\n",
      "Epoch 24/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6491 - acc: 0.8845 - val_loss: 0.7036 - val_acc: 0.8676\n",
      "Epoch 25/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6536 - acc: 0.8836 - val_loss: 0.7278 - val_acc: 0.8628\n",
      "Epoch 26/200\n",
      "45000/45000 [==============================] - 21s 478us/step - loss: 0.6473 - acc: 0.8847 - val_loss: 0.7722 - val_acc: 0.8534\n",
      "Epoch 27/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6457 - acc: 0.8848 - val_loss: 0.9259 - val_acc: 0.8152\n",
      "Epoch 28/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6381 - acc: 0.8870 - val_loss: 0.7429 - val_acc: 0.8618\n",
      "Epoch 29/200\n",
      "45000/45000 [==============================] - 21s 476us/step - loss: 0.6438 - acc: 0.8847 - val_loss: 0.7064 - val_acc: 0.8750\n",
      "Epoch 30/200\n",
      "45000/45000 [==============================] - 22s 478us/step - loss: 0.6525 - acc: 0.8822 - val_loss: 0.7686 - val_acc: 0.8518\n",
      "Epoch 31/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6408 - acc: 0.8856 - val_loss: 0.7368 - val_acc: 0.8632\n",
      "Epoch 32/200\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 0.6523 - acc: 0.8834 - val_loss: 0.7893 - val_acc: 0.8478s - loss: 0.6498 - - ETA: 1s - loss: \n",
      "Epoch 33/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6407 - acc: 0.8855 - val_loss: 0.8190 - val_acc: 0.8402\n",
      "Epoch 34/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6471 - acc: 0.8864 - val_loss: 0.7097 - val_acc: 0.8698 - ETA: 0s - loss: 0.6470 - acc: 0.886\n",
      "Epoch 35/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6349 - acc: 0.8891 - val_loss: 0.7953 - val_acc: 0.8474\n",
      "Epoch 36/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6372 - acc: 0.8865 - val_loss: 0.7495 - val_acc: 0.8558\n",
      "Epoch 37/200\n",
      "45000/45000 [==============================] - 22s 478us/step - loss: 0.6411 - acc: 0.8870 - val_loss: 0.8388 - val_acc: 0.8282\n",
      "Epoch 38/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6415 - acc: 0.8865 - val_loss: 0.7217 - val_acc: 0.8654\n",
      "Epoch 39/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6557 - acc: 0.8821 - val_loss: 0.7804 - val_acc: 0.8472\n",
      "Epoch 40/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6441 - acc: 0.8859 - val_loss: 0.7610 - val_acc: 0.8576\n",
      "Epoch 41/200\n",
      "45000/45000 [==============================] - 21s 476us/step - loss: 0.6392 - acc: 0.8901 - val_loss: 0.8004 - val_acc: 0.8476\n",
      "Epoch 42/200\n",
      "45000/45000 [==============================] - 21s 474us/step - loss: 0.6534 - acc: 0.8833 - val_loss: 0.7337 - val_acc: 0.8620\n",
      "Epoch 43/200\n",
      "45000/45000 [==============================] - 22s 478us/step - loss: 0.6485 - acc: 0.8872 - val_loss: 0.7373 - val_acc: 0.86521s - loss: \n",
      "Epoch 44/200\n",
      "45000/45000 [==============================] - 22s 478us/step - loss: 0.6432 - acc: 0.8878 - val_loss: 0.8914 - val_acc: 0.8174\n",
      "Epoch 45/200\n",
      "45000/45000 [==============================] - 21s 478us/step - loss: 0.6458 - acc: 0.8863 - val_loss: 0.7531 - val_acc: 0.8568\n",
      "Epoch 46/200\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 0.6335 - acc: 0.8893 - val_loss: 0.7058 - val_acc: 0.8708\n",
      "Epoch 47/200\n",
      "45000/45000 [==============================] - 22s 479us/step - loss: 0.6370 - acc: 0.8895 - val_loss: 0.7791 - val_acc: 0.8584\n",
      "Epoch 48/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6319 - acc: 0.8899 - val_loss: 0.8600 - val_acc: 0.8324\n",
      "Epoch 49/200\n",
      "45000/45000 [==============================] - 21s 476us/step - loss: 0.6309 - acc: 0.8888 - val_loss: 0.7218 - val_acc: 0.8650\n",
      "Epoch 50/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6376 - acc: 0.8873 - val_loss: 0.7224 - val_acc: 0.8666\n",
      "Epoch 51/200\n",
      "45000/45000 [==============================] - 20s 454us/step - loss: 0.6385 - acc: 0.8886 - val_loss: 0.7612 - val_acc: 0.8496\n",
      "Epoch 52/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6387 - acc: 0.8873 - val_loss: 0.8056 - val_acc: 0.8404\n",
      "Epoch 53/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6382 - acc: 0.8894 - val_loss: 0.7803 - val_acc: 0.8474\n",
      "Epoch 54/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6392 - acc: 0.8895 - val_loss: 0.9410 - val_acc: 0.8112\n",
      "Epoch 55/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6386 - acc: 0.8876 - val_loss: 0.8152 - val_acc: 0.8448\n",
      "Epoch 56/200\n",
      "45000/45000 [==============================] - 22s 479us/step - loss: 0.6495 - acc: 0.8843 - val_loss: 0.7477 - val_acc: 0.8670- loss: 0.6435 - a \n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6360 - acc: 0.8894 - val_loss: 0.7273 - val_acc: 0.8684\n",
      "Epoch 58/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6322 - acc: 0.8911 - val_loss: 0.7653 - val_acc: 0.8546\n",
      "Epoch 59/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6410 - acc: 0.8878 - val_loss: 0.7665 - val_acc: 0.8540\n",
      "Epoch 60/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6264 - acc: 0.8918 - val_loss: 0.7229 - val_acc: 0.8632\n",
      "Epoch 61/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6352 - acc: 0.8890 - val_loss: 0.9751 - val_acc: 0.7998\n",
      "Epoch 62/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6422 - acc: 0.8874 - val_loss: 0.7872 - val_acc: 0.8548\n",
      "Epoch 63/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6362 - acc: 0.8895 - val_loss: 0.7783 - val_acc: 0.8498\n",
      "Epoch 64/200\n",
      "45000/45000 [==============================] - 22s 478us/step - loss: 0.6451 - acc: 0.8880 - val_loss: 0.7313 - val_acc: 0.8654\n",
      "Epoch 65/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6348 - acc: 0.8920 - val_loss: 0.7482 - val_acc: 0.8588\n",
      "Epoch 66/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6448 - acc: 0.8867 - val_loss: 0.7159 - val_acc: 0.8662\n",
      "Epoch 67/200\n",
      "45000/45000 [==============================] - 22s 479us/step - loss: 0.6296 - acc: 0.8914 - val_loss: 0.7594 - val_acc: 0.8562\n",
      "Epoch 68/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6327 - acc: 0.8901 - val_loss: 0.7412 - val_acc: 0.8538\n",
      "Epoch 69/200\n",
      "45000/45000 [==============================] - 22s 485us/step - loss: 0.6319 - acc: 0.8893 - val_loss: 0.7470 - val_acc: 0.8612\n",
      "Epoch 70/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6363 - acc: 0.8885 - val_loss: 0.8117 - val_acc: 0.8386\n",
      "Epoch 71/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6288 - acc: 0.8904 - val_loss: 0.7438 - val_acc: 0.8568\n",
      "Epoch 72/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6284 - acc: 0.8893 - val_loss: 0.7199 - val_acc: 0.8692\n",
      "Epoch 73/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6245 - acc: 0.8929 - val_loss: 0.7316 - val_acc: 0.8656\n",
      "Epoch 74/200\n",
      "45000/45000 [==============================] - 22s 485us/step - loss: 0.6284 - acc: 0.8913 - val_loss: 0.8205 - val_acc: 0.8432\n",
      "Epoch 75/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6280 - acc: 0.8921 - val_loss: 0.7314 - val_acc: 0.8654\n",
      "Epoch 76/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6337 - acc: 0.8904 - val_loss: 0.7328 - val_acc: 0.8672\n",
      "Epoch 77/200\n",
      "45000/45000 [==============================] - 21s 475us/step - loss: 0.6317 - acc: 0.8903 - val_loss: 0.7864 - val_acc: 0.8538\n",
      "Epoch 78/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6329 - acc: 0.8910 - val_loss: 0.8791 - val_acc: 0.8170\n",
      "Epoch 79/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6353 - acc: 0.8902 - val_loss: 0.7651 - val_acc: 0.8566\n",
      "Epoch 80/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6304 - acc: 0.8908 - val_loss: 0.7082 - val_acc: 0.8740\n",
      "Epoch 81/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6294 - acc: 0.8919 - val_loss: 0.9486 - val_acc: 0.8096\n",
      "Epoch 82/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6389 - acc: 0.8893 - val_loss: 0.7072 - val_acc: 0.8648\n",
      "Epoch 83/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6242 - acc: 0.8942 - val_loss: 0.7866 - val_acc: 0.8532\n",
      "Epoch 84/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6343 - acc: 0.8924 - val_loss: 0.7293 - val_acc: 0.8620\n",
      "Epoch 85/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6244 - acc: 0.8936 - val_loss: 0.8649 - val_acc: 0.8220\n",
      "Epoch 86/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6371 - acc: 0.8892 - val_loss: 0.7430 - val_acc: 0.8578\n",
      "Epoch 87/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6260 - acc: 0.8915 - val_loss: 0.7578 - val_acc: 0.8590\n",
      "Epoch 88/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6243 - acc: 0.8940 - val_loss: 0.7360 - val_acc: 0.8604 ac\n",
      "Epoch 89/200\n",
      "45000/45000 [==============================] - 22s 479us/step - loss: 0.6283 - acc: 0.8924 - val_loss: 0.7597 - val_acc: 0.8586\n",
      "Epoch 90/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6200 - acc: 0.8945 - val_loss: 0.9304 - val_acc: 0.7968\n",
      "Epoch 91/200\n",
      "45000/45000 [==============================] - 22s 485us/step - loss: 0.6241 - acc: 0.8953 - val_loss: 0.7559 - val_acc: 0.8638\n",
      "Epoch 92/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6229 - acc: 0.8940 - val_loss: 0.7276 - val_acc: 0.8682\n",
      "Epoch 93/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6225 - acc: 0.8935 - val_loss: 0.7673 - val_acc: 0.8584\n",
      "Epoch 94/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6405 - acc: 0.8880 - val_loss: 0.7143 - val_acc: 0.8706\n",
      "Epoch 95/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6302 - acc: 0.8929 - val_loss: 0.7324 - val_acc: 0.8606\n",
      "Epoch 96/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6238 - acc: 0.8932 - val_loss: 0.7678 - val_acc: 0.8604\n",
      "Epoch 97/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6326 - acc: 0.8900 - val_loss: 0.7208 - val_acc: 0.8676\n",
      "Epoch 98/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6218 - acc: 0.8942 - val_loss: 0.7623 - val_acc: 0.8524\n",
      "Epoch 99/200\n",
      "45000/45000 [==============================] - 22s 479us/step - loss: 0.6249 - acc: 0.8920 - val_loss: 0.7474 - val_acc: 0.8610\n",
      "Epoch 100/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6300 - acc: 0.8905 - val_loss: 0.7317 - val_acc: 0.8646\n",
      "Epoch 101/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6318 - acc: 0.8916 - val_loss: 0.9350 - val_acc: 0.8054\n",
      "Epoch 102/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6304 - acc: 0.8915 - val_loss: 0.7149 - val_acc: 0.8674\n",
      "Epoch 103/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6338 - acc: 0.8891 - val_loss: 0.7757 - val_acc: 0.8558\n",
      "Epoch 104/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6226 - acc: 0.8926 - val_loss: 0.8175 - val_acc: 0.8376\n",
      "Epoch 105/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6317 - acc: 0.8924 - val_loss: 0.7345 - val_acc: 0.8620\n",
      "Epoch 106/200\n",
      "45000/45000 [==============================] - 22s 478us/step - loss: 0.6224 - acc: 0.8948 - val_loss: 0.8158 - val_acc: 0.8368\n",
      "Epoch 107/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6220 - acc: 0.8936 - val_loss: 0.7366 - val_acc: 0.8598\n",
      "Epoch 108/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6277 - acc: 0.8926 - val_loss: 0.9682 - val_acc: 0.8008\n",
      "Epoch 109/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6245 - acc: 0.8928 - val_loss: 0.7730 - val_acc: 0.8568\n",
      "Epoch 110/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6180 - acc: 0.8944 - val_loss: 0.8151 - val_acc: 0.8446\n",
      "Epoch 111/200\n",
      "45000/45000 [==============================] - 22s 479us/step - loss: 0.6182 - acc: 0.8963 - val_loss: 0.6992 - val_acc: 0.8760\n",
      "Epoch 112/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6304 - acc: 0.8907 - val_loss: 0.7295 - val_acc: 0.8640\n",
      "Epoch 113/200\n",
      "45000/45000 [==============================] - 22s 485us/step - loss: 0.6351 - acc: 0.8906 - val_loss: 0.7148 - val_acc: 0.8704\n",
      "Epoch 114/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6248 - acc: 0.8928 - val_loss: 0.7327 - val_acc: 0.8660\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6318 - acc: 0.8911 - val_loss: 0.7925 - val_acc: 0.8430\n",
      "Epoch 116/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6301 - acc: 0.8912 - val_loss: 0.7547 - val_acc: 0.8604\n",
      "Epoch 117/200\n",
      "45000/45000 [==============================] - 22s 478us/step - loss: 0.6270 - acc: 0.8922 - val_loss: 0.7443 - val_acc: 0.8628\n",
      "Epoch 118/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6131 - acc: 0.8954 - val_loss: 0.9722 - val_acc: 0.8024\n",
      "Epoch 119/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6274 - acc: 0.8926 - val_loss: 0.8127 - val_acc: 0.8362\n",
      "Epoch 120/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6215 - acc: 0.8943 - val_loss: 0.7341 - val_acc: 0.8638\n",
      "Epoch 121/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6340 - acc: 0.8900 - val_loss: 0.7608 - val_acc: 0.8604\n",
      "Epoch 122/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6305 - acc: 0.8919 - val_loss: 0.7815 - val_acc: 0.8498\n",
      "Epoch 123/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6257 - acc: 0.8923 - val_loss: 0.7368 - val_acc: 0.8616\n",
      "Epoch 124/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6284 - acc: 0.8912 - val_loss: 0.7710 - val_acc: 0.8534\n",
      "Epoch 125/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6180 - acc: 0.8960 - val_loss: 0.7182 - val_acc: 0.8676\n",
      "Epoch 126/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6156 - acc: 0.8944 - val_loss: 0.7966 - val_acc: 0.8512\n",
      "Epoch 127/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6227 - acc: 0.8930 - val_loss: 0.8731 - val_acc: 0.8256\n",
      "Epoch 128/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6292 - acc: 0.8917 - val_loss: 0.7861 - val_acc: 0.8582\n",
      "Epoch 129/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6198 - acc: 0.8943 - val_loss: 0.7314 - val_acc: 0.8686\n",
      "Epoch 130/200\n",
      "45000/45000 [==============================] - 22s 485us/step - loss: 0.6315 - acc: 0.8916 - val_loss: 0.7516 - val_acc: 0.8584\n",
      "Epoch 131/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6323 - acc: 0.8918 - val_loss: 0.7303 - val_acc: 0.8666\n",
      "Epoch 132/200\n",
      "45000/45000 [==============================] - 22s 485us/step - loss: 0.6259 - acc: 0.8930 - val_loss: 0.7688 - val_acc: 0.8526\n",
      "Epoch 133/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6196 - acc: 0.8937 - val_loss: 0.7945 - val_acc: 0.8528\n",
      "Epoch 134/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6227 - acc: 0.8922 - val_loss: 0.7715 - val_acc: 0.8524\n",
      "Epoch 135/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6253 - acc: 0.8926 - val_loss: 0.7661 - val_acc: 0.8566\n",
      "Epoch 136/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6243 - acc: 0.8920 - val_loss: 0.7847 - val_acc: 0.8480\n",
      "Epoch 137/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6181 - acc: 0.8971 - val_loss: 0.6966 - val_acc: 0.8734\n",
      "Epoch 138/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6180 - acc: 0.8950 - val_loss: 0.7379 - val_acc: 0.8618\n",
      "Epoch 139/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6144 - acc: 0.8971 - val_loss: 0.7309 - val_acc: 0.8630\n",
      "Epoch 140/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6163 - acc: 0.8945 - val_loss: 0.7336 - val_acc: 0.8628\n",
      "Epoch 141/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6161 - acc: 0.8955 - val_loss: 0.6900 - val_acc: 0.8740\n",
      "Epoch 142/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6194 - acc: 0.8928 - val_loss: 0.7867 - val_acc: 0.8472\n",
      "Epoch 143/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6124 - acc: 0.8964 - val_loss: 0.7392 - val_acc: 0.8598 ETA: 3s - loss: - E - ETA: 0s - loss: 0.6122 - acc: 0\n",
      "Epoch 144/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6211 - acc: 0.8947 - val_loss: 0.7466 - val_acc: 0.8568\n",
      "Epoch 145/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6163 - acc: 0.8945 - val_loss: 0.7352 - val_acc: 0.8644\n",
      "Epoch 146/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6206 - acc: 0.8948 - val_loss: 0.7341 - val_acc: 0.8668\n",
      "Epoch 147/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6234 - acc: 0.8928 - val_loss: 0.8492 - val_acc: 0.8320\n",
      "Epoch 148/200\n",
      "45000/45000 [==============================] - 22s 485us/step - loss: 0.6134 - acc: 0.8955 - val_loss: 0.8240 - val_acc: 0.8430\n",
      "Epoch 149/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6256 - acc: 0.8922 - val_loss: 0.7228 - val_acc: 0.8700\n",
      "Epoch 150/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6135 - acc: 0.8977 - val_loss: 0.7160 - val_acc: 0.8716\n",
      "Epoch 151/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6119 - acc: 0.8969 - val_loss: 0.7339 - val_acc: 0.8690\n",
      "Epoch 152/200\n",
      "45000/45000 [==============================] - 22s 487us/step - loss: 0.6129 - acc: 0.8967 - val_loss: 0.7258 - val_acc: 0.8688ss: 0.6128 - acc\n",
      "Epoch 153/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6158 - acc: 0.8952 - val_loss: 0.7225 - val_acc: 0.8700\n",
      "Epoch 154/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6124 - acc: 0.8970 - val_loss: 0.7181 - val_acc: 0.8708\n",
      "Epoch 155/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6144 - acc: 0.8948 - val_loss: 0.7240 - val_acc: 0.8752\n",
      "Epoch 156/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6158 - acc: 0.8957 - val_loss: 0.7964 - val_acc: 0.8480\n",
      "Epoch 157/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6157 - acc: 0.8949 - val_loss: 0.8347 - val_acc: 0.8366\n",
      "Epoch 158/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6164 - acc: 0.8945 - val_loss: 0.7280 - val_acc: 0.8642\n",
      "Epoch 159/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6132 - acc: 0.8963 - val_loss: 0.7343 - val_acc: 0.8658\n",
      "Epoch 160/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6127 - acc: 0.8953 - val_loss: 0.9524 - val_acc: 0.8020\n",
      "Epoch 161/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6139 - acc: 0.8967 - val_loss: 0.7379 - val_acc: 0.8680\n",
      "Epoch 162/200\n",
      "45000/45000 [==============================] - 22s 485us/step - loss: 0.6104 - acc: 0.8983 - val_loss: 0.8335 - val_acc: 0.8312\n",
      "Epoch 163/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6175 - acc: 0.8944 - val_loss: 0.7235 - val_acc: 0.8686\n",
      "Epoch 164/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6219 - acc: 0.8936 - val_loss: 0.7482 - val_acc: 0.8582\n",
      "Epoch 165/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6209 - acc: 0.8953 - val_loss: 0.7302 - val_acc: 0.8668\n",
      "Epoch 166/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6221 - acc: 0.8932 - val_loss: 0.7610 - val_acc: 0.8490\n",
      "Epoch 167/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6155 - acc: 0.8945 - val_loss: 0.7940 - val_acc: 0.8514\n",
      "Epoch 168/200\n",
      "45000/45000 [==============================] - 22s 478us/step - loss: 0.6139 - acc: 0.8956 - val_loss: 0.7340 - val_acc: 0.8626\n",
      "Epoch 169/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6155 - acc: 0.8947 - val_loss: 0.6912 - val_acc: 0.8710\n",
      "Epoch 170/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6142 - acc: 0.8964 - val_loss: 0.7371 - val_acc: 0.8602\n",
      "Epoch 171/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6058 - acc: 0.8968 - val_loss: 0.7759 - val_acc: 0.8572\n",
      "Epoch 172/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 22s 485us/step - loss: 0.6099 - acc: 0.8975 - val_loss: 0.7266 - val_acc: 0.8710\n",
      "Epoch 173/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6115 - acc: 0.8963 - val_loss: 0.7046 - val_acc: 0.8684\n",
      "Epoch 174/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6076 - acc: 0.8988 - val_loss: 0.7320 - val_acc: 0.8632\n",
      "Epoch 175/200\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 0.6115 - acc: 0.8965 - val_loss: 0.7636 - val_acc: 0.8580\n",
      "Epoch 176/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6187 - acc: 0.8942 - val_loss: 0.7574 - val_acc: 0.8566\n",
      "Epoch 177/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6129 - acc: 0.8958 - val_loss: 0.7386 - val_acc: 0.86460.89\n",
      "Epoch 178/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6109 - acc: 0.8966 - val_loss: 0.9504 - val_acc: 0.8018\n",
      "Epoch 179/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6150 - acc: 0.8957 - val_loss: 0.7987 - val_acc: 0.8482\n",
      "Epoch 180/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6112 - acc: 0.8961 - val_loss: 0.7068 - val_acc: 0.8682\n",
      "Epoch 181/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6103 - acc: 0.8958 - val_loss: 0.7486 - val_acc: 0.8562\n",
      "Epoch 182/200\n",
      "45000/45000 [==============================] - 22s 487us/step - loss: 0.6149 - acc: 0.8953 - val_loss: 0.7179 - val_acc: 0.8658\n",
      "Epoch 183/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6069 - acc: 0.8974 - val_loss: 0.7465 - val_acc: 0.8598\n",
      "Epoch 184/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6041 - acc: 0.8984 - val_loss: 0.7670 - val_acc: 0.8574\n",
      "Epoch 185/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6042 - acc: 0.8978 - val_loss: 0.8340 - val_acc: 0.8366\n",
      "Epoch 186/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6147 - acc: 0.8952 - val_loss: 0.7185 - val_acc: 0.8654\n",
      "Epoch 187/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6096 - acc: 0.8980 - val_loss: 0.7201 - val_acc: 0.8690 0s - loss: 0.6100 - acc: 0\n",
      "Epoch 188/200\n",
      "45000/45000 [==============================] - 22s 485us/step - loss: 0.6052 - acc: 0.8984 - val_loss: 0.7031 - val_acc: 0.8720\n",
      "Epoch 189/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.5970 - acc: 0.9002 - val_loss: 0.6994 - val_acc: 0.8730\n",
      "Epoch 190/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6087 - acc: 0.8966 - val_loss: 0.7309 - val_acc: 0.8672\n",
      "Epoch 191/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.5992 - acc: 0.9000 - val_loss: 0.7444 - val_acc: 0.8614\n",
      "Epoch 192/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6076 - acc: 0.8988 - val_loss: 0.7491 - val_acc: 0.8566\n",
      "Epoch 193/200\n",
      "45000/45000 [==============================] - 22s 483us/step - loss: 0.6120 - acc: 0.8961 - val_loss: 0.7685 - val_acc: 0.8538\n",
      "Epoch 194/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6069 - acc: 0.8968 - val_loss: 0.7507 - val_acc: 0.8552\n",
      "Epoch 195/200\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 0.6128 - acc: 0.8949 - val_loss: 0.7486 - val_acc: 0.8630\n",
      "Epoch 196/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6089 - acc: 0.8965 - val_loss: 0.7145 - val_acc: 0.8676\n",
      "Epoch 197/200\n",
      "45000/45000 [==============================] - 22s 480us/step - loss: 0.6076 - acc: 0.8973 - val_loss: 0.6790 - val_acc: 0.8800\n",
      "Epoch 198/200\n",
      "45000/45000 [==============================] - 22s 481us/step - loss: 0.6088 - acc: 0.8989 - val_loss: 0.7395 - val_acc: 0.8648\n",
      "Epoch 199/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6145 - acc: 0.8947 - val_loss: 0.7117 - val_acc: 0.8712\n",
      "Epoch 200/200\n",
      "45000/45000 [==============================] - 22s 482us/step - loss: 0.6099 - acc: 0.8963 - val_loss: 0.7722 - val_acc: 0.8470\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_4.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    " \n",
    "train_history=model_4.fit(x_train,y_train,epochs=200, batch_size=128,\n",
    "             validation_split=0.1, verbose=1)\n",
    "\n",
    "model_4.save('8.6.5ent_b_cifar10vgg16_4_1.h5')\n",
    "model_4.save_weights('8.6.5ent_b_cifar10vgg16_4_1_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7926940968513488, 0.8423, 0.99, 0.8423]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import keras\n",
    "top1_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=1) #top-1精度\n",
    "\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "\n",
    "model_4.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',metrics=['accuracy','top_k_categorical_accuracy',top1_acc])\n",
    "\n",
    "model_4.evaluate(x_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试损失为：0.7927\n",
      "测试准确率为：0.8423\n"
     ]
    }
   ],
   "source": [
    "score=model_4.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"测试损失为：{:.4f}\".format(score[0]))\n",
    "print(\"测试准确率为：{:.4f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [18.991333289886825, 8.577113524457863, 22.13858709870182]\n",
      "1 [5.166963440565113, 19.464758927728685, 3.6774239674999625]\n",
      "2 [5.06771346664038, 11.127846557253116, 13.810227496284973]\n",
      "3 [27.32610786893458, 25.88465084936154, 17.519336086108993]\n",
      "4 [10.090865535001614, 41.40366930710713, 9.85733170654223]\n",
      "5 [28.55270010611272, 11.739058330793592, 23.135302990927965]\n",
      "6 [7.654492951237571, 3.1211834299048804, 17.911182822589286]\n",
      "7 [12.40541721498812, 22.755171900573284, 4.5438278842217015]\n",
      "8 [7.611988519607803, 11.359744835963072, 12.893112737871805]\n",
      "9 [3.784221899966173, 58.85938037925961, 34.47135388386168]\n",
      "10 [28.294144248532948, 15.879104924571989, 22.071218437573666]\n",
      "11 [23.246606394532186, 12.408400726306139, 7.409383569629593]\n",
      "12 [10.419049613502798, 17.696129624655978, 17.70347332253998]\n",
      "13 [5.664887403731101, 13.033574269808438, 10.904106158967013]\n",
      "14 [15.171697676434626, 16.896566695710703, 11.296192027191532]\n",
      "15 [24.223377519095415, 143.74741718128038, 55.057022636328135]\n",
      "16 [9.36248212514681, 18.708211998946073, 16.01563946979148]\n",
      "17 [48.65618351593081, 45.17532239417854, 92.80319736497287]\n",
      "18 [7.856740248260967, 10.377833726259254, 16.63488821083255]\n",
      "19 [3.2937571168999993, 24.223145736967744, 81.35367756076631]\n",
      "20 [9.82546303606799, 15.126521798492735, 1.0915464820418257]\n",
      "21 [9.05587036727148, 27.76703306630321, 16.850208693210917]\n",
      "22 [14.007199594734033, 14.031720873571713, 11.831813191885217]\n",
      "23 [12.124121368664088, 12.002993667229664, 15.220224219010278]\n",
      "24 [13.047967227742609, 18.107955495184324, 14.71066424925873]\n",
      "25 [7.41628395300403, 7.031245049368917, 11.903449943275394]\n",
      "26 [34.37825546108126, 46.31020132272709, 32.80731915944501]\n",
      "27 [37.80518556579325, 11.28297174476332, 5.658942493276629]\n",
      "28 [15.054385456881825, 20.52816800423524, 15.179667835546804]\n",
      "29 [26.549239674688938, 12.98697494018233, 6.51802352741428]\n",
      "30 [7.039301623687052, 15.526770438880336, 10.50825591445788]\n",
      "31 [8.229962782839385, 9.488128246268873, 8.223235933072091]\n",
      "32 [4.573733205459507, 19.819658496428215, 12.059714593625834]\n",
      "33 [103971552.44555695, 264134356.3870149, 275677797.3282135]\n",
      "34 [10.750743219277712, 16.869274054517778, 27.762408961696863]\n",
      "35 [10.93536816120685, 4.5792767895319315, 10.298124963957513]\n",
      "36 [7.137845690050375, 33.58832089578377, 38.861544031242346]\n",
      "37 [7.936509348407498, 10.279876825989971, 11.029713812787355]\n",
      "38 [14.76892472792175, 31.233631531653348, 3.753892848916799]\n",
      "39 [6.303435965996402, 20.566166211762496, 18.20165543415827]\n",
      "40 [14.476819286755532, 9.879894926616696, 13.732979279339927]\n",
      "41 [38.73420951041585, 102.37064979262779, 15.370250557598895]\n",
      "42 [8.661722119952524, 15.81381838281911, 5.585801129577188]\n",
      "43 [11.50115589884034, 16.249233990734595, 24.75807158508232]\n",
      "44 [12.261667106254736, 23.67084441003158, 42.31259033554103]\n",
      "45 [18.098665969646184, 20.64986214718746, 5.675784673512549]\n",
      "46 [17.74903848219789, 23.03782189444959, 28.277979118922765]\n",
      "47 [11.678071416089338, 7.926620025395948, 9.794474379109738]\n",
      "48 [21.147552866307638, 15.965115904442571, 31.978701656245217]\n",
      "49 [17.926362411245524, 15.210735208021422, 1.9768923759360133]\n",
      "50 [18.58080711927594, 16.79706559893678, 22.004474374543094]\n",
      "51 [7.981059836270752, 19.440799334991777, 9.548773256871055]\n",
      "52 [15.17839488495809, 12.076123998387578, 21.308282630670124]\n",
      "53 [21.409231760673112, 18.36828874785722, 13.488648905357133]\n",
      "54 [14.440740180448802, 13.495122452272565, 10.265830167116567]\n",
      "55 [24.155634952428525, 35.493018659314046, 53.828910868407235]\n",
      "56 [17.95593056532398, 14.878822668936944, 8.288538881206888]\n",
      "57 [10.553185684350433, 26.406332000063372, 10.32005458743422]\n",
      "58 [23.450094353784934, 21.3395623808789, 39.37268999377857]\n",
      "59 [12.071498350883333, 18.36818825718474, 13.219780054383632]\n",
      "60 [10.88412031762333, 16.942329872361306, 46.67877679550954]\n",
      "61 [12.150699152276996, 12.874185554891705, 9.619393338209559]\n",
      "62 [4.561038684670043, 27.692555243759106, 11.494760325149562]\n",
      "63 [20.4322799409017, 30.413899603269535, 19.06544729492925]\n",
      "64 [43.38965361362602, 27.79270846364964, 89.84062732345035]\n",
      "65 [32.89035673417171, 25.75773274875336, 31.656711842367887]\n",
      "66 [8.526023633250789, 4.446548477805962, 4.90403114246107]\n",
      "67 [17.167862983178182, 15.397800503991398, 27.37396626719798]\n",
      "68 [11.51972116318317, 20.84019815131087, 4.252065829674471]\n",
      "69 [9.331335617459601, 29.242834664434053, 14.047513354290523]\n",
      "70 [11.27328829491445, 8.297754876847492, 5.8492414814143165]\n",
      "71 [24.377858410030296, 46.26313162297487, 23.560444587890892]\n",
      "72 [29.8982892258284, 13.116857128061156, 14.058638513000401]\n",
      "73 [3.5571921107974345, 12.943044699807956, 13.306318128276127]\n",
      "74 [20.823718189297868, 81.9451279686763, 27.856258104582924]\n",
      "75 [20.351733257141127, 11.859347176299295, 18.446739054364247]\n",
      "76 [1.9393154375848913, 20.409120062849528, 15.166682100113503]\n",
      "77 [8.29024138413254, 14.278922594602385, 22.50040677020816]\n",
      "78 [14.261474741026625, 23.24375349138628, 40.22868076362243]\n",
      "79 [9.959892331809138, 14.35150581726609, 15.533847860210852]\n",
      "80 [26.01571307009891, 18.291962443793885, 5.073082588923674]\n",
      "81 [22.815453460972265, 18.73154642547067, 49.989409259679334]\n",
      "82 [6.207107380231482, 6.788726201304579, 5.529565759479068]\n",
      "83 [5.478632558842875, 26.947945430481234, 11.003285112545928]\n",
      "84 [6.630116412376377, 9.585240619327598, 12.140538867070486]\n",
      "85 [3.554758393974543, 19.79871203924856, 29.95786421032261]\n",
      "86 [25.854915434891403, 9.0903121375113, 32.93682435010475]\n",
      "87 [7.148991897847351, 29.62015052731404, 16.104542285771178]\n",
      "88 [12.515664775656598, 12.795951278939585, 10.116334185460337]\n",
      "89 [3.1151189609302357, 3.1209968485011133, 24.94785943127612]\n",
      "90 [20.5253595643603, 20.531095089030433, 20.48287187847482]\n",
      "91 [6.991184746642169, 17.05640137501462, 17.45135831984115]\n",
      "92 [6.108743171508545, 11.342930955979549, 6.527188639963744]\n",
      "93 [27.212829559110904, 14.467193659875756, 31.174198930202188]\n",
      "94 [9.617828471735121, 43.819527012175755, 77.50791388682765]\n",
      "95 [17.425153771027077, 34.874394018452804, 15.834942357807863]\n",
      "96 [9.472597505053026, 12.22329018772383, 13.45961513876337]\n",
      "97 [21.72307803713953, 32.207005851370084, 10.43421959865543]\n",
      "98 [8.867758630853166, 13.453882920451145, 21.51967158940883]\n",
      "99 [7.487968426350515, 18.062873136896723, 14.645362909755885]\n",
      "100 [24.692890978957518, 1.8362055156764945, 12.028850534132536]\n",
      "101 [31.480618402788288, 14.15849978525182, 23.97478424070648]\n",
      "102 [21.580687666967762, 23.016801170860298, 26.69502827507275]\n",
      "103 [8.498296342544664, 26.85505237603358, 8.876661789816723]\n",
      "104 [14.994803482197643, 37.48566213531056, 25.996688338429372]\n",
      "105 [26.26550108672169, 17.384648669699843, 20.49410397944314]\n",
      "106 [24.063929606984036, 37.71622090050252, 42.13352739635474]\n",
      "107 [12.588168135396225, 12.020350842679406, 15.012894031775488]\n",
      "108 [8.720898148255879, 11.659289211835855, 18.67305663771133]\n",
      "109 [11.927275747998737, 7.260173171424008, 44.18412364270649]\n",
      "110 [12.217015521366951, 8.978133619379353, 1.420869576596927]\n",
      "111 [10.920408311413464, 25.636455822255634, 17.408628126648956]\n",
      "112 [10.4956177694777, 10.346108988729291, 16.00481622449195]\n",
      "113 [16.54363007828065, 57.848929545819324, 75.7808006792414]\n",
      "114 [34.707328119333894, 34.663242580577354, 30.849378704139685]\n",
      "115 [87.01690018593371, 36.193878296914725, 27.111988163747085]\n",
      "116 [1.8530249634577927, 13.432947212582949, 16.328896799053307]\n",
      "117 [18.36144558532495, 17.549696684387147, 71.57185551820866]\n",
      "118 [15.464419184457707, 22.721775320498395, 31.008683372872234]\n",
      "119 [22.513820049042845, 40.300659806598304, 40.5528750528101]\n",
      "120 [20.450678534081668, 4.487411906193226, 23.395526318957387]\n",
      "121 [24.214861073974674, 16.611960409915003, 9.220928801053697]\n",
      "122 [1.5422679999926308, 19.56603170087052, 12.762181278624135]\n",
      "123 [8.265281785498793, 21.329989598410744, 34.10557791544206]\n",
      "124 [8.265214617804867, 18.5891917179077, 14.054831316955871]\n",
      "125 [15.270847032583353, 5.901462067468341, 11.717807060452246]\n",
      "126 [5.4541155071302265, 11.94446895557422, 7.46257721622972]\n",
      "127 [22.683073552550265, 31.269281283159227, 5.679301126763058]\n",
      "[[18.991333289886825, 8.577113524457863, 22.13858709870182], [5.166963440565113, 19.464758927728685, 3.6774239674999625], [5.06771346664038, 11.127846557253116, 13.810227496284973], [27.32610786893458, 25.88465084936154, 17.519336086108993], [10.090865535001614, 41.40366930710713, 9.85733170654223], [28.55270010611272, 11.739058330793592, 23.135302990927965], [7.654492951237571, 3.1211834299048804, 17.911182822589286], [12.40541721498812, 22.755171900573284, 4.5438278842217015], [7.611988519607803, 11.359744835963072, 12.893112737871805], [3.784221899966173, 58.85938037925961, 34.47135388386168], [28.294144248532948, 15.879104924571989, 22.071218437573666], [23.246606394532186, 12.408400726306139, 7.409383569629593], [10.419049613502798, 17.696129624655978, 17.70347332253998], [5.664887403731101, 13.033574269808438, 10.904106158967013], [15.171697676434626, 16.896566695710703, 11.296192027191532], [24.223377519095415, 143.74741718128038, 55.057022636328135], [9.36248212514681, 18.708211998946073, 16.01563946979148], [48.65618351593081, 45.17532239417854, 92.80319736497287], [7.856740248260967, 10.377833726259254, 16.63488821083255], [3.2937571168999993, 24.223145736967744, 81.35367756076631], [9.82546303606799, 15.126521798492735, 1.0915464820418257], [9.05587036727148, 27.76703306630321, 16.850208693210917], [14.007199594734033, 14.031720873571713, 11.831813191885217], [12.124121368664088, 12.002993667229664, 15.220224219010278], [13.047967227742609, 18.107955495184324, 14.71066424925873], [7.41628395300403, 7.031245049368917, 11.903449943275394], [34.37825546108126, 46.31020132272709, 32.80731915944501], [37.80518556579325, 11.28297174476332, 5.658942493276629], [15.054385456881825, 20.52816800423524, 15.179667835546804], [26.549239674688938, 12.98697494018233, 6.51802352741428], [7.039301623687052, 15.526770438880336, 10.50825591445788], [8.229962782839385, 9.488128246268873, 8.223235933072091], [4.573733205459507, 19.819658496428215, 12.059714593625834], [103971552.44555695, 264134356.3870149, 275677797.3282135], [10.750743219277712, 16.869274054517778, 27.762408961696863], [10.93536816120685, 4.5792767895319315, 10.298124963957513], [7.137845690050375, 33.58832089578377, 38.861544031242346], [7.936509348407498, 10.279876825989971, 11.029713812787355], [14.76892472792175, 31.233631531653348, 3.753892848916799], [6.303435965996402, 20.566166211762496, 18.20165543415827], [14.476819286755532, 9.879894926616696, 13.732979279339927], [38.73420951041585, 102.37064979262779, 15.370250557598895], [8.661722119952524, 15.81381838281911, 5.585801129577188], [11.50115589884034, 16.249233990734595, 24.75807158508232], [12.261667106254736, 23.67084441003158, 42.31259033554103], [18.098665969646184, 20.64986214718746, 5.675784673512549], [17.74903848219789, 23.03782189444959, 28.277979118922765], [11.678071416089338, 7.926620025395948, 9.794474379109738], [21.147552866307638, 15.965115904442571, 31.978701656245217], [17.926362411245524, 15.210735208021422, 1.9768923759360133], [18.58080711927594, 16.79706559893678, 22.004474374543094], [7.981059836270752, 19.440799334991777, 9.548773256871055], [15.17839488495809, 12.076123998387578, 21.308282630670124], [21.409231760673112, 18.36828874785722, 13.488648905357133], [14.440740180448802, 13.495122452272565, 10.265830167116567], [24.155634952428525, 35.493018659314046, 53.828910868407235], [17.95593056532398, 14.878822668936944, 8.288538881206888], [10.553185684350433, 26.406332000063372, 10.32005458743422], [23.450094353784934, 21.3395623808789, 39.37268999377857], [12.071498350883333, 18.36818825718474, 13.219780054383632], [10.88412031762333, 16.942329872361306, 46.67877679550954], [12.150699152276996, 12.874185554891705, 9.619393338209559], [4.561038684670043, 27.692555243759106, 11.494760325149562], [20.4322799409017, 30.413899603269535, 19.06544729492925], [43.38965361362602, 27.79270846364964, 89.84062732345035], [32.89035673417171, 25.75773274875336, 31.656711842367887], [8.526023633250789, 4.446548477805962, 4.90403114246107], [17.167862983178182, 15.397800503991398, 27.37396626719798], [11.51972116318317, 20.84019815131087, 4.252065829674471], [9.331335617459601, 29.242834664434053, 14.047513354290523], [11.27328829491445, 8.297754876847492, 5.8492414814143165], [24.377858410030296, 46.26313162297487, 23.560444587890892], [29.8982892258284, 13.116857128061156, 14.058638513000401], [3.5571921107974345, 12.943044699807956, 13.306318128276127], [20.823718189297868, 81.9451279686763, 27.856258104582924], [20.351733257141127, 11.859347176299295, 18.446739054364247], [1.9393154375848913, 20.409120062849528, 15.166682100113503], [8.29024138413254, 14.278922594602385, 22.50040677020816], [14.261474741026625, 23.24375349138628, 40.22868076362243], [9.959892331809138, 14.35150581726609, 15.533847860210852], [26.01571307009891, 18.291962443793885, 5.073082588923674], [22.815453460972265, 18.73154642547067, 49.989409259679334], [6.207107380231482, 6.788726201304579, 5.529565759479068], [5.478632558842875, 26.947945430481234, 11.003285112545928], [6.630116412376377, 9.585240619327598, 12.140538867070486], [3.554758393974543, 19.79871203924856, 29.95786421032261], [25.854915434891403, 9.0903121375113, 32.93682435010475], [7.148991897847351, 29.62015052731404, 16.104542285771178], [12.515664775656598, 12.795951278939585, 10.116334185460337], [3.1151189609302357, 3.1209968485011133, 24.94785943127612], [20.5253595643603, 20.531095089030433, 20.48287187847482], [6.991184746642169, 17.05640137501462, 17.45135831984115], [6.108743171508545, 11.342930955979549, 6.527188639963744], [27.212829559110904, 14.467193659875756, 31.174198930202188], [9.617828471735121, 43.819527012175755, 77.50791388682765], [17.425153771027077, 34.874394018452804, 15.834942357807863], [9.472597505053026, 12.22329018772383, 13.45961513876337], [21.72307803713953, 32.207005851370084, 10.43421959865543], [8.867758630853166, 13.453882920451145, 21.51967158940883], [7.487968426350515, 18.062873136896723, 14.645362909755885], [24.692890978957518, 1.8362055156764945, 12.028850534132536], [31.480618402788288, 14.15849978525182, 23.97478424070648], [21.580687666967762, 23.016801170860298, 26.69502827507275], [8.498296342544664, 26.85505237603358, 8.876661789816723], [14.994803482197643, 37.48566213531056, 25.996688338429372], [26.26550108672169, 17.384648669699843, 20.49410397944314], [24.063929606984036, 37.71622090050252, 42.13352739635474], [12.588168135396225, 12.020350842679406, 15.012894031775488], [8.720898148255879, 11.659289211835855, 18.67305663771133], [11.927275747998737, 7.260173171424008, 44.18412364270649], [12.217015521366951, 8.978133619379353, 1.420869576596927], [10.920408311413464, 25.636455822255634, 17.408628126648956], [10.4956177694777, 10.346108988729291, 16.00481622449195], [16.54363007828065, 57.848929545819324, 75.7808006792414], [34.707328119333894, 34.663242580577354, 30.849378704139685], [87.01690018593371, 36.193878296914725, 27.111988163747085], [1.8530249634577927, 13.432947212582949, 16.328896799053307], [18.36144558532495, 17.549696684387147, 71.57185551820866], [15.464419184457707, 22.721775320498395, 31.008683372872234], [22.513820049042845, 40.300659806598304, 40.5528750528101], [20.450678534081668, 4.487411906193226, 23.395526318957387], [24.214861073974674, 16.611960409915003, 9.220928801053697], [1.5422679999926308, 19.56603170087052, 12.762181278624135], [8.265281785498793, 21.329989598410744, 34.10557791544206], [8.265214617804867, 18.5891917179077, 14.054831316955871], [15.270847032583353, 5.901462067468341, 11.717807060452246], [5.4541155071302265, 11.94446895557422, 7.46257721622972], [22.683073552550265, 31.269281283159227, 5.679301126763058]]\n"
     ]
    }
   ],
   "source": [
    "tensor =[]\n",
    "for i in range(0,128):\n",
    "   \n",
    "    u=model_4.get_layer('conv2d_3').get_weights()[0][:,:,:,i].squeeze()\n",
    "    v=np.mean(u, axis=1)\n",
    "    #print(v)\n",
    "    vT=v.T\n",
    "    D=np.cov(vT)\n",
    "    try:                 \n",
    "        invD=np.linalg.inv(D)\n",
    "        #t=np.mean(v,axis=1)\n",
    "        a=[]\n",
    "\n",
    "        for j in range(0,2): #表示filter的大小\n",
    "            for k in range(j+1,3):\n",
    "               #if (j!=k) and (j<k):\n",
    "                tp=v[j]-v[k]\n",
    "                d=np.sqrt(abs(np.dot(np.dot(tp,invD),tp.T)))\n",
    "                a.append(d)\n",
    "            \n",
    "        print(i,a)\n",
    "        tensor.append(a)\n",
    "    except:\n",
    "        print(\"不可逆\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{0.0, 60.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0. 100. 100.]\n",
      "{140.0, 20.0, 60.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{50.0, 90.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{0.0, 80.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{50.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [1.039716e+08 2.641344e+08 2.756778e+08]\n",
      "{264134360.0, 275677800.0, 103971550.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{40.0, 100.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{40.0, 50.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 50.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{40.0, 90.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{30.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{50.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{80.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{50.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{40.0, 10.0, 80.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0. 100. 100.]\n",
      "{80.0, 20.0, 60.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{30.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [100.   0.   0.]\n",
      "{40.0, 90.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{20.0, 70.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "[0.9182958340544896, 1.584962500721156, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.0, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.0, 1.584962500721156, 0.9182958340544896, 0.0, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.0, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.0, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.0, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156]\n"
     ]
    }
   ],
   "source": [
    "entt=[]\n",
    "for i in range (0,128):\n",
    "    \n",
    "    data=tensor[i]\n",
    "    data0=np.array(data)\n",
    "    print('四舍五入，精确到个位\\n',np.round(data0,decimals=-2))\n",
    "    data1=np.round(data0,decimals=-1)\n",
    "\n",
    "    data1_value_list=set([data1[i] for i in range (data1.shape[0])])\n",
    "    print(data1_value_list)\n",
    "    ent=0.0\n",
    "    for data1_value in data1_value_list:\n",
    "        p=float(data1[data1==data1_value].shape[0])/data1.shape[0]\n",
    "        print(p)\n",
    "        logp=np.log2(p)\n",
    "        ent-=p*logp\n",
    "        print(ent)   \n",
    "    entt.append(ent)\n",
    "print(entt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896]\n",
      "[2, 8, 13, 22, 25, 28, 31, 37, 40, 47, 50, 54, 61, 65, 70, 82, 84, 88, 90, 92, 96, 114, 126, 0, 3, 4, 10, 11, 12, 14, 16, 17, 18, 23, 24, 26, 27, 29, 30, 35, 39, 42, 43, 45, 46, 48, 49, 51, 52, 53, 56, 57, 58, 59, 63, 66, 67, 69, 71, 72, 73, 75, 76, 77, 79, 81, 83, 86, 89, 91, 93, 95, 98, 99, 102, 103]\n"
     ]
    }
   ],
   "source": [
    "import heapq #获取list中最小的\n",
    "\n",
    "f=int(len(entt)*0.6) #计算滤波器熵个数的80%\n",
    "m=entt\n",
    "max_number=heapq.nsmallest(f,m) #从m中找出最小的f个数，最大用nlargest\n",
    "max_index=[]\n",
    "for t in max_number:\n",
    "    index=m.index(t)\n",
    "    max_index.append(index)\n",
    "    m[index]=float('-inf')\n",
    "print(max_number)#输出最小的f个数\n",
    "print(max_index)#输出对应索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 76/128 channels from layer: conv2d_3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 52)        30004     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 52)        208       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 52)        24388     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 52)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 103)         48307     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               53248     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,021,657\n",
      "Trainable params: 1,017,597\n",
      "Non-trainable params: 4,060\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerassurgeon.operations import delete_layer,insert_layer,delete_channels\n",
    "model_3 = delete_channels(model_4, model_4.layers[9],max_index)\n",
    "model_3.summary()\n",
    "model_3.save('8.6.5ent_b_cifar10vgg16_3.h5')\n",
    "model_3.save_weights('8.6.5ent_b_cifar10vgg16_3_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "45000/45000 [==============================] - 27s 595us/step - loss: 0.7602 - acc: 0.8333 - val_loss: 0.7117 - val_acc: 0.8520\n",
      "Epoch 2/200\n",
      "45000/45000 [==============================] - 21s 463us/step - loss: 0.7089 - acc: 0.8493 - val_loss: 0.6782 - val_acc: 0.8648\n",
      "Epoch 3/200\n",
      "45000/45000 [==============================] - 21s 464us/step - loss: 0.6970 - acc: 0.8552 - val_loss: 0.7281 - val_acc: 0.8454\n",
      "Epoch 4/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6922 - acc: 0.8576 - val_loss: 0.7328 - val_acc: 0.8524\n",
      "Epoch 5/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6953 - acc: 0.8590 - val_loss: 0.7196 - val_acc: 0.8546\n",
      "Epoch 6/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6938 - acc: 0.8603 - val_loss: 0.7257 - val_acc: 0.8500\n",
      "Epoch 7/200\n",
      "45000/45000 [==============================] - 21s 463us/step - loss: 0.6940 - acc: 0.8609 - val_loss: 0.7687 - val_acc: 0.8418\n",
      "Epoch 8/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6815 - acc: 0.8657 - val_loss: 0.7545 - val_acc: 0.8456\n",
      "Epoch 9/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6766 - acc: 0.8671 - val_loss: 0.7432 - val_acc: 0.8536\n",
      "Epoch 10/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6784 - acc: 0.8659 - val_loss: 0.7912 - val_acc: 0.8406\n",
      "Epoch 11/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6756 - acc: 0.8667 - val_loss: 0.7361 - val_acc: 0.8546\n",
      "Epoch 12/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6775 - acc: 0.8661 - val_loss: 0.6945 - val_acc: 0.8654\n",
      "Epoch 13/200\n",
      "45000/45000 [==============================] - 21s 464us/step - loss: 0.6849 - acc: 0.8654 - val_loss: 0.7889 - val_acc: 0.8374\n",
      "Epoch 14/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6890 - acc: 0.8658 - val_loss: 0.6982 - val_acc: 0.8592\n",
      "Epoch 15/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6833 - acc: 0.8677 - val_loss: 0.7516 - val_acc: 0.8494\n",
      "Epoch 16/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6758 - acc: 0.8695 - val_loss: 0.7290 - val_acc: 0.8522\n",
      "Epoch 17/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6650 - acc: 0.8710 - val_loss: 0.6686 - val_acc: 0.8744\n",
      "Epoch 18/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6792 - acc: 0.8673 - val_loss: 0.7061 - val_acc: 0.8614\n",
      "Epoch 19/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6737 - acc: 0.8712 - val_loss: 0.7336 - val_acc: 0.8492\n",
      "Epoch 20/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6710 - acc: 0.8706 - val_loss: 0.7174 - val_acc: 0.8566\n",
      "Epoch 21/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6734 - acc: 0.8695 - val_loss: 0.7275 - val_acc: 0.8558\n",
      "Epoch 22/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6787 - acc: 0.8685 - val_loss: 0.7614 - val_acc: 0.8466\n",
      "Epoch 23/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6705 - acc: 0.8716 - val_loss: 0.7561 - val_acc: 0.8476\n",
      "Epoch 24/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6731 - acc: 0.8697 - val_loss: 0.7705 - val_acc: 0.8474\n",
      "Epoch 25/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6624 - acc: 0.8736 - val_loss: 0.6858 - val_acc: 0.8662\n",
      "Epoch 26/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6629 - acc: 0.8745 - val_loss: 0.8063 - val_acc: 0.8340\n",
      "Epoch 27/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6778 - acc: 0.8690 - val_loss: 0.7001 - val_acc: 0.8596\n",
      "Epoch 28/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6689 - acc: 0.8712 - val_loss: 0.7082 - val_acc: 0.8588\n",
      "Epoch 29/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6706 - acc: 0.8702 - val_loss: 0.7127 - val_acc: 0.8634\n",
      "Epoch 30/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6764 - acc: 0.8683 - val_loss: 0.7119 - val_acc: 0.8602\n",
      "Epoch 31/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6671 - acc: 0.8740 - val_loss: 0.7588 - val_acc: 0.8502\n",
      "Epoch 32/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6675 - acc: 0.8721 - val_loss: 0.7828 - val_acc: 0.8404\n",
      "Epoch 33/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6667 - acc: 0.8730 - val_loss: 0.8533 - val_acc: 0.8134\n",
      "Epoch 34/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6753 - acc: 0.8679 - val_loss: 0.6858 - val_acc: 0.8650\n",
      "Epoch 35/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6625 - acc: 0.8720 - val_loss: 0.8720 - val_acc: 0.8068\n",
      "Epoch 36/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6655 - acc: 0.8735 - val_loss: 0.6853 - val_acc: 0.8686\n",
      "Epoch 37/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6697 - acc: 0.8729 - val_loss: 0.7134 - val_acc: 0.8646\n",
      "Epoch 38/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6691 - acc: 0.8718 - val_loss: 0.7249 - val_acc: 0.8552\n",
      "Epoch 39/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6584 - acc: 0.8747 - val_loss: 0.7227 - val_acc: 0.8604\n",
      "Epoch 40/200\n",
      "45000/45000 [==============================] - 20s 451us/step - loss: 0.6644 - acc: 0.8734 - val_loss: 0.6915 - val_acc: 0.8688\n",
      "Epoch 41/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6634 - acc: 0.8736 - val_loss: 0.7585 - val_acc: 0.8478\n",
      "Epoch 42/200\n",
      "45000/45000 [==============================] - 20s 454us/step - loss: 0.6603 - acc: 0.8724 - val_loss: 0.7421 - val_acc: 0.8524\n",
      "Epoch 43/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6609 - acc: 0.8746 - val_loss: 0.6759 - val_acc: 0.8644\n",
      "Epoch 44/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6509 - acc: 0.8769 - val_loss: 0.7174 - val_acc: 0.8542\n",
      "Epoch 45/200\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 0.6621 - acc: 0.8735 - val_loss: 0.7836 - val_acc: 0.8338\n",
      "Epoch 46/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6645 - acc: 0.8740 - val_loss: 0.7346 - val_acc: 0.8572\n",
      "Epoch 47/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6634 - acc: 0.8733 - val_loss: 0.7599 - val_acc: 0.8408\n",
      "Epoch 48/200\n",
      "45000/45000 [==============================] - 20s 453us/step - loss: 0.6565 - acc: 0.8760 - val_loss: 0.6851 - val_acc: 0.8710\n",
      "Epoch 49/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6617 - acc: 0.8736 - val_loss: 0.7268 - val_acc: 0.8578\n",
      "Epoch 50/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6580 - acc: 0.8770 - val_loss: 0.7424 - val_acc: 0.8562\n",
      "Epoch 51/200\n",
      "45000/45000 [==============================] - 20s 454us/step - loss: 0.6599 - acc: 0.8750 - val_loss: 0.7582 - val_acc: 0.8436\n",
      "Epoch 52/200\n",
      "45000/45000 [==============================] - 20s 455us/step - loss: 0.6533 - acc: 0.8765 - val_loss: 0.7756 - val_acc: 0.8424\n",
      "Epoch 53/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6637 - acc: 0.8750 - val_loss: 0.7043 - val_acc: 0.8568\n",
      "Epoch 54/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6560 - acc: 0.8756 - val_loss: 0.7969 - val_acc: 0.8362\n",
      "Epoch 55/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6580 - acc: 0.8762 - val_loss: 0.8245 - val_acc: 0.8334\n",
      "Epoch 56/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6662 - acc: 0.8733 - val_loss: 0.7610 - val_acc: 0.8428\n",
      "Epoch 57/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6577 - acc: 0.8763 - val_loss: 0.7620 - val_acc: 0.8444\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6538 - acc: 0.8756 - val_loss: 0.7287 - val_acc: 0.8528\n",
      "Epoch 59/200\n",
      "45000/45000 [==============================] - 21s 464us/step - loss: 0.6550 - acc: 0.8761 - val_loss: 0.7170 - val_acc: 0.8620\n",
      "Epoch 60/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6572 - acc: 0.8750 - val_loss: 0.7009 - val_acc: 0.8672\n",
      "Epoch 61/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6544 - acc: 0.8768 - val_loss: 0.8994 - val_acc: 0.7994\n",
      "Epoch 62/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6539 - acc: 0.8773 - val_loss: 0.8059 - val_acc: 0.8314\n",
      "Epoch 63/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6580 - acc: 0.8738 - val_loss: 0.7934 - val_acc: 0.8350\n",
      "Epoch 64/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6563 - acc: 0.8749 - val_loss: 0.6851 - val_acc: 0.8740\n",
      "Epoch 65/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6549 - acc: 0.8759 - val_loss: 0.7562 - val_acc: 0.8456\n",
      "Epoch 66/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6592 - acc: 0.8749 - val_loss: 0.7582 - val_acc: 0.8494\n",
      "Epoch 67/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6670 - acc: 0.8722 - val_loss: 0.7177 - val_acc: 0.8640\n",
      "Epoch 68/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6569 - acc: 0.8762 - val_loss: 0.8639 - val_acc: 0.8276\n",
      "Epoch 69/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6551 - acc: 0.8762 - val_loss: 0.7715 - val_acc: 0.8416\n",
      "Epoch 70/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6502 - acc: 0.8773 - val_loss: 0.7271 - val_acc: 0.8598\n",
      "Epoch 71/200\n",
      "45000/45000 [==============================] - 21s 464us/step - loss: 0.6541 - acc: 0.8753 - val_loss: 0.7202 - val_acc: 0.8624\n",
      "Epoch 72/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6626 - acc: 0.8726 - val_loss: 0.7848 - val_acc: 0.8404\n",
      "Epoch 73/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6608 - acc: 0.8735 - val_loss: 0.8425 - val_acc: 0.8356\n",
      "Epoch 74/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6562 - acc: 0.8772 - val_loss: 0.7216 - val_acc: 0.8568\n",
      "Epoch 75/200\n",
      "45000/45000 [==============================] - 21s 463us/step - loss: 0.6560 - acc: 0.8741 - val_loss: 0.7296 - val_acc: 0.8572\n",
      "Epoch 76/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6496 - acc: 0.8771 - val_loss: 0.7033 - val_acc: 0.8668\n",
      "Epoch 77/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6540 - acc: 0.8775 - val_loss: 0.7329 - val_acc: 0.8554\n",
      "Epoch 78/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6475 - acc: 0.8775 - val_loss: 0.7276 - val_acc: 0.8602\n",
      "Epoch 79/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6459 - acc: 0.8797 - val_loss: 0.6971 - val_acc: 0.8706\n",
      "Epoch 80/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6501 - acc: 0.8786 - val_loss: 0.6922 - val_acc: 0.8676\n",
      "Epoch 81/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6466 - acc: 0.8783 - val_loss: 0.6868 - val_acc: 0.8644\n",
      "Epoch 82/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6617 - acc: 0.8749 - val_loss: 0.7370 - val_acc: 0.8476\n",
      "Epoch 83/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6520 - acc: 0.8759 - val_loss: 0.7406 - val_acc: 0.8508\n",
      "Epoch 84/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6548 - acc: 0.8759 - val_loss: 0.7227 - val_acc: 0.8600\n",
      "Epoch 85/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6561 - acc: 0.8768 - val_loss: 0.6913 - val_acc: 0.8656\n",
      "Epoch 86/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6477 - acc: 0.8772 - val_loss: 0.7099 - val_acc: 0.8606\n",
      "Epoch 87/200\n",
      "45000/45000 [==============================] - 20s 455us/step - loss: 0.6552 - acc: 0.8742 - val_loss: 0.6850 - val_acc: 0.8708\n",
      "Epoch 88/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6440 - acc: 0.8772 - val_loss: 0.7379 - val_acc: 0.8512\n",
      "Epoch 89/200\n",
      "45000/45000 [==============================] - 21s 463us/step - loss: 0.6533 - acc: 0.8780 - val_loss: 0.7077 - val_acc: 0.8636\n",
      "Epoch 90/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6544 - acc: 0.8758 - val_loss: 0.7303 - val_acc: 0.8520\n",
      "Epoch 91/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6531 - acc: 0.8770 - val_loss: 0.7582 - val_acc: 0.8472\n",
      "Epoch 92/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6632 - acc: 0.8727 - val_loss: 0.6802 - val_acc: 0.8694\n",
      "Epoch 93/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6574 - acc: 0.8752 - val_loss: 0.8035 - val_acc: 0.8326\n",
      "Epoch 94/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6575 - acc: 0.8755 - val_loss: 0.7240 - val_acc: 0.8564\n",
      "Epoch 95/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6476 - acc: 0.8796 - val_loss: 0.6717 - val_acc: 0.8768\n",
      "Epoch 96/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6529 - acc: 0.8778 - val_loss: 0.6957 - val_acc: 0.8644\n",
      "Epoch 97/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6528 - acc: 0.8774 - val_loss: 0.7863 - val_acc: 0.8380\n",
      "Epoch 98/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6663 - acc: 0.8726 - val_loss: 0.6783 - val_acc: 0.8700\n",
      "Epoch 99/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6456 - acc: 0.8786 - val_loss: 0.7201 - val_acc: 0.8622\n",
      "Epoch 100/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6483 - acc: 0.8780 - val_loss: 0.6647 - val_acc: 0.8726\n",
      "Epoch 101/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6523 - acc: 0.8773 - val_loss: 0.6824 - val_acc: 0.8660\n",
      "Epoch 102/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6612 - acc: 0.8753 - val_loss: 0.7195 - val_acc: 0.8530\n",
      "Epoch 103/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6627 - acc: 0.8740 - val_loss: 0.7428 - val_acc: 0.8526\n",
      "Epoch 104/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6600 - acc: 0.8757 - val_loss: 0.7571 - val_acc: 0.8488\n",
      "Epoch 105/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6631 - acc: 0.8734 - val_loss: 0.8663 - val_acc: 0.8220\n",
      "Epoch 106/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6618 - acc: 0.8749 - val_loss: 0.8062 - val_acc: 0.8366\n",
      "Epoch 107/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6567 - acc: 0.8750 - val_loss: 0.7479 - val_acc: 0.8548\n",
      "Epoch 108/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6570 - acc: 0.8744 - val_loss: 0.7730 - val_acc: 0.8352\n",
      "Epoch 109/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6669 - acc: 0.8723 - val_loss: 0.8129 - val_acc: 0.8252\n",
      "Epoch 110/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6612 - acc: 0.8737 - val_loss: 0.7445 - val_acc: 0.8544\n",
      "Epoch 111/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6552 - acc: 0.8756 - val_loss: 0.7992 - val_acc: 0.8386\n",
      "Epoch 112/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6522 - acc: 0.8776 - val_loss: 0.7138 - val_acc: 0.8592\n",
      "Epoch 113/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6406 - acc: 0.8795 - val_loss: 0.7198 - val_acc: 0.8614\n",
      "Epoch 114/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6587 - acc: 0.8748 - val_loss: 0.6842 - val_acc: 0.8688\n",
      "Epoch 115/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6547 - acc: 0.8762 - val_loss: 0.9365 - val_acc: 0.7978\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6526 - acc: 0.8786 - val_loss: 0.7601 - val_acc: 0.8476\n",
      "Epoch 117/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6523 - acc: 0.8766 - val_loss: 0.7454 - val_acc: 0.8536\n",
      "Epoch 118/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6633 - acc: 0.8731 - val_loss: 0.6961 - val_acc: 0.8650\n",
      "Epoch 119/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6493 - acc: 0.8772 - val_loss: 0.7534 - val_acc: 0.8480\n",
      "Epoch 120/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6472 - acc: 0.8774 - val_loss: 0.7344 - val_acc: 0.8532\n",
      "Epoch 121/200\n",
      "45000/45000 [==============================] - 21s 463us/step - loss: 0.6484 - acc: 0.8801 - val_loss: 0.7971 - val_acc: 0.8366\n",
      "Epoch 122/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6487 - acc: 0.8785 - val_loss: 0.7387 - val_acc: 0.8524\n",
      "Epoch 123/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6556 - acc: 0.8758 - val_loss: 0.7493 - val_acc: 0.8498\n",
      "Epoch 124/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6436 - acc: 0.8791 - val_loss: 0.6926 - val_acc: 0.8714\n",
      "Epoch 125/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6500 - acc: 0.8773 - val_loss: 0.6783 - val_acc: 0.8704\n",
      "Epoch 126/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6502 - acc: 0.8763 - val_loss: 0.6960 - val_acc: 0.8626\n",
      "Epoch 127/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6440 - acc: 0.8810 - val_loss: 0.7030 - val_acc: 0.8718\n",
      "Epoch 128/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6460 - acc: 0.8790 - val_loss: 0.7161 - val_acc: 0.8572\n",
      "Epoch 129/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6477 - acc: 0.8775 - val_loss: 0.7304 - val_acc: 0.8542\n",
      "Epoch 130/200\n",
      "45000/45000 [==============================] - 21s 464us/step - loss: 0.6418 - acc: 0.8786 - val_loss: 0.7468 - val_acc: 0.8532\n",
      "Epoch 131/200\n",
      "45000/45000 [==============================] - 21s 464us/step - loss: 0.6527 - acc: 0.8753 - val_loss: 0.8191 - val_acc: 0.8246\n",
      "Epoch 132/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6369 - acc: 0.8824 - val_loss: 0.7610 - val_acc: 0.8500\n",
      "Epoch 133/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6439 - acc: 0.8811 - val_loss: 0.8011 - val_acc: 0.8378\n",
      "Epoch 134/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6358 - acc: 0.8820 - val_loss: 0.7036 - val_acc: 0.8636\n",
      "Epoch 135/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6407 - acc: 0.8811 - val_loss: 0.7223 - val_acc: 0.8604\n",
      "Epoch 136/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6353 - acc: 0.8814 - val_loss: 0.7455 - val_acc: 0.8504\n",
      "Epoch 137/200\n",
      "45000/45000 [==============================] - 21s 463us/step - loss: 0.6377 - acc: 0.8797 - val_loss: 0.7369 - val_acc: 0.8544\n",
      "Epoch 138/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6456 - acc: 0.8766 - val_loss: 0.7205 - val_acc: 0.8602\n",
      "Epoch 139/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6447 - acc: 0.8794 - val_loss: 0.6881 - val_acc: 0.8638\n",
      "Epoch 140/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6439 - acc: 0.8787 - val_loss: 0.7036 - val_acc: 0.8612\n",
      "Epoch 141/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6445 - acc: 0.8791 - val_loss: 0.7489 - val_acc: 0.8468\n",
      "Epoch 142/200\n",
      "45000/45000 [==============================] - 21s 463us/step - loss: 0.6484 - acc: 0.8772 - val_loss: 0.7355 - val_acc: 0.8556\n",
      "Epoch 143/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6466 - acc: 0.8789 - val_loss: 0.6791 - val_acc: 0.8702\n",
      "Epoch 144/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6448 - acc: 0.8780 - val_loss: 0.8088 - val_acc: 0.8276\n",
      "Epoch 145/200\n",
      "45000/45000 [==============================] - 21s 456us/step - loss: 0.6450 - acc: 0.8792 - val_loss: 0.7449 - val_acc: 0.8514\n",
      "Epoch 146/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6449 - acc: 0.8786 - val_loss: 0.8007 - val_acc: 0.8366\n",
      "Epoch 147/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6461 - acc: 0.8782 - val_loss: 0.7239 - val_acc: 0.8608\n",
      "Epoch 148/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6516 - acc: 0.8774 - val_loss: 0.8181 - val_acc: 0.8352\n",
      "Epoch 149/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6451 - acc: 0.8789 - val_loss: 0.6989 - val_acc: 0.8668\n",
      "Epoch 150/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6398 - acc: 0.8808 - val_loss: 0.7126 - val_acc: 0.8566\n",
      "Epoch 151/200\n",
      "45000/45000 [==============================] - 20s 455us/step - loss: 0.6474 - acc: 0.8770 - val_loss: 0.6833 - val_acc: 0.8682\n",
      "Epoch 152/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6476 - acc: 0.8782 - val_loss: 0.7325 - val_acc: 0.8528\n",
      "Epoch 153/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6483 - acc: 0.8769 - val_loss: 0.7598 - val_acc: 0.8504\n",
      "Epoch 154/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6404 - acc: 0.8807 - val_loss: 0.7993 - val_acc: 0.8348\n",
      "Epoch 155/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6482 - acc: 0.8760 - val_loss: 0.7803 - val_acc: 0.8366\n",
      "Epoch 156/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6374 - acc: 0.8815 - val_loss: 0.7234 - val_acc: 0.8524\n",
      "Epoch 157/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6456 - acc: 0.8794 - val_loss: 0.7227 - val_acc: 0.8568\n",
      "Epoch 158/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6352 - acc: 0.8816 - val_loss: 0.7068 - val_acc: 0.8598\n",
      "Epoch 159/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6473 - acc: 0.8796 - val_loss: 0.6976 - val_acc: 0.8616\n",
      "Epoch 160/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6377 - acc: 0.8800 - val_loss: 0.6732 - val_acc: 0.8704\n",
      "Epoch 161/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6378 - acc: 0.8814 - val_loss: 0.7282 - val_acc: 0.8506\n",
      "Epoch 162/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6380 - acc: 0.8805 - val_loss: 0.8707 - val_acc: 0.8068\n",
      "Epoch 163/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6445 - acc: 0.8784 - val_loss: 0.7647 - val_acc: 0.8436 - loss: 0.641\n",
      "Epoch 164/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6373 - acc: 0.8807 - val_loss: 0.8534 - val_acc: 0.8222\n",
      "Epoch 165/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6435 - acc: 0.8788 - val_loss: 0.7215 - val_acc: 0.8590\n",
      "Epoch 166/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6491 - acc: 0.8768 - val_loss: 0.7512 - val_acc: 0.8504\n",
      "Epoch 167/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6352 - acc: 0.8811 - val_loss: 0.6658 - val_acc: 0.8772\n",
      "Epoch 168/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6376 - acc: 0.8794 - val_loss: 0.7117 - val_acc: 0.8618\n",
      "Epoch 169/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6275 - acc: 0.8840 - val_loss: 0.8073 - val_acc: 0.8370\n",
      "Epoch 170/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6436 - acc: 0.8792 - val_loss: 0.6819 - val_acc: 0.8724\n",
      "Epoch 171/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6531 - acc: 0.8748 - val_loss: 0.7503 - val_acc: 0.8462\n",
      "Epoch 172/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6445 - acc: 0.8796 - val_loss: 0.7589 - val_acc: 0.8486\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6411 - acc: 0.8790 - val_loss: 0.6814 - val_acc: 0.8712\n",
      "Epoch 174/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6476 - acc: 0.8770 - val_loss: 0.7911 - val_acc: 0.8378\n",
      "Epoch 175/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6473 - acc: 0.8782 - val_loss: 0.7455 - val_acc: 0.8532\n",
      "Epoch 176/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6442 - acc: 0.8800 - val_loss: 0.7157 - val_acc: 0.8590\n",
      "Epoch 177/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6640 - acc: 0.8734 - val_loss: 0.7010 - val_acc: 0.8648\n",
      "Epoch 178/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6403 - acc: 0.8800 - val_loss: 0.6808 - val_acc: 0.8698\n",
      "Epoch 179/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6450 - acc: 0.8774 - val_loss: 0.8293 - val_acc: 0.8174\n",
      "Epoch 180/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6513 - acc: 0.8772 - val_loss: 0.7328 - val_acc: 0.8550\n",
      "Epoch 181/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6574 - acc: 0.8754 - val_loss: 0.7192 - val_acc: 0.8572\n",
      "Epoch 182/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6492 - acc: 0.8785 - val_loss: 0.7134 - val_acc: 0.8534\n",
      "Epoch 183/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6440 - acc: 0.8795 - val_loss: 0.7105 - val_acc: 0.8632\n",
      "Epoch 184/200\n",
      "45000/45000 [==============================] - 21s 464us/step - loss: 0.6418 - acc: 0.8800 - val_loss: 0.7132 - val_acc: 0.8588\n",
      "Epoch 185/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6362 - acc: 0.8810 - val_loss: 0.9047 - val_acc: 0.8100\n",
      "Epoch 186/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6459 - acc: 0.8783 - val_loss: 0.7718 - val_acc: 0.8442\n",
      "Epoch 187/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6480 - acc: 0.8762 - val_loss: 0.7656 - val_acc: 0.8464\n",
      "Epoch 188/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6460 - acc: 0.8792 - val_loss: 0.7327 - val_acc: 0.8552\n",
      "Epoch 189/200\n",
      "45000/45000 [==============================] - 21s 463us/step - loss: 0.6511 - acc: 0.8787 - val_loss: 0.7193 - val_acc: 0.8590\n",
      "Epoch 190/200\n",
      "45000/45000 [==============================] - 21s 463us/step - loss: 0.6490 - acc: 0.8790 - val_loss: 0.7100 - val_acc: 0.8654\n",
      "Epoch 191/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6421 - acc: 0.8796 - val_loss: 0.7556 - val_acc: 0.8510\n",
      "Epoch 192/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6325 - acc: 0.8824 - val_loss: 0.7337 - val_acc: 0.8528\n",
      "Epoch 193/200\n",
      "45000/45000 [==============================] - 21s 460us/step - loss: 0.6461 - acc: 0.8774 - val_loss: 0.7003 - val_acc: 0.8610\n",
      "Epoch 194/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6380 - acc: 0.8788 - val_loss: 0.6884 - val_acc: 0.8666\n",
      "Epoch 195/200\n",
      "45000/45000 [==============================] - 21s 463us/step - loss: 0.6318 - acc: 0.8798 - val_loss: 0.7325 - val_acc: 0.8542\n",
      "Epoch 196/200\n",
      "45000/45000 [==============================] - 21s 457us/step - loss: 0.6431 - acc: 0.8786 - val_loss: 0.8286 - val_acc: 0.8236\n",
      "Epoch 197/200\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 0.6352 - acc: 0.8818 - val_loss: 0.7803 - val_acc: 0.8428\n",
      "Epoch 198/200\n",
      "45000/45000 [==============================] - 21s 458us/step - loss: 0.6371 - acc: 0.8805 - val_loss: 0.7390 - val_acc: 0.8562\n",
      "Epoch 199/200\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 0.6401 - acc: 0.8795 - val_loss: 0.6999 - val_acc: 0.8662\n",
      "Epoch 200/200\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 0.6484 - acc: 0.8784 - val_loss: 0.6763 - val_acc: 0.8702\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    " \n",
    "train_history=model_3.fit(x_train,y_train,epochs=200, batch_size=128,\n",
    "             validation_split=0.1, verbose=1)\n",
    "\n",
    "model_3.save('8.6.5ent_b_cifar10vgg16_3_1.h5')\n",
    "model_3.save_weights('8.6.5ent_b_cifar10vgg16_3_1_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7084896286964416, 0.8608, 0.9932, 0.8608]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import keras\n",
    "top1_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=1) #top-1精度\n",
    "\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "\n",
    "model_3.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',metrics=['accuracy','top_k_categorical_accuracy',top1_acc])\n",
    "\n",
    "model_3.evaluate(x_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试损失为：0.7085\n",
      "测试准确率为：0.8608\n"
     ]
    }
   ],
   "source": [
    "score=model_3.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"测试损失为：{:.4f}\".format(score[0]))\n",
    "print(\"测试准确率为：{:.4f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [14.401949706143517, 7.5589079246974125, 11.477813268219702]\n",
      "1 [4.760339469895873, 18.547938508391486, 6.635745277432136]\n",
      "2 [2.378491934039505, 13.95086877230064, 13.300709325116998]\n",
      "3 [11.55166605042967, 18.82176036340624, 10.216877570368691]\n",
      "4 [14.791874629784004, 17.64735468041426, 26.430024431580343]\n",
      "5 [13.16990652342276, 8.465297994849417, 14.28368535552048]\n",
      "6 [10.528161430805387, 14.590282984075031, 24.651073342684516]\n",
      "7 [31.671242721261084, 19.422211868912367, 27.847662996485806]\n",
      "8 [1.6664256477925956, 17.982417261067752, 14.614368193337326]\n",
      "9 [9.887562788463747, 5.535306057622315, 2.709904102726087]\n",
      "10 [18.0144536612761, 5.541357256442709, 1.9509795956192322]\n",
      "11 [17.70383715228286, 18.547048129243414, 28.82134289862996]\n",
      "12 [11.031415589578348, 4.935862185056673, 6.369193780204056]\n",
      "13 [52.43725160305766, 115.64538249401703, 119.37822050374034]\n",
      "14 [11.415896624099076, 5.044313897018146, 15.49179044321458]\n",
      "15 [6.910286789475681, 13.98552589156021, 12.96335289116389]\n",
      "16 [30.673331367480454, 17.12355781663976, 15.956294448258696]\n",
      "17 [20.2045298926499, 44.57889977392568, 44.73663335036948]\n",
      "18 [8.588944954056515, 12.518642142826781, 7.746973923593609]\n",
      "19 [7.77297407906285, 9.460198265635754, 6.939121683989723]\n",
      "20 [16.407768455353985, 12.908111659675567, 20.52451338777777]\n",
      "21 [7.613237626492436, 13.223011529338894, 14.69091555468938]\n",
      "22 [17.071148032683936, 25.733668657532753, 20.764883563465915]\n",
      "23 [6.022548189330664, 11.692795884697393, 13.131715891007026]\n",
      "24 [14.400438704695608, 19.902683219971955, 19.939348308546684]\n",
      "25 [31.11882144623274, 26.198307511211986, 31.777770805654523]\n",
      "26 [8.802327148551857, 19.416725022169622, 14.528553669480997]\n",
      "27 [14.302881665417516, 5.793028673619604, 10.148193051020224]\n",
      "28 [38.89428904566108, 7.518394261392846, 20.93851479021163]\n",
      "29 [9.630175728516635, 7.9047306696777495, 20.52534814021267]\n",
      "30 [12.70816360527054, 9.094861254354676, 10.327154209144368]\n",
      "31 [8.450698391420966, 7.586933985907519, 13.827057119397058]\n",
      "32 [21.910743851041595, 21.986108186964863, 36.53237190188099]\n",
      "33 [6.446162904066302, 8.824050611267824, 3.399927623763817]\n",
      "34 [17.769034371396625, 9.561288357933405, 12.025290462292986]\n",
      "35 [9.763640527127377, 7.935830398741157, 20.410637470029773]\n",
      "36 [9.934134147246713, 16.745875405876244, 10.720387214207912]\n",
      "37 [11.215143755783544, 3.210848878997256, 10.893021044960939]\n",
      "38 [10.524072629344492, 12.42682784085131, 10.937335209286276]\n",
      "39 [10.326564647866544, 12.932833774535787, 2.7650685801287547]\n",
      "40 [48.4404802353078, 40.189965583310055, 55.52808226403893]\n",
      "41 [12.231277482942904, 40.25311039818558, 32.60407422004894]\n",
      "42 [11.26390853588232, 11.968146724785797, 24.632049248676186]\n",
      "43 [12.176165352258767, 8.940780357763058, 21.84068842267494]\n",
      "44 [8.420567136713093, 13.819179421614722, 14.909764009612692]\n",
      "45 [18.88819393799359, 80.74335331917179, 43.984676065989916]\n",
      "46 [6.104637670301395, 8.842329706510254, 9.561936124993617]\n",
      "47 [16.366994866669206, 16.271016329502693, 17.744936953002938]\n",
      "48 [9.640070078504058, 17.905471065815195, 18.428977082625902]\n",
      "49 [10.999558049773828, 5.7780373384583745, 17.76560284320453]\n",
      "50 [24.471714195879585, 33.59122114732263, 20.082620009210107]\n",
      "51 [18.0218775569781, 16.81028710305635, 19.547853099279614]\n",
      "52 [6.821040670668762, 18.815402481899707, 5.1890806984058235]\n",
      "53 [7.2333355809866, 7.77209816901012, 13.667815367564945]\n",
      "54 [11.216170160453046, 22.40481315911164, 16.244592691295317]\n",
      "55 [6.678039847274215, 32.08338656617162, 11.19931778533012]\n",
      "56 [13.238983135690097, 4.170158233018926, 24.237098250617944]\n",
      "57 [251.06670669552645, 86.65379429614129, 1.6981273434608415]\n",
      "58 [21.171282252498685, 18.56309435114458, 23.913188451018424]\n",
      "59 [12.17423387032156, 20.661297064702254, 4.227012455200154]\n",
      "60 [3.115360728604287, 8.502127549432629, 8.361932894375649]\n",
      "61 [16.157649084787785, 20.542578604098523, 18.537917688953947]\n",
      "62 [14.520438226754628, 17.113570527168037, 10.496536648729347]\n",
      "63 [12.565656578557451, 13.122485119707099, 10.970343992913142]\n",
      "[[14.401949706143517, 7.5589079246974125, 11.477813268219702], [4.760339469895873, 18.547938508391486, 6.635745277432136], [2.378491934039505, 13.95086877230064, 13.300709325116998], [11.55166605042967, 18.82176036340624, 10.216877570368691], [14.791874629784004, 17.64735468041426, 26.430024431580343], [13.16990652342276, 8.465297994849417, 14.28368535552048], [10.528161430805387, 14.590282984075031, 24.651073342684516], [31.671242721261084, 19.422211868912367, 27.847662996485806], [1.6664256477925956, 17.982417261067752, 14.614368193337326], [9.887562788463747, 5.535306057622315, 2.709904102726087], [18.0144536612761, 5.541357256442709, 1.9509795956192322], [17.70383715228286, 18.547048129243414, 28.82134289862996], [11.031415589578348, 4.935862185056673, 6.369193780204056], [52.43725160305766, 115.64538249401703, 119.37822050374034], [11.415896624099076, 5.044313897018146, 15.49179044321458], [6.910286789475681, 13.98552589156021, 12.96335289116389], [30.673331367480454, 17.12355781663976, 15.956294448258696], [20.2045298926499, 44.57889977392568, 44.73663335036948], [8.588944954056515, 12.518642142826781, 7.746973923593609], [7.77297407906285, 9.460198265635754, 6.939121683989723], [16.407768455353985, 12.908111659675567, 20.52451338777777], [7.613237626492436, 13.223011529338894, 14.69091555468938], [17.071148032683936, 25.733668657532753, 20.764883563465915], [6.022548189330664, 11.692795884697393, 13.131715891007026], [14.400438704695608, 19.902683219971955, 19.939348308546684], [31.11882144623274, 26.198307511211986, 31.777770805654523], [8.802327148551857, 19.416725022169622, 14.528553669480997], [14.302881665417516, 5.793028673619604, 10.148193051020224], [38.89428904566108, 7.518394261392846, 20.93851479021163], [9.630175728516635, 7.9047306696777495, 20.52534814021267], [12.70816360527054, 9.094861254354676, 10.327154209144368], [8.450698391420966, 7.586933985907519, 13.827057119397058], [21.910743851041595, 21.986108186964863, 36.53237190188099], [6.446162904066302, 8.824050611267824, 3.399927623763817], [17.769034371396625, 9.561288357933405, 12.025290462292986], [9.763640527127377, 7.935830398741157, 20.410637470029773], [9.934134147246713, 16.745875405876244, 10.720387214207912], [11.215143755783544, 3.210848878997256, 10.893021044960939], [10.524072629344492, 12.42682784085131, 10.937335209286276], [10.326564647866544, 12.932833774535787, 2.7650685801287547], [48.4404802353078, 40.189965583310055, 55.52808226403893], [12.231277482942904, 40.25311039818558, 32.60407422004894], [11.26390853588232, 11.968146724785797, 24.632049248676186], [12.176165352258767, 8.940780357763058, 21.84068842267494], [8.420567136713093, 13.819179421614722, 14.909764009612692], [18.88819393799359, 80.74335331917179, 43.984676065989916], [6.104637670301395, 8.842329706510254, 9.561936124993617], [16.366994866669206, 16.271016329502693, 17.744936953002938], [9.640070078504058, 17.905471065815195, 18.428977082625902], [10.999558049773828, 5.7780373384583745, 17.76560284320453], [24.471714195879585, 33.59122114732263, 20.082620009210107], [18.0218775569781, 16.81028710305635, 19.547853099279614], [6.821040670668762, 18.815402481899707, 5.1890806984058235], [7.2333355809866, 7.77209816901012, 13.667815367564945], [11.216170160453046, 22.40481315911164, 16.244592691295317], [6.678039847274215, 32.08338656617162, 11.19931778533012], [13.238983135690097, 4.170158233018926, 24.237098250617944], [251.06670669552645, 86.65379429614129, 1.6981273434608415], [21.171282252498685, 18.56309435114458, 23.913188451018424], [12.17423387032156, 20.661297064702254, 4.227012455200154], [3.115360728604287, 8.502127549432629, 8.361932894375649], [16.157649084787785, 20.542578604098523, 18.537917688953947], [14.520438226754628, 17.113570527168037, 10.496536648729347], [12.565656578557451, 13.122485119707099, 10.970343992913142]]\n"
     ]
    }
   ],
   "source": [
    "tensor =[]\n",
    "for i in range(0,64):\n",
    "   \n",
    "    u=model_3.get_layer('conv2d_2').get_weights()[0][:,:,:,i].squeeze()\n",
    "    v=np.mean(u, axis=1)\n",
    "    #print(v)\n",
    "    vT=v.T\n",
    "    D=np.cov(vT)\n",
    "    try:                 \n",
    "        invD=np.linalg.inv(D)\n",
    "        #t=np.mean(v,axis=1)\n",
    "        a=[]\n",
    "\n",
    "        for j in range(0,2): #表示filter的大小\n",
    "            for k in range(j+1,3):\n",
    "               #if (j!=k) and (j<k):\n",
    "                tp=v[j]-v[k]\n",
    "                d=np.sqrt(abs(np.dot(np.dot(tp,invD),tp.T)))\n",
    "                a.append(d)\n",
    "            \n",
    "        print(i,a)\n",
    "        tensor.append(a)\n",
    "    except:\n",
    "        print(\"不可逆\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [100. 100. 100.]\n",
      "{120.0, 50.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{30.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [  0.   0. 100.]\n",
      "{40.0, 50.0, 60.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{40.0, 10.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [  0. 100.   0.]\n",
      "{80.0, 40.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 30.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [300. 100.   0.]\n",
      "{0.0, 250.0, 90.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{20.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "[0.0, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 1.584962500721156, 0.9182958340544896, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.0, 1.584962500721156, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 1.584962500721156, 1.584962500721156, 0.0, 1.584962500721156, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0]\n"
     ]
    }
   ],
   "source": [
    "entt=[]\n",
    "for i in range (0,64):\n",
    "    \n",
    "    data=tensor[i]\n",
    "    data0=np.array(data)\n",
    "    print('四舍五入，精确到个位\\n',np.round(data0,decimals=-2))\n",
    "    data1=np.round(data0,decimals=-1)\n",
    "\n",
    "    data1_value_list=set([data1[i] for i in range (data1.shape[0])])\n",
    "    print(data1_value_list)\n",
    "    ent=0.0\n",
    "    for data1_value in data1_value_list:\n",
    "        p=float(data1[data1==data1_value].shape[0])/data1.shape[0]\n",
    "        print(p)\n",
    "        logp=np.log2(p)\n",
    "        ent-=p*logp\n",
    "        print(ent)   \n",
    "    entt.append(ent)\n",
    "print(entt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896]\n",
      "[0, 5, 15, 18, 19, 21, 23, 25, 27, 30, 31, 38, 44, 46, 47, 51, 53, 58, 61, 63, 2, 3, 6, 7, 9, 11, 12, 13, 14, 16, 17, 20]\n"
     ]
    }
   ],
   "source": [
    "import heapq #获取list中最小的\n",
    "\n",
    "f=int(len(entt)*0.5) #计算滤波器熵个数的80%\n",
    "m=entt\n",
    "max_number=heapq.nsmallest(f,m) #从m中找出最小的f个数，最大用nlargest\n",
    "max_index=[]\n",
    "for t in max_number:\n",
    "    index=m.index(t)\n",
    "    max_index.append(index)\n",
    "    m[index]=float('-inf')\n",
    "print(max_number)#输出最小的f个数\n",
    "print(max_index)#输出对应索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 32/64 channels from layer: conv2d_2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        18464     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 52)        15028     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 52)        208       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 52)        24388     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 52)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 103)         48307     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               53248     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 988,089\n",
      "Trainable params: 984,093\n",
      "Non-trainable params: 3,996\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerassurgeon.operations import delete_layer,insert_layer,delete_channels\n",
    "model_2 = delete_channels(model_3, model_3.layers[5],max_index)\n",
    "model_2.summary()\n",
    "model_2.save('8.6.5ent_b_cifar10vgg16_2.h5')\n",
    "model_2.save_weights('8.6.5ent_b_cifar10vgg16_2_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "45000/45000 [==============================] - 27s 589us/step - loss: 0.7773 - acc: 0.8274 - val_loss: 0.7075 - val_acc: 0.8522\n",
      "Epoch 2/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.7269 - acc: 0.8441 - val_loss: 0.8982 - val_acc: 0.7984\n",
      "Epoch 3/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.7169 - acc: 0.8495 - val_loss: 0.7336 - val_acc: 0.8422\n",
      "Epoch 4/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.7102 - acc: 0.8514 - val_loss: 0.6971 - val_acc: 0.8576\n",
      "Epoch 5/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.7012 - acc: 0.8560 - val_loss: 0.7771 - val_acc: 0.8352\n",
      "Epoch 6/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.6987 - acc: 0.8571 - val_loss: 0.7880 - val_acc: 0.8342\n",
      "Epoch 7/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.7028 - acc: 0.8569 - val_loss: 0.7214 - val_acc: 0.8474\n",
      "Epoch 8/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6932 - acc: 0.8592 - val_loss: 0.6898 - val_acc: 0.8600\n",
      "Epoch 9/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6882 - acc: 0.8626 - val_loss: 0.9916 - val_acc: 0.7788\n",
      "Epoch 10/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6985 - acc: 0.8612 - val_loss: 0.7820 - val_acc: 0.8338\n",
      "Epoch 11/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6830 - acc: 0.8622 - val_loss: 0.7359 - val_acc: 0.8486\n",
      "Epoch 12/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6944 - acc: 0.8612 - val_loss: 0.7690 - val_acc: 0.8396\n",
      "Epoch 13/200\n",
      "45000/45000 [==============================] - 19s 431us/step - loss: 0.6929 - acc: 0.8616 - val_loss: 0.7392 - val_acc: 0.8454\n",
      "Epoch 14/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6990 - acc: 0.8593 - val_loss: 0.7868 - val_acc: 0.8350\n",
      "Epoch 15/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6971 - acc: 0.8593 - val_loss: 0.8499 - val_acc: 0.8220\n",
      "Epoch 16/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6871 - acc: 0.8632 - val_loss: 0.7220 - val_acc: 0.8566\n",
      "Epoch 17/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6853 - acc: 0.8623 - val_loss: 0.7119 - val_acc: 0.8510\n",
      "Epoch 18/200\n",
      "45000/45000 [==============================] - 20s 433us/step - loss: 0.6918 - acc: 0.8630 - val_loss: 0.7684 - val_acc: 0.8394\n",
      "Epoch 19/200\n",
      "45000/45000 [==============================] - 19s 431us/step - loss: 0.6941 - acc: 0.8602 - val_loss: 0.7467 - val_acc: 0.8466\n",
      "Epoch 20/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6837 - acc: 0.8631 - val_loss: 0.7494 - val_acc: 0.8438\n",
      "Epoch 21/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6881 - acc: 0.8627 - val_loss: 0.7799 - val_acc: 0.8366\n",
      "Epoch 22/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6794 - acc: 0.8660 - val_loss: 0.6802 - val_acc: 0.8690\n",
      "Epoch 23/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6826 - acc: 0.8645 - val_loss: 0.8923 - val_acc: 0.8090\n",
      "Epoch 24/200\n",
      "45000/45000 [==============================] - 19s 431us/step - loss: 0.6870 - acc: 0.8642 - val_loss: 0.7189 - val_acc: 0.8588\n",
      "Epoch 25/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6788 - acc: 0.8663 - val_loss: 0.7469 - val_acc: 0.8526\n",
      "Epoch 26/200\n",
      "45000/45000 [==============================] - 20s 433us/step - loss: 0.6847 - acc: 0.8636 - val_loss: 0.7583 - val_acc: 0.8426\n",
      "Epoch 27/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6779 - acc: 0.8668 - val_loss: 0.7533 - val_acc: 0.8452\n",
      "Epoch 28/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6812 - acc: 0.8662 - val_loss: 0.8288 - val_acc: 0.8198\n",
      "Epoch 29/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.6819 - acc: 0.8656 - val_loss: 0.8117 - val_acc: 0.8284\n",
      "Epoch 30/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6873 - acc: 0.8621 - val_loss: 0.7169 - val_acc: 0.8544\n",
      "Epoch 31/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6757 - acc: 0.8670 - val_loss: 0.7252 - val_acc: 0.8574\n",
      "Epoch 32/200\n",
      "45000/45000 [==============================] - 19s 421us/step - loss: 0.6879 - acc: 0.8641 - val_loss: 0.7891 - val_acc: 0.8400\n",
      "Epoch 33/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6732 - acc: 0.8690 - val_loss: 0.7183 - val_acc: 0.8576\n",
      "Epoch 34/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6772 - acc: 0.8690 - val_loss: 0.7859 - val_acc: 0.8462\n",
      "Epoch 35/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.6846 - acc: 0.8621 - val_loss: 0.6875 - val_acc: 0.8640\n",
      "Epoch 36/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.6752 - acc: 0.8685 - val_loss: 0.6712 - val_acc: 0.8690\n",
      "Epoch 37/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6809 - acc: 0.8675 - val_loss: 0.7001 - val_acc: 0.8622\n",
      "Epoch 38/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6796 - acc: 0.8648 - val_loss: 0.8763 - val_acc: 0.8100\n",
      "Epoch 39/200\n",
      "45000/45000 [==============================] - 19s 431us/step - loss: 0.6790 - acc: 0.8652 - val_loss: 0.7295 - val_acc: 0.8516\n",
      "Epoch 40/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6806 - acc: 0.8661 - val_loss: 0.7884 - val_acc: 0.8360\n",
      "Epoch 41/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6783 - acc: 0.8674 - val_loss: 0.7647 - val_acc: 0.8386\n",
      "Epoch 42/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6801 - acc: 0.8670 - val_loss: 0.7541 - val_acc: 0.8486\n",
      "Epoch 43/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6837 - acc: 0.8643 - val_loss: 0.7904 - val_acc: 0.8360\n",
      "Epoch 44/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6794 - acc: 0.8666 - val_loss: 0.6855 - val_acc: 0.8670\n",
      "Epoch 45/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6823 - acc: 0.8646 - val_loss: 0.7355 - val_acc: 0.8496\n",
      "Epoch 46/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6737 - acc: 0.8687 - val_loss: 0.9544 - val_acc: 0.7996\n",
      "Epoch 47/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6785 - acc: 0.8677 - val_loss: 0.7413 - val_acc: 0.8500\n",
      "Epoch 48/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6769 - acc: 0.8663 - val_loss: 0.6966 - val_acc: 0.8630\n",
      "Epoch 49/200\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 0.6796 - acc: 0.8646 - val_loss: 0.7241 - val_acc: 0.8554\n",
      "Epoch 50/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6742 - acc: 0.8678 - val_loss: 0.7595 - val_acc: 0.8484\n",
      "Epoch 51/200\n",
      "45000/45000 [==============================] - 19s 426us/step - loss: 0.6733 - acc: 0.8682 - val_loss: 1.1347 - val_acc: 0.7438\n",
      "Epoch 52/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6708 - acc: 0.8685 - val_loss: 0.6985 - val_acc: 0.8596\n",
      "Epoch 53/200\n",
      "45000/45000 [==============================] - 19s 427us/step - loss: 0.6769 - acc: 0.8671 - val_loss: 0.6917 - val_acc: 0.8598\n",
      "Epoch 54/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6698 - acc: 0.8686 - val_loss: 0.7041 - val_acc: 0.8576\n",
      "Epoch 55/200\n",
      "45000/45000 [==============================] - 20s 439us/step - loss: 0.6695 - acc: 0.8689 - val_loss: 0.7961 - val_acc: 0.8326\n",
      "Epoch 56/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6695 - acc: 0.8696 - val_loss: 0.7308 - val_acc: 0.8516\n",
      "Epoch 57/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6713 - acc: 0.8688 - val_loss: 0.7916 - val_acc: 0.8394\n",
      "Epoch 58/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6703 - acc: 0.8682 - val_loss: 0.7612 - val_acc: 0.8468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6833 - acc: 0.8658 - val_loss: 0.7057 - val_acc: 0.8588\n",
      "Epoch 60/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6704 - acc: 0.8687 - val_loss: 0.7080 - val_acc: 0.8564\n",
      "Epoch 61/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6738 - acc: 0.8680 - val_loss: 0.7712 - val_acc: 0.8428\n",
      "Epoch 62/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6747 - acc: 0.8676 - val_loss: 0.7808 - val_acc: 0.8458\n",
      "Epoch 63/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6665 - acc: 0.8702 - val_loss: 0.7218 - val_acc: 0.8602\n",
      "Epoch 64/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6604 - acc: 0.8715 - val_loss: 0.7169 - val_acc: 0.8586\n",
      "Epoch 65/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6722 - acc: 0.8674 - val_loss: 0.8187 - val_acc: 0.8260\n",
      "Epoch 66/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6720 - acc: 0.8688 - val_loss: 0.7302 - val_acc: 0.8532\n",
      "Epoch 67/200\n",
      "45000/45000 [==============================] - 19s 431us/step - loss: 0.6594 - acc: 0.8718 - val_loss: 0.6937 - val_acc: 0.8656\n",
      "Epoch 68/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6691 - acc: 0.8693 - val_loss: 0.7192 - val_acc: 0.8574\n",
      "Epoch 69/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6755 - acc: 0.8670 - val_loss: 0.7333 - val_acc: 0.8530\n",
      "Epoch 70/200\n",
      "45000/45000 [==============================] - 20s 438us/step - loss: 0.6684 - acc: 0.8700 - val_loss: 0.8541 - val_acc: 0.8154\n",
      "Epoch 71/200\n",
      "45000/45000 [==============================] - 20s 440us/step - loss: 0.6726 - acc: 0.8677 - val_loss: 0.7284 - val_acc: 0.8524\n",
      "Epoch 72/200\n",
      "45000/45000 [==============================] - 20s 438us/step - loss: 0.6751 - acc: 0.8679 - val_loss: 0.7229 - val_acc: 0.8522\n",
      "Epoch 73/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6723 - acc: 0.8679 - val_loss: 0.7223 - val_acc: 0.8502\n",
      "Epoch 74/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6732 - acc: 0.8676 - val_loss: 0.7300 - val_acc: 0.8522\n",
      "Epoch 75/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6686 - acc: 0.8698 - val_loss: 0.7940 - val_acc: 0.8392\n",
      "Epoch 76/200\n",
      "45000/45000 [==============================] - 20s 438us/step - loss: 0.6658 - acc: 0.8698 - val_loss: 0.7213 - val_acc: 0.8492\n",
      "Epoch 77/200\n",
      "45000/45000 [==============================] - 20s 438us/step - loss: 0.6684 - acc: 0.8692 - val_loss: 0.7284 - val_acc: 0.8474\n",
      "Epoch 78/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6673 - acc: 0.8700 - val_loss: 0.7137 - val_acc: 0.8606\n",
      "Epoch 79/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6667 - acc: 0.8714 - val_loss: 0.7093 - val_acc: 0.8586\n",
      "Epoch 80/200\n",
      "45000/45000 [==============================] - 19s 431us/step - loss: 0.6623 - acc: 0.8718 - val_loss: 0.8091 - val_acc: 0.8280\n",
      "Epoch 81/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6615 - acc: 0.8709 - val_loss: 0.7292 - val_acc: 0.8518\n",
      "Epoch 82/200\n",
      "45000/45000 [==============================] - 20s 433us/step - loss: 0.6611 - acc: 0.8726 - val_loss: 1.1397 - val_acc: 0.7420\n",
      "Epoch 83/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6708 - acc: 0.8692 - val_loss: 0.7189 - val_acc: 0.8610\n",
      "Epoch 84/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6644 - acc: 0.8706 - val_loss: 0.7154 - val_acc: 0.8558\n",
      "Epoch 85/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6636 - acc: 0.8698 - val_loss: 0.6943 - val_acc: 0.8620\n",
      "Epoch 86/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6683 - acc: 0.8702 - val_loss: 0.7378 - val_acc: 0.8428\n",
      "Epoch 87/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6676 - acc: 0.8703 - val_loss: 0.6894 - val_acc: 0.8610\n",
      "Epoch 88/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6661 - acc: 0.8713 - val_loss: 0.7143 - val_acc: 0.8600\n",
      "Epoch 89/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6699 - acc: 0.8690 - val_loss: 0.6782 - val_acc: 0.8644\n",
      "Epoch 90/200\n",
      "45000/45000 [==============================] - 20s 438us/step - loss: 0.6711 - acc: 0.8698 - val_loss: 0.7619 - val_acc: 0.8428\n",
      "Epoch 91/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6678 - acc: 0.8701 - val_loss: 0.8220 - val_acc: 0.8214\n",
      "Epoch 92/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6674 - acc: 0.8707 - val_loss: 0.7709 - val_acc: 0.8360\n",
      "Epoch 93/200\n",
      "45000/45000 [==============================] - 20s 439us/step - loss: 0.6651 - acc: 0.8704 - val_loss: 0.6869 - val_acc: 0.8664\n",
      "Epoch 94/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6596 - acc: 0.8713 - val_loss: 0.7621 - val_acc: 0.8382\n",
      "Epoch 95/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6658 - acc: 0.8692 - val_loss: 0.6878 - val_acc: 0.8690\n",
      "Epoch 96/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6593 - acc: 0.8722 - val_loss: 0.7232 - val_acc: 0.8562\n",
      "Epoch 97/200\n",
      "45000/45000 [==============================] - 20s 433us/step - loss: 0.6610 - acc: 0.8718 - val_loss: 0.7136 - val_acc: 0.8576\n",
      "Epoch 98/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6676 - acc: 0.8694 - val_loss: 0.9872 - val_acc: 0.7792\n",
      "Epoch 99/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6702 - acc: 0.8685 - val_loss: 0.8260 - val_acc: 0.8210\n",
      "Epoch 100/200\n",
      "45000/45000 [==============================] - 20s 438us/step - loss: 0.6738 - acc: 0.8677 - val_loss: 0.9423 - val_acc: 0.7818\n",
      "Epoch 101/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6671 - acc: 0.8711 - val_loss: 0.7359 - val_acc: 0.8510\n",
      "Epoch 102/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6639 - acc: 0.8711 - val_loss: 0.6930 - val_acc: 0.8620\n",
      "Epoch 103/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6631 - acc: 0.8711 - val_loss: 0.7150 - val_acc: 0.8548\n",
      "Epoch 104/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6613 - acc: 0.8718 - val_loss: 0.8142 - val_acc: 0.8244\n",
      "Epoch 105/200\n",
      "45000/45000 [==============================] - 19s 431us/step - loss: 0.6562 - acc: 0.8747 - val_loss: 0.7454 - val_acc: 0.8474\n",
      "Epoch 106/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6598 - acc: 0.8724 - val_loss: 0.6702 - val_acc: 0.8726\n",
      "Epoch 107/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6563 - acc: 0.8724 - val_loss: 0.6813 - val_acc: 0.8724\n",
      "Epoch 108/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6627 - acc: 0.8705 - val_loss: 0.7016 - val_acc: 0.8656\n",
      "Epoch 109/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6620 - acc: 0.8696 - val_loss: 1.4704 - val_acc: 0.6626\n",
      "Epoch 110/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6598 - acc: 0.8721 - val_loss: 0.7332 - val_acc: 0.8524\n",
      "Epoch 111/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6573 - acc: 0.8728 - val_loss: 0.7336 - val_acc: 0.8546\n",
      "Epoch 112/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6605 - acc: 0.8727 - val_loss: 0.6882 - val_acc: 0.8662\n",
      "Epoch 113/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6574 - acc: 0.8737 - val_loss: 0.6960 - val_acc: 0.8614\n",
      "Epoch 114/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6596 - acc: 0.8736 - val_loss: 0.7379 - val_acc: 0.8538\n",
      "Epoch 115/200\n",
      "45000/45000 [==============================] - 19s 428us/step - loss: 0.6635 - acc: 0.8706 - val_loss: 0.7337 - val_acc: 0.8520\n",
      "Epoch 116/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6623 - acc: 0.8710 - val_loss: 0.7273 - val_acc: 0.8580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6674 - acc: 0.8699 - val_loss: 0.7333 - val_acc: 0.8534\n",
      "Epoch 118/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6642 - acc: 0.8699 - val_loss: 0.7114 - val_acc: 0.8622\n",
      "Epoch 119/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6598 - acc: 0.8723 - val_loss: 0.6994 - val_acc: 0.8648\n",
      "Epoch 120/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6647 - acc: 0.8689 - val_loss: 0.7486 - val_acc: 0.8474\n",
      "Epoch 121/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6514 - acc: 0.8735 - val_loss: 0.7088 - val_acc: 0.8632\n",
      "Epoch 122/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6582 - acc: 0.8716 - val_loss: 0.7353 - val_acc: 0.8478\n",
      "Epoch 123/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6610 - acc: 0.8719 - val_loss: 0.7184 - val_acc: 0.8560\n",
      "Epoch 124/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6595 - acc: 0.8723 - val_loss: 0.7472 - val_acc: 0.8492\n",
      "Epoch 125/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6658 - acc: 0.8710 - val_loss: 0.6735 - val_acc: 0.8708\n",
      "Epoch 126/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6590 - acc: 0.8711 - val_loss: 0.7256 - val_acc: 0.8566\n",
      "Epoch 127/200\n",
      "45000/45000 [==============================] - 20s 439us/step - loss: 0.6560 - acc: 0.8730 - val_loss: 0.7726 - val_acc: 0.8382\n",
      "Epoch 128/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6651 - acc: 0.8706 - val_loss: 0.6985 - val_acc: 0.8588\n",
      "Epoch 129/200\n",
      "45000/45000 [==============================] - 19s 431us/step - loss: 0.6633 - acc: 0.8718 - val_loss: 0.9833 - val_acc: 0.7796\n",
      "Epoch 130/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6624 - acc: 0.8715 - val_loss: 0.9041 - val_acc: 0.7980\n",
      "Epoch 131/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6644 - acc: 0.8704 - val_loss: 0.7733 - val_acc: 0.8404\n",
      "Epoch 132/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6581 - acc: 0.8730 - val_loss: 0.7502 - val_acc: 0.8492\n",
      "Epoch 133/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6528 - acc: 0.8747 - val_loss: 0.7140 - val_acc: 0.8590\n",
      "Epoch 134/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6627 - acc: 0.8694 - val_loss: 0.8305 - val_acc: 0.8218\n",
      "Epoch 135/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6630 - acc: 0.8697 - val_loss: 0.7923 - val_acc: 0.8362\n",
      "Epoch 136/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6592 - acc: 0.8720 - val_loss: 0.7262 - val_acc: 0.8568\n",
      "Epoch 137/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6609 - acc: 0.8714 - val_loss: 0.6967 - val_acc: 0.8642\n",
      "Epoch 138/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6582 - acc: 0.8710 - val_loss: 0.7186 - val_acc: 0.8572\n",
      "Epoch 139/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6602 - acc: 0.8704 - val_loss: 0.7049 - val_acc: 0.8582\n",
      "Epoch 140/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6629 - acc: 0.8714 - val_loss: 0.8151 - val_acc: 0.8286\n",
      "Epoch 141/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6542 - acc: 0.8743 - val_loss: 0.7076 - val_acc: 0.8572\n",
      "Epoch 142/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6585 - acc: 0.8713 - val_loss: 0.8078 - val_acc: 0.8294\n",
      "Epoch 143/200\n",
      "45000/45000 [==============================] - 19s 431us/step - loss: 0.6604 - acc: 0.8704 - val_loss: 0.8296 - val_acc: 0.8178\n",
      "Epoch 144/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6586 - acc: 0.8719 - val_loss: 0.8418 - val_acc: 0.8246\n",
      "Epoch 145/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6647 - acc: 0.8695 - val_loss: 0.6937 - val_acc: 0.8614\n",
      "Epoch 146/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6596 - acc: 0.8727 - val_loss: 0.7444 - val_acc: 0.8546\n",
      "Epoch 147/200\n",
      "45000/45000 [==============================] - 20s 433us/step - loss: 0.6567 - acc: 0.8724 - val_loss: 0.8354 - val_acc: 0.8278\n",
      "Epoch 148/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6657 - acc: 0.8713 - val_loss: 0.7610 - val_acc: 0.8460\n",
      "Epoch 149/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6602 - acc: 0.8725 - val_loss: 0.7229 - val_acc: 0.8552\n",
      "Epoch 150/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6563 - acc: 0.8720 - val_loss: 0.7298 - val_acc: 0.8552\n",
      "Epoch 151/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6627 - acc: 0.8710 - val_loss: 0.7552 - val_acc: 0.8420\n",
      "Epoch 152/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6569 - acc: 0.8707 - val_loss: 0.7522 - val_acc: 0.8492\n",
      "Epoch 153/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6639 - acc: 0.8705 - val_loss: 0.7651 - val_acc: 0.8380\n",
      "Epoch 154/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6535 - acc: 0.8716 - val_loss: 0.8179 - val_acc: 0.8252\n",
      "Epoch 155/200\n",
      "45000/45000 [==============================] - 19s 430us/step - loss: 0.6534 - acc: 0.8738 - val_loss: 0.7321 - val_acc: 0.8508\n",
      "Epoch 156/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6578 - acc: 0.8712 - val_loss: 0.7143 - val_acc: 0.8584\n",
      "Epoch 157/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.6534 - acc: 0.8737 - val_loss: 0.7224 - val_acc: 0.8572\n",
      "Epoch 158/200\n",
      "45000/45000 [==============================] - 20s 439us/step - loss: 0.6590 - acc: 0.8718 - val_loss: 0.6893 - val_acc: 0.8692\n",
      "Epoch 159/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6653 - acc: 0.8716 - val_loss: 0.7083 - val_acc: 0.8536\n",
      "Epoch 160/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6558 - acc: 0.8736 - val_loss: 0.7206 - val_acc: 0.8560\n",
      "Epoch 161/200\n",
      "45000/45000 [==============================] - 19s 431us/step - loss: 0.6602 - acc: 0.8733 - val_loss: 0.7172 - val_acc: 0.8604\n",
      "Epoch 162/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6470 - acc: 0.8758 - val_loss: 0.8269 - val_acc: 0.8248\n",
      "Epoch 163/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6592 - acc: 0.8708 - val_loss: 0.7898 - val_acc: 0.8310\n",
      "Epoch 164/200\n",
      "45000/45000 [==============================] - 19s 429us/step - loss: 0.6541 - acc: 0.8739 - val_loss: 0.7205 - val_acc: 0.8576\n",
      "Epoch 165/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6555 - acc: 0.8738 - val_loss: 0.6962 - val_acc: 0.8622\n",
      "Epoch 166/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6540 - acc: 0.8734 - val_loss: 0.7056 - val_acc: 0.8614\n",
      "Epoch 167/200\n",
      "45000/45000 [==============================] - 20s 433us/step - loss: 0.6649 - acc: 0.8698 - val_loss: 0.7626 - val_acc: 0.8414\n",
      "Epoch 168/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6560 - acc: 0.8733 - val_loss: 0.6995 - val_acc: 0.8604\n",
      "Epoch 169/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6649 - acc: 0.8707 - val_loss: 0.7218 - val_acc: 0.8562\n",
      "Epoch 170/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6571 - acc: 0.8728 - val_loss: 0.7101 - val_acc: 0.8618\n",
      "Epoch 171/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6574 - acc: 0.8716 - val_loss: 0.7135 - val_acc: 0.8560\n",
      "Epoch 172/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6612 - acc: 0.8703 - val_loss: 0.7300 - val_acc: 0.8592\n",
      "Epoch 173/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6586 - acc: 0.8710 - val_loss: 0.7413 - val_acc: 0.8494\n",
      "Epoch 174/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6584 - acc: 0.8712 - val_loss: 0.7217 - val_acc: 0.8492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6546 - acc: 0.8736 - val_loss: 0.7631 - val_acc: 0.8402\n",
      "Epoch 176/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6445 - acc: 0.8771 - val_loss: 0.6989 - val_acc: 0.8580\n",
      "Epoch 177/200\n",
      "45000/45000 [==============================] - 20s 438us/step - loss: 0.6485 - acc: 0.8755 - val_loss: 0.9518 - val_acc: 0.7840\n",
      "Epoch 178/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6496 - acc: 0.8749 - val_loss: 0.9783 - val_acc: 0.7868\n",
      "Epoch 179/200\n",
      "45000/45000 [==============================] - 20s 439us/step - loss: 0.6556 - acc: 0.8736 - val_loss: 0.7413 - val_acc: 0.8462\n",
      "Epoch 180/200\n",
      "45000/45000 [==============================] - 20s 433us/step - loss: 0.6481 - acc: 0.8734 - val_loss: 0.7145 - val_acc: 0.8596\n",
      "Epoch 181/200\n",
      "45000/45000 [==============================] - 20s 438us/step - loss: 0.6633 - acc: 0.8700 - val_loss: 0.7065 - val_acc: 0.8600\n",
      "Epoch 182/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6505 - acc: 0.8744 - val_loss: 0.7627 - val_acc: 0.8438\n",
      "Epoch 183/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6466 - acc: 0.8736 - val_loss: 0.6940 - val_acc: 0.8670\n",
      "Epoch 184/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6511 - acc: 0.8753 - val_loss: 0.6802 - val_acc: 0.8716\n",
      "Epoch 185/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6534 - acc: 0.8734 - val_loss: 0.7423 - val_acc: 0.8484\n",
      "Epoch 186/200\n",
      "45000/45000 [==============================] - 19s 432us/step - loss: 0.6493 - acc: 0.8726 - val_loss: 0.7323 - val_acc: 0.8512\n",
      "Epoch 187/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6511 - acc: 0.8737 - val_loss: 0.7055 - val_acc: 0.8568\n",
      "Epoch 188/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6506 - acc: 0.8750 - val_loss: 0.7361 - val_acc: 0.8520\n",
      "Epoch 189/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6501 - acc: 0.8728 - val_loss: 0.7344 - val_acc: 0.8524\n",
      "Epoch 190/200\n",
      "45000/45000 [==============================] - 20s 436us/step - loss: 0.6585 - acc: 0.8710 - val_loss: 0.7004 - val_acc: 0.8602\n",
      "Epoch 191/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6545 - acc: 0.8740 - val_loss: 0.7052 - val_acc: 0.8594\n",
      "Epoch 192/200\n",
      "45000/45000 [==============================] - 20s 438us/step - loss: 0.6561 - acc: 0.8732 - val_loss: 0.6976 - val_acc: 0.8574\n",
      "Epoch 193/200\n",
      "45000/45000 [==============================] - 20s 437us/step - loss: 0.6504 - acc: 0.8739 - val_loss: 0.7788 - val_acc: 0.8396\n",
      "Epoch 194/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6475 - acc: 0.8756 - val_loss: 0.7943 - val_acc: 0.8346\n",
      "Epoch 195/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6520 - acc: 0.8734 - val_loss: 0.6600 - val_acc: 0.8706\n",
      "Epoch 196/200\n",
      "45000/45000 [==============================] - 20s 435us/step - loss: 0.6509 - acc: 0.8746 - val_loss: 0.6973 - val_acc: 0.8632\n",
      "Epoch 197/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6434 - acc: 0.8771 - val_loss: 0.7560 - val_acc: 0.8406\n",
      "Epoch 198/200\n",
      "45000/45000 [==============================] - 20s 434us/step - loss: 0.6522 - acc: 0.8732 - val_loss: 0.7207 - val_acc: 0.8556\n",
      "Epoch 199/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6472 - acc: 0.8743 - val_loss: 0.7516 - val_acc: 0.8478\n",
      "Epoch 200/200\n",
      "45000/45000 [==============================] - 19s 433us/step - loss: 0.6505 - acc: 0.8728 - val_loss: 0.8360 - val_acc: 0.8246\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    " \n",
    "train_history=model_2.fit(x_train,y_train,epochs=200, batch_size=128,\n",
    "             validation_split=0.1, verbose=1)\n",
    "\n",
    "model_2.save('8.6.5ent_b_cifar10vgg16_2_1.h5')\n",
    "model_2.save_weights('8.6.5ent_b_cifar10vgg16_2_1_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8455086454391479, 0.8194, 0.9902, 0.8194]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import keras\n",
    "top1_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=1) #top-1精度\n",
    "\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',metrics=['accuracy','top_k_categorical_accuracy',top1_acc])\n",
    "\n",
    "model_2.evaluate(x_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试损失为：0.8455\n",
      "测试准确率为：0.8194\n"
     ]
    }
   ],
   "source": [
    "score=model_2.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"测试损失为：{:.4f}\".format(score[0]))\n",
    "print(\"测试准确率为：{:.4f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2.0450879396633783, 2.1083841327784483, 1.285267694621184]\n",
      "1 [2.9154759474226504, 1.3228756555322954, 0.5590169943749475]\n",
      "2 [2.017294297375586, 4.121371609690142, 5.446591844054512]\n",
      "3 [2.5369909678440226, 10.207552639958413, 8.831261420326921]\n",
      "4 [0.9882006428568496, 1.4583378766564883, 0.8561259161315505]\n",
      "5 [3.4348286565999024, 1.716220421986735, 1.2730988596356685]\n",
      "6 [2.843714392715533, 2.6701209706356464, 11.063107775571373]\n",
      "7 [2.227754615077702, 2.4980461114639176, 1.653594569415369]\n",
      "8 [10.110114210598656, 10.218423295704296, 2.001953065453505]\n",
      "9 [12.611061947657188, 2.0000011813576455, 1.2348743471297698]\n",
      "10 [2.099106952968333, 1.9921721813136535, 2.692582403567252]\n",
      "11 [2.413712040915332, 9.809795340980354, 2.571836848572772]\n",
      "12 [30.66874045788391, 21.8770303056471, 9.106411905099579]\n",
      "13 [13.856441699170102, 0.0, 11.215700207848984]\n",
      "14 [8.51305487808666, 11.551561439541976, 2.1610195494446764]\n",
      "15 [1.557491974297139, 1.25, 2.0223362109204297]\n",
      "16 [1.8192224472284855, 1.5077922602268523, 1.7316278919704429]\n",
      "17 [1.6495861553172175, 1.5350200796522526, 2.6819443541111156]\n",
      "18 [3.1735294330602954, 2.375, 1.8856468551931986]\n",
      "19 [2.9154759474226504, 2.1578345627040085, 1.9803724397193574]\n",
      "20 [6.615561288843075, 1.418498783514774, 6.912972772789931]\n",
      "21 [1.9999999999220108, 1.4150803375810637, 6.362827490506664]\n",
      "22 [4.270337404750052, 5.130455067957466, 3.5273072390434366]\n",
      "23 [4.046603514059662, 4.782781617427248, 0.5]\n",
      "24 [7.752963945222052, 11.560709984864964, 12.412073493383422]\n",
      "25 [0.4956019434310234, 16.970735479432236, 21.99685414537237]\n",
      "26 [3.9304938730867365, 1.1134237967800726, 10.437195505052166]\n",
      "27 [1.3228415920952514, 5.034373149243991, 5.656854249492381]\n",
      "28 [1.3247746451389613, 1.6414371977788638, 1.942653583192111]\n",
      "29 [2.000447590555097, 3.1253077736929336, 0.5619021523505877]\n",
      "30 [2.2707377655731187, 2.4109126902482387, 1.7919088983539313]\n",
      "31 [16.142515561074315, 2.0000008998175005, 2.452218769994729]\n",
      "32 [1.5811388300841898, 0.0, 0.0]\n",
      "33 [8.694996021745856, 5.576615383060979, 1.6572933821394051]\n",
      "34 [1.7216017736527922, 4.69086544340604, 4.752046069119368]\n",
      "35 [1.6298006013006623, 2.462214450449026, 1.0897247358851685]\n",
      "36 [5.058260260823498, 10.604498190403437, 3.777590943762197]\n",
      "37 [4.581618125353563, 1.0470532976563316, 6.009107799808618]\n",
      "38 [2.2492472048248455, 13.80967338258642, 2.925539055083248]\n",
      "39 [1.983716968256561, 10.80569258805486, 10.916147932560243]\n",
      "40 [1.9611011722993357, 1.9999998045905902, 2.1989485047340582]\n",
      "41 [9.425748063814352, 1.59375, 1.7365554986812255]\n",
      "42 [10.74726553448881, 2.273942690526974, 9.76437592109442]\n",
      "43 [2.111097077035389, 1.9814942905994912, 3.043505952987502]\n",
      "44 [8.802166539378279, 2.5890595297373356, 0.31042581273799164]\n",
      "45 [5.680238070032084, 5.852388657232216, 2.154209875319704]\n",
      "46 [2.8840212218549537, 3.1473384197746563, 3.048422239608539]\n",
      "47 [2.171369095755026, 2.339370909026613, 1.9588042430268524]\n",
      "48 [2.8996737535070474, 1.905246470469234, 0.35296398137667917]\n",
      "49 [7.0710678118654755, 4.367851302414037, 1.7633419974582356]\n",
      "50 [7.242453956023106, 2.116456176289396, 4.5702924026960305]\n",
      "51 [3.40572051211351, 3.08092100696771, 17.45053116828883]\n",
      "52 [3.583766481599806, 2.8747918634124106, 6.177924597986724]\n",
      "53 [2.5887503869218964, 1.5904483910713618, 3.0083947769932307]\n",
      "54 [2.2707377655731187, 2.6339134382131846, 4.3535186918169995]\n",
      "55 [1.9973393126742423, 12.80244443843013, 12.928217446464869]\n",
      "56 [3.36804839632687, 2.053959590644373, 2.75]\n",
      "57 [2.0138270913066045, 2.2912926860023486, 1.7283214912664873]\n",
      "58 [2.6164294611168097, 1.9105995459540965, 1.7853571071357126]\n",
      "59 [16.397817417323093, 2.757596257352776, 8.567986109350683]\n",
      "60 [11.605333911031543, 3.8598822287396666, 12.984391746097053]\n",
      "61 [8.430854129758698, 8.588159886178197, 2.292354637974552]\n",
      "62 [0.9746885859956406, 1.3146277831691846, 1.2715678971336126]\n",
      "63 [4.992363265499854, 4.751404554607682, 2.1102322591004574]\n",
      "[[2.0450879396633783, 2.1083841327784483, 1.285267694621184], [2.9154759474226504, 1.3228756555322954, 0.5590169943749475], [2.017294297375586, 4.121371609690142, 5.446591844054512], [2.5369909678440226, 10.207552639958413, 8.831261420326921], [0.9882006428568496, 1.4583378766564883, 0.8561259161315505], [3.4348286565999024, 1.716220421986735, 1.2730988596356685], [2.843714392715533, 2.6701209706356464, 11.063107775571373], [2.227754615077702, 2.4980461114639176, 1.653594569415369], [10.110114210598656, 10.218423295704296, 2.001953065453505], [12.611061947657188, 2.0000011813576455, 1.2348743471297698], [2.099106952968333, 1.9921721813136535, 2.692582403567252], [2.413712040915332, 9.809795340980354, 2.571836848572772], [30.66874045788391, 21.8770303056471, 9.106411905099579], [13.856441699170102, 0.0, 11.215700207848984], [8.51305487808666, 11.551561439541976, 2.1610195494446764], [1.557491974297139, 1.25, 2.0223362109204297], [1.8192224472284855, 1.5077922602268523, 1.7316278919704429], [1.6495861553172175, 1.5350200796522526, 2.6819443541111156], [3.1735294330602954, 2.375, 1.8856468551931986], [2.9154759474226504, 2.1578345627040085, 1.9803724397193574], [6.615561288843075, 1.418498783514774, 6.912972772789931], [1.9999999999220108, 1.4150803375810637, 6.362827490506664], [4.270337404750052, 5.130455067957466, 3.5273072390434366], [4.046603514059662, 4.782781617427248, 0.5], [7.752963945222052, 11.560709984864964, 12.412073493383422], [0.4956019434310234, 16.970735479432236, 21.99685414537237], [3.9304938730867365, 1.1134237967800726, 10.437195505052166], [1.3228415920952514, 5.034373149243991, 5.656854249492381], [1.3247746451389613, 1.6414371977788638, 1.942653583192111], [2.000447590555097, 3.1253077736929336, 0.5619021523505877], [2.2707377655731187, 2.4109126902482387, 1.7919088983539313], [16.142515561074315, 2.0000008998175005, 2.452218769994729], [1.5811388300841898, 0.0, 0.0], [8.694996021745856, 5.576615383060979, 1.6572933821394051], [1.7216017736527922, 4.69086544340604, 4.752046069119368], [1.6298006013006623, 2.462214450449026, 1.0897247358851685], [5.058260260823498, 10.604498190403437, 3.777590943762197], [4.581618125353563, 1.0470532976563316, 6.009107799808618], [2.2492472048248455, 13.80967338258642, 2.925539055083248], [1.983716968256561, 10.80569258805486, 10.916147932560243], [1.9611011722993357, 1.9999998045905902, 2.1989485047340582], [9.425748063814352, 1.59375, 1.7365554986812255], [10.74726553448881, 2.273942690526974, 9.76437592109442], [2.111097077035389, 1.9814942905994912, 3.043505952987502], [8.802166539378279, 2.5890595297373356, 0.31042581273799164], [5.680238070032084, 5.852388657232216, 2.154209875319704], [2.8840212218549537, 3.1473384197746563, 3.048422239608539], [2.171369095755026, 2.339370909026613, 1.9588042430268524], [2.8996737535070474, 1.905246470469234, 0.35296398137667917], [7.0710678118654755, 4.367851302414037, 1.7633419974582356], [7.242453956023106, 2.116456176289396, 4.5702924026960305], [3.40572051211351, 3.08092100696771, 17.45053116828883], [3.583766481599806, 2.8747918634124106, 6.177924597986724], [2.5887503869218964, 1.5904483910713618, 3.0083947769932307], [2.2707377655731187, 2.6339134382131846, 4.3535186918169995], [1.9973393126742423, 12.80244443843013, 12.928217446464869], [3.36804839632687, 2.053959590644373, 2.75], [2.0138270913066045, 2.2912926860023486, 1.7283214912664873], [2.6164294611168097, 1.9105995459540965, 1.7853571071357126], [16.397817417323093, 2.757596257352776, 8.567986109350683], [11.605333911031543, 3.8598822287396666, 12.984391746097053], [8.430854129758698, 8.588159886178197, 2.292354637974552], [0.9746885859956406, 1.3146277831691846, 1.2715678971336126], [4.992363265499854, 4.751404554607682, 2.1102322591004574]]\n"
     ]
    }
   ],
   "source": [
    "tensor =[]\n",
    "for i in range(0,64):\n",
    "   \n",
    "    u=model_2.get_layer('conv2d_1').get_weights()[0][:,:,:,i].squeeze()\n",
    "    v=np.mean(u, axis=1)\n",
    "    #print(v)\n",
    "    vT=v.T\n",
    "    D=np.cov(vT)\n",
    "    try:                 \n",
    "        invD=np.linalg.inv(D)\n",
    "        #t=np.mean(v,axis=1)\n",
    "        a=[]\n",
    "\n",
    "        for j in range(0,2): #表示filter的大小\n",
    "            for k in range(j+1,3):\n",
    "               #if (j!=k) and (j<k):\n",
    "                tp=v[j]-v[k]\n",
    "                d=np.sqrt(abs(np.dot(np.dot(tp,invD),tp.T)))\n",
    "                a.append(d)\n",
    "            \n",
    "        print(i,a)\n",
    "        tensor.append(a)\n",
    "    except:\n",
    "        print(\"不可逆\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0, 20.0, 30.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{10.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 20.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.6666666666666666\n",
      "0.38997500048077083\n",
      "0.3333333333333333\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0, 20.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.3333333333333333\n",
      "1.0566416671474375\n",
      "0.3333333333333333\n",
      "1.584962500721156\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0, 10.0}\n",
      "0.3333333333333333\n",
      "0.5283208335737187\n",
      "0.6666666666666666\n",
      "0.9182958340544896\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "四舍五入，精确到个位\n",
      " [0. 0. 0.]\n",
      "{0.0}\n",
      "1.0\n",
      "0.0\n",
      "[0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0, 0.0, 0.9182958340544896, 0.0, 0.9182958340544896, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0, 0.0, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0, 0.9182958340544896, 0.0, 0.0, 0.0, 1.584962500721156, 0.9182958340544896, 0.9182958340544896, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "entt=[]\n",
    "for i in range (0,64):\n",
    "    \n",
    "    data=tensor[i]\n",
    "    data0=np.array(data)\n",
    "    print('四舍五入，精确到个位\\n',np.round(data0,decimals=-2))\n",
    "    data1=np.round(data0,decimals=-1)\n",
    "\n",
    "    data1_value_list=set([data1[i] for i in range (data1.shape[0])])\n",
    "    print(data1_value_list)\n",
    "    ent=0.0\n",
    "    for data1_value in data1_value_list:\n",
    "        p=float(data1[data1==data1_value].shape[0])/data1.shape[0]\n",
    "        print(p)\n",
    "        logp=np.log2(p)\n",
    "        ent-=p*logp\n",
    "        print(ent)   \n",
    "    entt.append(ent)\n",
    "print(entt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9182958340544896]\n",
      "[0, 1, 4, 5, 7, 10, 15, 16, 17, 18, 19, 23, 24, 28, 29, 30, 32, 34, 35, 40, 43, 46, 47, 48, 53, 54, 56, 57, 58, 62, 63, 2]\n"
     ]
    }
   ],
   "source": [
    "import heapq #获取list中最小的\n",
    "\n",
    "f=int(len(entt)*0.5) #计算滤波器熵个数的80%\n",
    "m=entt\n",
    "max_number=heapq.nsmallest(f,m) #从m中找出最小的f个数，最大用nlargest\n",
    "max_index=[]\n",
    "for t in max_number:\n",
    "    index=m.index(t)\n",
    "    max_index.append(index)\n",
    "    m[index]=float('-inf')\n",
    "print(max_number)#输出最小的f个数\n",
    "print(max_index)#输出对应索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 32/64 channels from layer: conv2d_1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 52)        15028     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 52)        208       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 52)        24388     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 52)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 52)          24388     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 52)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 52)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 52)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 103)         48307     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 103)         95584     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 103)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 103)         412       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 103)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               53248     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 977,849\n",
      "Trainable params: 973,917\n",
      "Non-trainable params: 3,932\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerassurgeon.operations import delete_layer,insert_layer,delete_channels\n",
    "model_1 = delete_channels(model_2, model_2.layers[1],max_index)\n",
    "model_1.summary()\n",
    "model_1.save('8.6.5ent_b_cifar10vgg16_1.h5')\n",
    "model_1.save_weights('8.6.5ent_b_cifar10vgg16_1_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "45000/45000 [==============================] - 23s 519us/step - loss: 0.7201 - acc: 0.8478 - val_loss: 0.8940 - val_acc: 0.8000\n",
      "Epoch 2/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6867 - acc: 0.8592 - val_loss: 0.7272 - val_acc: 0.8502\n",
      "Epoch 3/200\n",
      "45000/45000 [==============================] - 15s 341us/step - loss: 0.6976 - acc: 0.8559 - val_loss: 0.8300 - val_acc: 0.8192\n",
      "Epoch 4/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6889 - acc: 0.8601 - val_loss: 0.7205 - val_acc: 0.8552\n",
      "Epoch 5/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6915 - acc: 0.8589 - val_loss: 0.8627 - val_acc: 0.8058\n",
      "Epoch 6/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6838 - acc: 0.8608 - val_loss: 0.7104 - val_acc: 0.8554\n",
      "Epoch 7/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6819 - acc: 0.8639 - val_loss: 0.7248 - val_acc: 0.8522\n",
      "Epoch 8/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6804 - acc: 0.8629 - val_loss: 0.8111 - val_acc: 0.8242\n",
      "Epoch 9/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6892 - acc: 0.8615 - val_loss: 0.7517 - val_acc: 0.8438\n",
      "Epoch 10/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6767 - acc: 0.8662 - val_loss: 1.0114 - val_acc: 0.7744\n",
      "Epoch 11/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6805 - acc: 0.8646 - val_loss: 0.7827 - val_acc: 0.8380\n",
      "Epoch 12/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6841 - acc: 0.8634 - val_loss: 0.7555 - val_acc: 0.8438\n",
      "Epoch 13/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6818 - acc: 0.8651 - val_loss: 0.7633 - val_acc: 0.8398\n",
      "Epoch 14/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6804 - acc: 0.8644 - val_loss: 0.7128 - val_acc: 0.8572\n",
      "Epoch 15/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6863 - acc: 0.8633 - val_loss: 0.8339 - val_acc: 0.8156\n",
      "Epoch 16/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6827 - acc: 0.8658 - val_loss: 0.7161 - val_acc: 0.8600\n",
      "Epoch 17/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6732 - acc: 0.8678 - val_loss: 0.7058 - val_acc: 0.8618\n",
      "Epoch 18/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6762 - acc: 0.8671 - val_loss: 0.7530 - val_acc: 0.8396\n",
      "Epoch 19/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6814 - acc: 0.8644 - val_loss: 0.6810 - val_acc: 0.8704\n",
      "Epoch 20/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6777 - acc: 0.8668 - val_loss: 0.8497 - val_acc: 0.8184\n",
      "Epoch 21/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6727 - acc: 0.8676 - val_loss: 0.7118 - val_acc: 0.8564\n",
      "Epoch 22/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6846 - acc: 0.8636 - val_loss: 0.7582 - val_acc: 0.8358\n",
      "Epoch 23/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6740 - acc: 0.8685 - val_loss: 0.7306 - val_acc: 0.8496\n",
      "Epoch 24/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6782 - acc: 0.8669 - val_loss: 0.7895 - val_acc: 0.8292\n",
      "Epoch 25/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6750 - acc: 0.8660 - val_loss: 0.6713 - val_acc: 0.8708\n",
      "Epoch 26/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6699 - acc: 0.8671 - val_loss: 0.7563 - val_acc: 0.8492\n",
      "Epoch 27/200\n",
      "45000/45000 [==============================] - 15s 340us/step - loss: 0.6779 - acc: 0.8675 - val_loss: 0.7189 - val_acc: 0.8582\n",
      "Epoch 28/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6663 - acc: 0.8681 - val_loss: 0.7751 - val_acc: 0.8350\n",
      "Epoch 29/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6767 - acc: 0.8650 - val_loss: 0.7455 - val_acc: 0.8504\n",
      "Epoch 30/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6760 - acc: 0.8658 - val_loss: 0.7543 - val_acc: 0.8402\n",
      "Epoch 31/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6712 - acc: 0.8676 - val_loss: 0.7471 - val_acc: 0.8504\n",
      "Epoch 32/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6823 - acc: 0.8643 - val_loss: 0.8371 - val_acc: 0.8230\n",
      "Epoch 33/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6747 - acc: 0.8667 - val_loss: 0.6917 - val_acc: 0.8638\n",
      "Epoch 34/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6769 - acc: 0.8660 - val_loss: 0.8103 - val_acc: 0.8264\n",
      "Epoch 35/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6769 - acc: 0.8672 - val_loss: 0.8478 - val_acc: 0.8190\n",
      "Epoch 36/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6744 - acc: 0.8683 - val_loss: 0.7203 - val_acc: 0.8560\n",
      "Epoch 37/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6766 - acc: 0.8646 - val_loss: 0.7273 - val_acc: 0.8540\n",
      "Epoch 38/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6743 - acc: 0.8680 - val_loss: 0.7208 - val_acc: 0.8526\n",
      "Epoch 39/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6731 - acc: 0.8669 - val_loss: 0.7055 - val_acc: 0.8596\n",
      "Epoch 40/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6733 - acc: 0.8664 - val_loss: 0.7102 - val_acc: 0.8566\n",
      "Epoch 41/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6739 - acc: 0.8676 - val_loss: 0.7271 - val_acc: 0.8560\n",
      "Epoch 42/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6753 - acc: 0.8679 - val_loss: 0.8065 - val_acc: 0.8308\n",
      "Epoch 43/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6750 - acc: 0.8664 - val_loss: 0.7221 - val_acc: 0.8552\n",
      "Epoch 44/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6783 - acc: 0.8662 - val_loss: 0.8689 - val_acc: 0.8100\n",
      "Epoch 45/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6787 - acc: 0.8655 - val_loss: 0.7537 - val_acc: 0.8440\n",
      "Epoch 46/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6713 - acc: 0.8684 - val_loss: 0.8369 - val_acc: 0.8178\n",
      "Epoch 47/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6791 - acc: 0.8652 - val_loss: 0.8052 - val_acc: 0.8310\n",
      "Epoch 48/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6748 - acc: 0.8673 - val_loss: 0.7325 - val_acc: 0.8476\n",
      "Epoch 49/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6716 - acc: 0.8690 - val_loss: 0.6832 - val_acc: 0.8638\n",
      "Epoch 50/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6773 - acc: 0.8658 - val_loss: 0.7317 - val_acc: 0.8466\n",
      "Epoch 51/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6752 - acc: 0.8665 - val_loss: 0.6795 - val_acc: 0.8672\n",
      "Epoch 52/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6734 - acc: 0.8682 - val_loss: 0.7226 - val_acc: 0.8556\n",
      "Epoch 53/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6733 - acc: 0.8655 - val_loss: 0.7263 - val_acc: 0.8568\n",
      "Epoch 54/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6772 - acc: 0.8658 - val_loss: 0.7413 - val_acc: 0.8514\n",
      "Epoch 55/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6686 - acc: 0.8700 - val_loss: 0.6844 - val_acc: 0.8704\n",
      "Epoch 56/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6767 - acc: 0.8674 - val_loss: 0.7735 - val_acc: 0.8366\n",
      "Epoch 57/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6750 - acc: 0.8678 - val_loss: 0.7075 - val_acc: 0.8550\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 15s 340us/step - loss: 0.6744 - acc: 0.8683 - val_loss: 0.7503 - val_acc: 0.8488\n",
      "Epoch 59/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6722 - acc: 0.8672 - val_loss: 0.7113 - val_acc: 0.8590\n",
      "Epoch 60/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6724 - acc: 0.8676 - val_loss: 0.7849 - val_acc: 0.8378\n",
      "Epoch 61/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6781 - acc: 0.8664 - val_loss: 0.7092 - val_acc: 0.8598\n",
      "Epoch 62/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6703 - acc: 0.8678 - val_loss: 0.7434 - val_acc: 0.8460\n",
      "Epoch 63/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6712 - acc: 0.8667 - val_loss: 0.7248 - val_acc: 0.8506\n",
      "Epoch 64/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6655 - acc: 0.8696 - val_loss: 0.7252 - val_acc: 0.8578\n",
      "Epoch 65/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6721 - acc: 0.8683 - val_loss: 0.7099 - val_acc: 0.8522\n",
      "Epoch 66/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6639 - acc: 0.8693 - val_loss: 0.7791 - val_acc: 0.8392\n",
      "Epoch 67/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6667 - acc: 0.8704 - val_loss: 0.7380 - val_acc: 0.8526\n",
      "Epoch 68/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6711 - acc: 0.8681 - val_loss: 0.9224 - val_acc: 0.8044\n",
      "Epoch 69/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6705 - acc: 0.8676 - val_loss: 0.7703 - val_acc: 0.8404\n",
      "Epoch 70/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6747 - acc: 0.8685 - val_loss: 0.7538 - val_acc: 0.8496\n",
      "Epoch 71/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6722 - acc: 0.8667 - val_loss: 0.7058 - val_acc: 0.8560\n",
      "Epoch 72/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6653 - acc: 0.8695 - val_loss: 0.7204 - val_acc: 0.8588\n",
      "Epoch 73/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6715 - acc: 0.8673 - val_loss: 0.8201 - val_acc: 0.8216\n",
      "Epoch 74/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6731 - acc: 0.8677 - val_loss: 0.6856 - val_acc: 0.8646\n",
      "Epoch 75/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6711 - acc: 0.8674 - val_loss: 0.7828 - val_acc: 0.8428\n",
      "Epoch 76/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6693 - acc: 0.8689 - val_loss: 0.7021 - val_acc: 0.8598\n",
      "Epoch 77/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6666 - acc: 0.8687 - val_loss: 0.7872 - val_acc: 0.8394\n",
      "Epoch 78/200\n",
      "45000/45000 [==============================] - 15s 340us/step - loss: 0.6682 - acc: 0.8705 - val_loss: 0.6979 - val_acc: 0.8638\n",
      "Epoch 79/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6677 - acc: 0.8696 - val_loss: 0.7837 - val_acc: 0.8362\n",
      "Epoch 80/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6783 - acc: 0.8665 - val_loss: 0.7394 - val_acc: 0.8488\n",
      "Epoch 81/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6720 - acc: 0.8682 - val_loss: 0.7090 - val_acc: 0.8556\n",
      "Epoch 82/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6713 - acc: 0.8682 - val_loss: 0.6953 - val_acc: 0.8620\n",
      "Epoch 83/200\n",
      "45000/45000 [==============================] - 15s 334us/step - loss: 0.6808 - acc: 0.8640 - val_loss: 0.7198 - val_acc: 0.8586\n",
      "Epoch 84/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6699 - acc: 0.8683 - val_loss: 0.7433 - val_acc: 0.8494\n",
      "Epoch 85/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6707 - acc: 0.8684 - val_loss: 0.8015 - val_acc: 0.8326\n",
      "Epoch 86/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6737 - acc: 0.8684 - val_loss: 0.6950 - val_acc: 0.8646\n",
      "Epoch 87/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6672 - acc: 0.8690 - val_loss: 0.7714 - val_acc: 0.8426\n",
      "Epoch 88/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6675 - acc: 0.8693 - val_loss: 0.7677 - val_acc: 0.8334\n",
      "Epoch 89/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6692 - acc: 0.8693 - val_loss: 0.6781 - val_acc: 0.8644\n",
      "Epoch 90/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6728 - acc: 0.8675 - val_loss: 0.7786 - val_acc: 0.8370\n",
      "Epoch 91/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6708 - acc: 0.8682 - val_loss: 0.7203 - val_acc: 0.8618\n",
      "Epoch 92/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6693 - acc: 0.8683 - val_loss: 0.6716 - val_acc: 0.8720\n",
      "Epoch 93/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6713 - acc: 0.8688 - val_loss: 0.7334 - val_acc: 0.8470\n",
      "Epoch 94/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6775 - acc: 0.8665 - val_loss: 0.7465 - val_acc: 0.8434\n",
      "Epoch 95/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6657 - acc: 0.8695 - val_loss: 0.8473 - val_acc: 0.8096\n",
      "Epoch 96/200\n",
      "45000/45000 [==============================] - 15s 332us/step - loss: 0.6673 - acc: 0.8668 - val_loss: 0.6843 - val_acc: 0.8682\n",
      "Epoch 97/200\n",
      "45000/45000 [==============================] - 15s 323us/step - loss: 0.6683 - acc: 0.8692 - val_loss: 0.7315 - val_acc: 0.8550\n",
      "Epoch 98/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6599 - acc: 0.8715 - val_loss: 0.7367 - val_acc: 0.8494\n",
      "Epoch 99/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6659 - acc: 0.8713 - val_loss: 0.7219 - val_acc: 0.8512\n",
      "Epoch 100/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6705 - acc: 0.8677 - val_loss: 0.7415 - val_acc: 0.8508\n",
      "Epoch 101/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6636 - acc: 0.8696 - val_loss: 0.7959 - val_acc: 0.8382\n",
      "Epoch 102/200\n",
      "45000/45000 [==============================] - 15s 334us/step - loss: 0.6653 - acc: 0.8691 - val_loss: 0.7313 - val_acc: 0.8500\n",
      "Epoch 103/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6706 - acc: 0.8678 - val_loss: 0.7489 - val_acc: 0.8418\n",
      "Epoch 104/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6683 - acc: 0.8703 - val_loss: 0.8234 - val_acc: 0.8238\n",
      "Epoch 105/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6601 - acc: 0.8702 - val_loss: 0.7376 - val_acc: 0.8502\n",
      "Epoch 106/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6664 - acc: 0.8682 - val_loss: 0.6969 - val_acc: 0.8630\n",
      "Epoch 107/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6696 - acc: 0.8678 - val_loss: 0.7166 - val_acc: 0.8558\n",
      "Epoch 108/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6632 - acc: 0.8693 - val_loss: 0.8169 - val_acc: 0.8290\n",
      "Epoch 109/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6692 - acc: 0.8681 - val_loss: 0.6989 - val_acc: 0.8600\n",
      "Epoch 110/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6686 - acc: 0.8692 - val_loss: 0.6755 - val_acc: 0.8686\n",
      "Epoch 111/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6671 - acc: 0.8687 - val_loss: 0.7559 - val_acc: 0.8420\n",
      "Epoch 112/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6640 - acc: 0.8688 - val_loss: 0.7124 - val_acc: 0.8510\n",
      "Epoch 113/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6622 - acc: 0.8699 - val_loss: 0.7316 - val_acc: 0.8536\n",
      "Epoch 114/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6670 - acc: 0.8714 - val_loss: 0.7799 - val_acc: 0.8324\n",
      "Epoch 115/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6757 - acc: 0.8673 - val_loss: 0.6911 - val_acc: 0.8636\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6596 - acc: 0.8702 - val_loss: 0.7622 - val_acc: 0.8408\n",
      "Epoch 117/200\n",
      "45000/45000 [==============================] - 15s 334us/step - loss: 0.6652 - acc: 0.8710 - val_loss: 0.7327 - val_acc: 0.8504\n",
      "Epoch 118/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6554 - acc: 0.8740 - val_loss: 0.7271 - val_acc: 0.8492\n",
      "Epoch 119/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6667 - acc: 0.8681 - val_loss: 0.7103 - val_acc: 0.8532\n",
      "Epoch 120/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6678 - acc: 0.8683 - val_loss: 0.7585 - val_acc: 0.8458\n",
      "Epoch 121/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6575 - acc: 0.8714 - val_loss: 0.6853 - val_acc: 0.8680\n",
      "Epoch 122/200\n",
      "45000/45000 [==============================] - 15s 334us/step - loss: 0.6635 - acc: 0.8711 - val_loss: 0.6983 - val_acc: 0.8622\n",
      "Epoch 123/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6636 - acc: 0.8700 - val_loss: 0.6998 - val_acc: 0.8640\n",
      "Epoch 124/200\n",
      "45000/45000 [==============================] - 15s 334us/step - loss: 0.6620 - acc: 0.8698 - val_loss: 0.7895 - val_acc: 0.8364\n",
      "Epoch 125/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6660 - acc: 0.8689 - val_loss: 0.7161 - val_acc: 0.8540\n",
      "Epoch 126/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6657 - acc: 0.8692 - val_loss: 0.7076 - val_acc: 0.8512\n",
      "Epoch 127/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6593 - acc: 0.8702 - val_loss: 0.7014 - val_acc: 0.8604\n",
      "Epoch 128/200\n",
      "45000/45000 [==============================] - 15s 332us/step - loss: 0.6633 - acc: 0.8697 - val_loss: 0.6905 - val_acc: 0.8672\n",
      "Epoch 129/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6666 - acc: 0.8686 - val_loss: 0.7824 - val_acc: 0.8370\n",
      "Epoch 130/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6562 - acc: 0.8726 - val_loss: 0.6678 - val_acc: 0.8658\n",
      "Epoch 131/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6665 - acc: 0.8682 - val_loss: 0.6858 - val_acc: 0.8690\n",
      "Epoch 132/200\n",
      "45000/45000 [==============================] - 15s 334us/step - loss: 0.6622 - acc: 0.8691 - val_loss: 0.7124 - val_acc: 0.8578\n",
      "Epoch 133/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6629 - acc: 0.8698 - val_loss: 0.7259 - val_acc: 0.8546\n",
      "Epoch 134/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6556 - acc: 0.8717 - val_loss: 0.7955 - val_acc: 0.8352\n",
      "Epoch 135/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6604 - acc: 0.8722 - val_loss: 0.7213 - val_acc: 0.8584\n",
      "Epoch 136/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6669 - acc: 0.8686 - val_loss: 0.6585 - val_acc: 0.8760\n",
      "Epoch 137/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6592 - acc: 0.8702 - val_loss: 0.9297 - val_acc: 0.7938\n",
      "Epoch 138/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6624 - acc: 0.8693 - val_loss: 0.7139 - val_acc: 0.8572\n",
      "Epoch 139/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6601 - acc: 0.8712 - val_loss: 0.7171 - val_acc: 0.8564\n",
      "Epoch 140/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6562 - acc: 0.8700 - val_loss: 0.7594 - val_acc: 0.8462\n",
      "Epoch 141/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6629 - acc: 0.8681 - val_loss: 0.6927 - val_acc: 0.8642\n",
      "Epoch 142/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6580 - acc: 0.8711 - val_loss: 0.7268 - val_acc: 0.8474\n",
      "Epoch 143/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6586 - acc: 0.8701 - val_loss: 0.7307 - val_acc: 0.8530\n",
      "Epoch 144/200\n",
      "45000/45000 [==============================] - 15s 333us/step - loss: 0.6589 - acc: 0.8720 - val_loss: 0.6837 - val_acc: 0.8684\n",
      "Epoch 145/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6547 - acc: 0.8721 - val_loss: 0.7075 - val_acc: 0.8594\n",
      "Epoch 146/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6659 - acc: 0.8682 - val_loss: 0.7346 - val_acc: 0.8470\n",
      "Epoch 147/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6620 - acc: 0.8687 - val_loss: 0.6732 - val_acc: 0.8724\n",
      "Epoch 148/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6559 - acc: 0.8714 - val_loss: 0.7037 - val_acc: 0.8592\n",
      "Epoch 149/200\n",
      "45000/45000 [==============================] - 15s 340us/step - loss: 0.6573 - acc: 0.8714 - val_loss: 0.8295 - val_acc: 0.8214\n",
      "Epoch 150/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6552 - acc: 0.8719 - val_loss: 0.7883 - val_acc: 0.8310\n",
      "Epoch 151/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6632 - acc: 0.8690 - val_loss: 0.7242 - val_acc: 0.8516\n",
      "Epoch 152/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6616 - acc: 0.8709 - val_loss: 0.7831 - val_acc: 0.8424\n",
      "Epoch 153/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6596 - acc: 0.8715 - val_loss: 0.9419 - val_acc: 0.8006\n",
      "Epoch 154/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6614 - acc: 0.8703 - val_loss: 0.7116 - val_acc: 0.8576\n",
      "Epoch 155/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6608 - acc: 0.8707 - val_loss: 0.7190 - val_acc: 0.8548\n",
      "Epoch 156/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6603 - acc: 0.8709 - val_loss: 0.7314 - val_acc: 0.8474\n",
      "Epoch 157/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6605 - acc: 0.8696 - val_loss: 0.7604 - val_acc: 0.8386\n",
      "Epoch 158/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6559 - acc: 0.8720 - val_loss: 0.7170 - val_acc: 0.8550\n",
      "Epoch 159/200\n",
      "45000/45000 [==============================] - 15s 334us/step - loss: 0.6547 - acc: 0.8712 - val_loss: 0.6658 - val_acc: 0.8682\n",
      "Epoch 160/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6545 - acc: 0.8720 - val_loss: 0.7722 - val_acc: 0.8362\n",
      "Epoch 161/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6614 - acc: 0.8703 - val_loss: 0.7268 - val_acc: 0.8548\n",
      "Epoch 162/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6580 - acc: 0.8703 - val_loss: 0.7224 - val_acc: 0.8454\n",
      "Epoch 163/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6568 - acc: 0.8715 - val_loss: 0.6674 - val_acc: 0.8668\n",
      "Epoch 164/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6664 - acc: 0.8687 - val_loss: 0.6887 - val_acc: 0.8654\n",
      "Epoch 165/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6585 - acc: 0.8721 - val_loss: 0.8330 - val_acc: 0.8190\n",
      "Epoch 166/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6612 - acc: 0.8698 - val_loss: 0.7358 - val_acc: 0.8494\n",
      "Epoch 167/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6599 - acc: 0.8710 - val_loss: 0.7346 - val_acc: 0.8494\n",
      "Epoch 168/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6643 - acc: 0.8686 - val_loss: 0.7297 - val_acc: 0.8460\n",
      "Epoch 169/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6591 - acc: 0.8716 - val_loss: 0.7081 - val_acc: 0.8588\n",
      "Epoch 170/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6664 - acc: 0.8670 - val_loss: 0.7253 - val_acc: 0.8510\n",
      "Epoch 171/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6557 - acc: 0.8709 - val_loss: 0.7177 - val_acc: 0.8534\n",
      "Epoch 172/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6643 - acc: 0.8694 - val_loss: 0.7912 - val_acc: 0.8312\n",
      "Epoch 173/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6596 - acc: 0.8698 - val_loss: 0.7408 - val_acc: 0.8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6540 - acc: 0.8723 - val_loss: 0.7344 - val_acc: 0.8558\n",
      "Epoch 175/200\n",
      "45000/45000 [==============================] - 15s 334us/step - loss: 0.6569 - acc: 0.8717 - val_loss: 0.7021 - val_acc: 0.8564\n",
      "Epoch 176/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6536 - acc: 0.8737 - val_loss: 0.7379 - val_acc: 0.8454\n",
      "Epoch 177/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6598 - acc: 0.8696 - val_loss: 0.7325 - val_acc: 0.8530\n",
      "Epoch 178/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6631 - acc: 0.8693 - val_loss: 0.7773 - val_acc: 0.8378\n",
      "Epoch 179/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6597 - acc: 0.8708 - val_loss: 0.7387 - val_acc: 0.8490\n",
      "Epoch 180/200\n",
      "45000/45000 [==============================] - 15s 334us/step - loss: 0.6583 - acc: 0.8710 - val_loss: 0.7160 - val_acc: 0.8520\n",
      "Epoch 181/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6567 - acc: 0.8698 - val_loss: 0.7400 - val_acc: 0.8490\n",
      "Epoch 182/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6656 - acc: 0.8692 - val_loss: 0.6676 - val_acc: 0.8732\n",
      "Epoch 183/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6651 - acc: 0.8691 - val_loss: 0.6757 - val_acc: 0.8664\n",
      "Epoch 184/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6576 - acc: 0.8710 - val_loss: 0.7623 - val_acc: 0.8392\n",
      "Epoch 185/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6534 - acc: 0.8722 - val_loss: 0.7218 - val_acc: 0.8590\n",
      "Epoch 186/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6549 - acc: 0.8714 - val_loss: 0.7054 - val_acc: 0.8524\n",
      "Epoch 187/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6554 - acc: 0.8708 - val_loss: 0.7399 - val_acc: 0.8470\n",
      "Epoch 188/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6554 - acc: 0.8712 - val_loss: 0.7258 - val_acc: 0.8572\n",
      "Epoch 189/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6583 - acc: 0.8689 - val_loss: 0.8033 - val_acc: 0.8270\n",
      "Epoch 190/200\n",
      "45000/45000 [==============================] - 15s 338us/step - loss: 0.6566 - acc: 0.8710 - val_loss: 0.6950 - val_acc: 0.8576\n",
      "Epoch 191/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6585 - acc: 0.8709 - val_loss: 0.7357 - val_acc: 0.8500\n",
      "Epoch 192/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6527 - acc: 0.8734 - val_loss: 0.8076 - val_acc: 0.8308\n",
      "Epoch 193/200\n",
      "45000/45000 [==============================] - 15s 334us/step - loss: 0.6511 - acc: 0.8736 - val_loss: 0.9686 - val_acc: 0.7890\n",
      "Epoch 194/200\n",
      "45000/45000 [==============================] - 15s 337us/step - loss: 0.6527 - acc: 0.8735 - val_loss: 0.8118 - val_acc: 0.8214\n",
      "Epoch 195/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6555 - acc: 0.8709 - val_loss: 0.8091 - val_acc: 0.8308\n",
      "Epoch 196/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6625 - acc: 0.8692 - val_loss: 0.6869 - val_acc: 0.8620\n",
      "Epoch 197/200\n",
      "45000/45000 [==============================] - 15s 335us/step - loss: 0.6515 - acc: 0.8724 - val_loss: 0.7226 - val_acc: 0.8472\n",
      "Epoch 198/200\n",
      "45000/45000 [==============================] - 15s 334us/step - loss: 0.6596 - acc: 0.8688 - val_loss: 0.7151 - val_acc: 0.8554\n",
      "Epoch 199/200\n",
      "45000/45000 [==============================] - 15s 336us/step - loss: 0.6571 - acc: 0.8706 - val_loss: 0.7477 - val_acc: 0.8482\n",
      "Epoch 200/200\n",
      "45000/45000 [==============================] - 15s 339us/step - loss: 0.6478 - acc: 0.8744 - val_loss: 0.7723 - val_acc: 0.8382\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    " \n",
    "train_history=model_1.fit(x_train,y_train,epochs=200, batch_size=128,\n",
    "             validation_split=0.1, verbose=1)\n",
    "\n",
    "model_1.save('8.6.5ent_b_cifar10vgg16_1_1.h5')\n",
    "model_1.save_weights('8.6.5ent_b_cifar10vgg16_1_1_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7962649865150452, 0.8339, 0.9908, 0.8339]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import keras\n",
    "top1_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=1) #top-1精度\n",
    "\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',metrics=['accuracy','top_k_categorical_accuracy',top1_acc])\n",
    "\n",
    "model_1.evaluate(x_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试损失为：0.7963\n",
      "测试准确率为：0.8339\n"
     ]
    }
   ],
   "source": [
    "score=model_1.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"测试损失为：{:.4f}\".format(score[0]))\n",
    "print(\"测试准确率为：{:.4f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
